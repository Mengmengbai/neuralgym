Index: dummy.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- dummy.py	(date 1574676496000)
+++ dummy.py	(date 1574676496000)
@@ -0,0 +1,169 @@
+import os
+
+from skimage.feature import canny
+
+os.environ["CUDA_VISIBLE_DEVICE"] = "0"
+import argparse
+import cv2
+import numpy as np
+import tensorflow as tf
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--image', default='/mnt/disk/mm.bai/inpaiting/dsp-face/test8/image/fcfed5c2baf56ffa866e293812b8d894.jpg', type=str,
+                    help='The filename of image to be completed.')
+parser.add_argument('--mask', default='/mnt/disk/mm.bai/inpaiting/dsp-face/test8/image/mask/fcfed5c2baf56ffa866e293812b8d894.jpg', type=str,
+                    help='The filename of mask, value 255 indicates mask.')
+parser.add_argument('--guide', default='image/guide.jpg', type=str,
+                    help='The filename of guidelines, value 0 indicates guide line.')
+parser.add_argument('--output', default='image/result-test_eden.jpg', type=str,
+                    help='Where to write output.')
+parser.add_argument('--pbdir', default='/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/ffhqpaper-resize-2500.pb.pb', type=str,
+                    help='Where to import pb model.')
+
+
+def getNoise(offset_h, offset_w):
+    noiseMask = np.random.rand(height, width)
+    if offset_h:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    else:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=2)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=2)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    return noiseMask1, noiseMask2
+
+def gaussian_blur(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    pad_w = 0
+    padded = tf.pad(image, [[0, 0], [pad_w, pad_w], [pad_w, pad_w], [0, 0]], mode='CONSTANT')
+    # for channel_idx in range(cdim):
+    #     data_c = padded[:, :, :, channel_idx:(channel_idx + 1)]
+    #     # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+    #     g = kernel.reshape((kernel_size, kernel_size, 1, 1))
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     outputs.append(data_c)
+
+    k = kernel.reshape((kernel_size, kernel_size, 1, 1))
+    output = tf.nn.depthwise_conv2d(padded, k, [1,1,1,1], 'SAME')
+    return output
+
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+
+    height = 512
+    width = height
+    channel = 3
+    noise = np.random.rand(height, width)
+
+    image = cv2.imread(args.image)
+    mask = cv2.imread(args.mask)
+
+    image = cv2.resize(image, (height,width))
+    mask = cv2.resize(mask, (height, width))
+
+    h, w, _ = image.shape
+    grid = 8
+    image = image[:h // grid * grid, :w // grid * grid, :]
+    mask = mask[:h // grid * grid, :w // grid * grid, :]
+    print('Shape of image: {}'.format(image.shape))
+
+    image = (image / 127.5 - 1.).astype(np.float32)
+    mask = (mask > 127.5).astype(np.float32)
+
+    image = np.expand_dims(image, 0)
+    mask = np.expand_dims(mask, 0)
+
+    noiseMask1, noiseMask2 = getNoise(2, 1)
+    noiseMask1_1, noiseMask2_1 = getNoise(None, 2)
+
+    one_s = np.ones_like(mask)
+    image_original = image
+    image = image * (1. - mask)
+
+    # input_image = np.concatenate([image, one_s[:,:,:,0:1], mask[:,:,:,0:1], noise], axis=3)
+    input_image = np.concatenate([image, mask[:, :, :, 0:1]], axis=3)
+
+    # 3*3 gauss filter
+    gauss_filter_3 = np.array(
+        [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+
+    gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+
+    with tf.Graph().as_default():
+        output_graph_def = tf.GraphDef()
+        with open(args.pbdir, "rb") as f:
+            output_graph_def.ParseFromString(f.read())
+            _ = tf.import_graph_def(output_graph_def, name="")
+        config = tf.ConfigProto(allow_soft_placement=True)
+        with tf.Session(config=config) as sess:
+            sess.run(tf.global_variables_initializer())
+            input_image_tensor1 = sess.graph.get_tensor_by_name("input:0")
+            # input_image_tensor2 = sess.graph.get_tensor_by_name("input2:0")
+            # model_otput = sess.graph.get_tensor_by_name("inpaint_net/post/conv9-2/BiasAdd:0")#
+            output = sess.graph.get_tensor_by_name("output:0")
+            # output = tf.clip_by_value(model_otput, -1., 1.)
+            # output_tensor = tf.tanh(model_otput)
+            post_processing = 0
+            if post_processing:
+                x1 = output[:, :-2, :-1, :]
+                # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+                x2 = output[:, 2:, 1:, :]
+                # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+                with tf.variable_scope(name_or_scope='x1Multi'):
+                    x1 = x1 * noiseMask1
+                with tf.variable_scope(name_or_scope='x2Multi'):
+                    x2 = x2 * noiseMask2
+                # with tf.variable_scope(name_or_scope='x1Multix2'):
+                output = tf.add(x1, x2, name='x1Multix2')
+                output = tf.pad(output, paddings=[[0, 0], [1, 1], [1, 0], [0, 0]], mode="CONSTANT", name='output_1')
+
+                x1 = output[:, :, :-2, :]
+                # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+                x2 = output[:, :, 2:, :]
+                # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+                with tf.variable_scope(name_or_scope='x1Multi_1'):
+                    x1 = x1 * noiseMask1_1
+                with tf.variable_scope(name_or_scope='x2Multi_1'):
+                    x2 = x2 * noiseMask2_1
+                # with tf.variable_scope(name_or_scope='x1Multix2'):
+                output = tf.add(x1, x2, name='x1Multix2_1')
+                output = tf.pad(output, paddings=[[0, 0], [0, 0], [0, 2], [0, 0]], mode="CONSTANT", name='output_2')
+            blur = 0
+            if blur:
+                # Bilateral blur
+                # output = bilateral_blur(output)
+                # Gauss blur
+                output = gaussian_blur(output, gauss_filter_3, 3)
+
+
+            output = tf.clip_by_value(output, -1., 1.)
+            # B,G,R, _mask = tf.split(input_image_tensor1, axis=3, num_or_size_splits=4)
+            # img = tf.concat([B,G,R], axis=3)
+            #
+            # # img_mask = input_image_original*_mask
+            # # gray_img = tf.image.rgb_to_grayscale(img_mask)
+            #
+            # output_tensor = img*(1.-_mask) + output*_mask
+            # output_tensor = output*_mask*(1.-edge_mask)
+            output_tensor = (output + 1.) * 127.5
+            output_tensor = tf.saturate_cast(output_tensor, tf.uint8)
+            result = sess.run([output_tensor], feed_dict={input_image_tensor1: input_image})#,
+
+            img_output = result[0][-1]
+
+            # bilateral filter
+            # sigma_s = 1
+            # sigma_r = 0.1 * 255
+            # img_gray = cv2.cvtColor(img_output, cv2.COLOR_BGR2GRAY)
+            # img_output = bilateralfilter(img_output, img_gray, sigma_s, sigma_r)
+
+            cv2.imwrite(args.output, img_output)
+            print("test done")
\ No newline at end of file
Index: inpaint_loss.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_loss.py	(date 1573192737000)
+++ inpaint_loss.py	(date 1573192737000)
@@ -0,0 +1,175 @@
+import tensorflow as tf
+from vgg19 import Vgg19
+from vgg16 import Vgg16
+epsilon = 1e-14
+
+
+
+def _extractor_feature_vgg16_net(config, batch_pos, x2, batch_complete):
+    batch_input = tf.concat([batch_pos, x2, batch_complete], axis=0)
+    vgg16 = Vgg16(vgg16_npy_path=config.VGG_MODEL_FILE)
+    batch_output = vgg16.build(batch_input, reuse=tf.AUTO_REUSE)
+    gt_pool1, x2_pool1, comp_pool1 = tf.split(batch_output[0], num_or_size_splits=3, axis=0)
+    gt_pool2, x2_pool2, comp_pool2 = tf.split(batch_output[1], num_or_size_splits=3, axis=0)
+    gt_pool3, x2_pool3, comp_pool3 = tf.split(batch_output[2], num_or_size_splits=3, axis=0)
+    gt_feat = [gt_pool1, gt_pool2, gt_pool3]
+    x2_feat = [x2_pool1, x2_pool2, x2_pool3]
+    comp_feat = [comp_pool1, comp_pool2, comp_pool3]
+    return gt_feat, x2_feat, comp_feat
+
+
+def _prc_and_style_loss(feat_out_comp, feat_out, feat_gt):
+    prc_loss = 0
+    for i in range(3):
+        prc_loss += tf.reduce_mean(tf.abs(feat_out[i] - feat_gt[i]))
+        prc_loss += tf.reduce_mean(tf.abs(feat_out_comp[i] - feat_gt[i]))
+    style_loss = 0
+    for i in range(3):
+        style_loss += tf.reduce_mean(tf.abs(gram_matrix(feat_out[i]) - gram_matrix(feat_gt[i])))
+        style_loss += tf.reduce_mean(tf.abs(gram_matrix(feat_out_comp[i]) - gram_matrix(feat_gt[i])))
+    return prc_loss, style_loss
+
+
+def extractor_feature_vgg16_net(config, batch_pos, batch_complete):
+    batch_input = tf.concat([batch_pos, batch_complete], axis=0)
+    vgg16 = Vgg16(vgg16_npy_path=config.VGG_MODEL_FILE)
+    batch_output = vgg16.build(batch_input, reuse=tf.AUTO_REUSE)
+    gt_pool1, comp_pool1 = tf.split(batch_output[0], num_or_size_splits=2, axis=0)
+    gt_pool2, comp_pool2 = tf.split(batch_output[1], num_or_size_splits=2, axis=0)
+    gt_pool3, comp_pool3 = tf.split(batch_output[2], num_or_size_splits=2, axis=0)
+    gt_feat = [gt_pool1, gt_pool2, gt_pool3]
+    comp_feat = [comp_pool1, comp_pool2, comp_pool3]
+    return gt_feat, comp_feat
+
+
+def extractor_feature_vgg19_net(config, batch_pos, batch_complete):
+    batch_input = tf.concat([batch_pos, batch_complete], axis=0)
+    vgg19 = Vgg19(vgg19_npy_path=config.VGG19_MODEL_FILE)
+    batch_output = vgg19.build(batch_input, reuse=tf.AUTO_REUSE)
+    gt_pool1, comp_pool1 = tf.split(batch_output[0], num_or_size_splits=2, axis=0)
+    gt_pool2, comp_pool2 = tf.split(batch_output[1], num_or_size_splits=2, axis=0)
+    gt_pool3, comp_pool3 = tf.split(batch_output[2], num_or_size_splits=2, axis=0)
+    gt_pool4, comp_pool4 = tf.split(batch_output[3], num_or_size_splits=2, axis=0)
+    gt_pool5, comp_pool5 = tf.split(batch_output[4], num_or_size_splits=2, axis=0)
+    gt_feat = [gt_pool1, gt_pool2, gt_pool3, gt_pool4, gt_pool5]
+    comp_feat = [comp_pool1, comp_pool2, comp_pool3, comp_pool4, comp_pool5]
+    return gt_feat, comp_feat
+
+def extractor_feature_vgg19_net_edge(config, batch_pos, batch_complete):
+    batch_input = tf.concat([batch_pos, batch_complete], axis=0)
+    vgg19 = Vgg19(vgg19_npy_path=config.VGG19_MODEL_FILE)
+    batch_output = vgg19.build(batch_input, reuse=tf.AUTO_REUSE)
+    gt_pool1, comp_pool1 = tf.split(batch_output[0], num_or_size_splits=2, axis=0)
+    gt_pool2, comp_pool2 = tf.split(batch_output[1], num_or_size_splits=2, axis=0)
+    gt_pool3, comp_pool3 = tf.split(batch_output[2], num_or_size_splits=2, axis=0)
+    # gt_pool4, comp_pool4 = tf.split(batch_output[3], num_or_size_splits=2, axis=0)
+    # gt_pool5, comp_pool5 = tf.split(batch_output[4], num_or_size_splits=2, axis=0)
+    gt_feat = [gt_pool1, gt_pool2, gt_pool3]#, gt_pool4, gt_pool5]
+    comp_feat = [comp_pool1, comp_pool2, comp_pool3]#, comp_pool4, comp_pool5]
+    return gt_feat, comp_feat
+
+def extractor_feature_vgg19_net_3feature(config, batch_pos, x2, batch_complete):
+    batch_input = tf.concat([batch_pos, x2, batch_complete], axis=0)
+    vgg19 = Vgg19(vgg19_npy_path=config.VGG19_MODEL_FILE)
+    batch_output = vgg19.build(batch_input, reuse=tf.AUTO_REUSE)
+    gt_pool1, x2_ppol1, comp_pool1 = tf.split(batch_output[0], num_or_size_splits=3, axis=0)
+    gt_pool2, x2_ppol2, comp_pool2 = tf.split(batch_output[1], num_or_size_splits=3, axis=0)
+    gt_pool3, x2_ppol3, comp_pool3 = tf.split(batch_output[2], num_or_size_splits=3, axis=0)
+    gt_pool4, x2_ppol4, comp_pool4 = tf.split(batch_output[3], num_or_size_splits=3, axis=0)
+    gt_pool5, x2_ppol5, comp_pool5 = tf.split(batch_output[4], num_or_size_splits=3, axis=0)
+    gt_feat = [gt_pool1, gt_pool2, gt_pool3, gt_pool4, gt_pool5]
+    x2_feat = [x2_ppol1, x2_ppol2, x2_ppol3, x2_ppol4, x2_ppol5]
+    comp_feat = [comp_pool1, comp_pool2, comp_pool3, comp_pool4, comp_pool5]
+    return gt_feat, x2_feat, comp_feat
+
+def extractor_feature_vgg19_net_edge_x2(config, edge_pos, edge_complete, pos, x2, complete):
+    batch_input = tf.concat([edge_pos, edge_complete, pos, x2, complete], axis=0)
+    vgg19 = Vgg19(vgg19_npy_path=config.VGG19_MODEL_FILE)
+    batch_output = vgg19.build(batch_input, reuse=tf.AUTO_REUSE)
+    edge_gt_pool1, edge_comp_pool1, gt_pool1, x2_ppol1, comp_pool1 = tf.split(batch_output[0], num_or_size_splits=5, axis=0)
+    edge_gt_pool2, edge_comp_pool2, gt_pool2, x2_ppol2, comp_pool2 = tf.split(batch_output[1], num_or_size_splits=5, axis=0)
+    edge_gt_pool3, edge_comp_pool3, gt_pool3, x2_ppol3, comp_pool3 = tf.split(batch_output[2], num_or_size_splits=5, axis=0)
+    edge_gt_pool4, edge_comp_pool4, gt_pool4, x2_ppol4, comp_pool4 = tf.split(batch_output[3], num_or_size_splits=5, axis=0)
+    edge_gt_pool5, edge_comp_pool5, gt_pool5, x2_ppol5, comp_pool5 = tf.split(batch_output[4], num_or_size_splits=5, axis=0)
+    edge_gt_feat = [edge_gt_pool1, edge_gt_pool2, edge_gt_pool3, edge_gt_pool4, edge_gt_pool5]
+    edge_comp_feat = [edge_comp_pool1, edge_comp_pool2, edge_comp_pool3, edge_comp_pool4, edge_comp_pool5]
+    gt_feat = [gt_pool1, gt_pool2, gt_pool3, gt_pool4, gt_pool5]
+    x2_feat = [x2_ppol1, x2_ppol2, x2_ppol3, x2_ppol4, x2_ppol5]
+    comp_feat = [comp_pool1, comp_pool2, comp_pool3, comp_pool4, comp_pool5]
+    return edge_gt_feat, edge_comp_feat, gt_feat, x2_feat, comp_feat
+
+
+
+def prc_and_style_loss(feat_out_comp, feat_gt):
+    prc_loss = 0
+    for i in range(len(feat_out_comp)):
+        prc_loss += tf.reduce_mean(tf.abs(feat_out_comp[i] - feat_gt[i]))
+    style_loss = 0
+    for i in range(len(feat_gt)):
+        style_loss += tf.reduce_mean(tf.abs(gram_matrix(feat_out_comp[i]) - gram_matrix(feat_gt[i])))
+    return prc_loss, style_loss
+
+def prc_and_style_loss_3feature(feat_gt, feat_x2, feat_comp):
+    prc_loss = 0
+    for i in range(5):
+        prc_loss += tf.reduce_mean(tf.abs(feat_comp[i] - feat_gt[i]))
+        prc_loss += tf.reduce_mean(tf.abs(feat_x2[i] - feat_gt[i]))
+    style_loss = 0
+    for i in range(5):
+        style_loss += tf.reduce_mean(tf.abs(gram_matrix(feat_comp[i]) - gram_matrix(feat_gt[i])))
+        style_loss += tf.reduce_mean(tf.abs(gram_matrix(feat_x2[i]) - gram_matrix(feat_gt[i])))
+    return prc_loss, style_loss
+
+def prc_and_style_vgg16loss_3feature(feat_gt, feat_x2, feat_comp):
+    prc_loss = 0
+    for i in range(3):
+        prc_loss += tf.reduce_mean(tf.abs(feat_comp[i] - feat_gt[i]))
+        prc_loss += tf.reduce_mean(tf.abs(feat_x2[i] - feat_gt[i]))
+    style_loss = 0
+    for i in range(3):
+        style_loss += tf.reduce_mean(tf.abs(gram_matrix(feat_comp[i]) - gram_matrix(feat_gt[i])))
+        style_loss += tf.reduce_mean(tf.abs(gram_matrix(feat_x2[i]) - gram_matrix(feat_gt[i])))
+    return prc_loss, style_loss
+
+
+def gram_matrix(layer):
+    shape = tf.shape(layer)
+    b = shape[0]
+    w = shape[1]
+    h = shape[2]
+    c = shape[3]
+    filter = tf.reshape(layer, tf.stack([b, -1, c]))
+    grams = tf.matmul(filter, filter, transpose_a=True) / tf.to_float(w * h * c)
+    return grams
+
+def tvnoise(img, noise, offx, offy):
+    if offx is not None and offy is not None:
+        noise = noise[:,offx:, offy:,:]
+        noise_1 = 1.0-noise
+
+        x1 = img[:,:-offx,:-offy,:]
+        x2 = img[:,offx:,offy:,:]
+
+        x1 = x1 * noise
+        x2 = x2 * noise_1
+        return x1 + x2
+    elif offx is not None and offy == None:
+        noise = noise[:, offx:, :, :]
+        noise_1 = 1.0 - noise
+
+        x1 = img[:, :-offx, :, :]
+        x2 = img[:, offx:, :, :]
+
+        x1 = x1 * noise
+        x2 = x2 * noise_1
+        return x1 + x2
+    elif offx == None and offy is not None:
+        noise = noise[:, 2:, 2:, :]
+        noise_1 = 1.0 - noise
+
+        x1 = img[:, :, :-offy, :]
+        x2 = img[:, :, offy:, :]
+
+        x1 = x1 * noise
+        x2 = x2 * noise_1
+        return x1 + x2
Index: inpaint_layer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_layer.py	(date 1578389212000)
+++ inpaint_layer.py	(date 1578389212000)
@@ -0,0 +1,1226 @@
+import logging
+from queue import Queue
+
+import cv2
+import numpy as np
+from random import randint
+import math
+import random
+import tensorflow as tf
+from tensorflow.contrib.framework.python.ops import add_arg_scope
+
+from neuralgym.ops.layers import resize
+from neuralgym.ops.layers import *
+from neuralgym.ops.loss_ops import *
+from neuralgym.ops.summary_ops import *
+
+
+logger = logging.getLogger()
+np.random.seed(2018)
+epsilon = 1e-14
+weight_regularizer = None
+weight_init = tf.random_normal_initializer(mean=0.0, stddev=0.02)
+
+
+def maskConv( x, cnum, ksize, stride=1, rate=1):
+    pad = ksize // 2 * rate
+    input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="REFLECT")
+
+    conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    out_f = tf.nn.leaky_relu(conv_f, alpha=0.2)
+    # out_f = tf.nn.relu(conv_f)
+
+    conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    conv_g_1 = tf.nn.relu(conv_g_1)
+    padding_g1 = tf.pad(tensor=conv_g_1, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="REFLECT")
+    conv_g_2 = tf.layers.conv2d(inputs=padding_g1, filters=cnum, kernel_size=ksize, strides=1, dilation_rate=rate)
+    conv_g_2 = tf.nn.sigmoid(conv_g_2)
+    x = out_f * conv_g_2
+    return x
+
+def spectral_norm(w, iteration=1, name="SN"):
+    #Spectral normalization which was published on ICLR2018,please refer to "https://www.researchgate.net/publication/318572189_Spectral_Normalization_for_Generative_Adversarial_Networks"
+    #This function spectral_norm is forked from "https://github.com/taki0112/Spectral_Normalization-Tensorflow"
+    w_shape = w.shape.as_list()
+    w = tf.reshape(w, [-1, w_shape[-1]])
+    with tf.variable_scope(name, reuse=False):
+        u = tf.get_variable("u", [1, w_shape[-1]], initializer=tf.truncated_normal_initializer(), trainable=False)
+    u_hat = u
+    v_hat = None
+
+    def l2_norm(v, eps=1e-12):
+        return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)
+
+    for i in range(iteration):
+        v_ = tf.matmul(u_hat, tf.transpose(w))
+        v_hat = l2_norm(v_)
+        u_ = tf.matmul(v_hat, w)
+        u_hat = l2_norm(u_)
+    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))
+    w_norm = w / sigma
+    with tf.control_dependencies([u.assign(u_hat)]):
+        w_norm = tf.reshape(w_norm, w_shape)
+    return w_norm
+
+def conv(inputs, shape, strides, is_sn=False):
+    filters = tf.get_variable("kernel", shape=shape, initializer=tf.random_normal_initializer(stddev=0.02))
+    bias = tf.get_variable("bias", shape=[shape[-1]], initializer=tf.constant_initializer([0]))
+    if is_sn:
+        return tf.nn.conv2d(inputs, spectral_norm(filters, name='SN'), strides, "SAME") + bias
+    else:
+        return tf.nn.conv2d(inputs, filters, strides, "SAME") + bias
+
+def SNconv(x, channels, kernel=4, stride=2, pad=0, pad_type='zero', use_bias=True, sn=False, scope='conv_0'):
+    with tf.variable_scope(scope):
+        if pad_type == 'zero' :
+            x = tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]])
+        if pad_type == 'reflect' :
+            x = tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]], mode='REFLECT')
+
+        if sn :
+            w = tf.get_variable("kernel", shape=[kernel, kernel, x.get_shape()[-1], channels], initializer=weight_init,
+                                regularizer=weight_regularizer)
+            bias = tf.get_variable("bias", [channels], initializer=tf.constant_initializer(0.0))
+            x = tf.nn.conv2d(input=x, filter=spectral_norm(w),
+                             strides=[1, stride, stride, 1], padding='VALID')
+            if use_bias :
+                x = tf.nn.bias_add(x, bias)
+
+        else :
+            x = tf.layers.conv2d(inputs=x, filters=channels,
+                                 kernel_size=kernel, kernel_initializer=weight_init,
+                                 kernel_regularizer=weight_regularizer,
+                                 strides=stride, use_bias=use_bias)
+
+        return x
+
+def fully_connected(inputs, num_out, is_sn=False):
+    W = tf.get_variable("W", [inputs.shape[-1], num_out], initializer=tf.random_normal_initializer(stddev=0.02))
+    b = tf.get_variable("b", [num_out], initializer=tf.constant_initializer([0]))
+    if is_sn:
+        return tf.matmul(inputs, spectral_norm("sn", W)) + b
+    else:
+        return tf.matmul(inputs, W) + b
+
+
+def leaky_relu(inputs, alpha=0.2):
+    with tf.name_scope(name='mul'):
+        res_mul = tf.multiply(alpha, inputs)
+    with tf.name_scope(name='maximum'):
+        res = tf.maximum(res_mul, inputs)
+    return res
+
+def clip_by_value(x, min, max, name='clip'):
+    with tf.name_scope(name=name):
+        with tf.name_scope(name='clipmin'):
+            res = tf.maximum(x, min)
+            res = -1. * res
+        with tf.name_scope(name='clipmax'):
+            max = -1. * max
+            res = tf.maximum(res, max)
+        with tf.name_scope(name='clip_res'):
+            res = -1. * res
+        return res
+
+
+def instanceNorm(inputs):
+    mean, var = tf.nn.moments(inputs, axes=[1, 2], keep_dims=True)
+    scale = tf.get_variable("scale", shape=mean.shape, initializer=tf.constant_initializer([1.0]))
+    shift = tf.get_variable("shift", shape=mean.shape, initializer=tf.constant_initializer([0.0]))
+    return (inputs - mean) * scale / (tf.sqrt(var + epsilon)) + shift
+
+
+
+@add_arg_scope
+def _gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    # assert padding in ['SYMMETRIC', 'SAME', 'REFELECT']
+    # if padding == 'SYMMETRIC' or padding == 'REFELECT':
+    #     p = int(rate*(ksize-1)/2)
+    #     x = tf.pad(x, [[0,0], [p, p], [p, p], [0,0]], mode=padding)
+    #     padding = 'VALID'
+    # x = tf.layers.conv2d(
+    #     x, cnum, ksize, stride, dilation_rate=rate,
+    #     activation=None, padding=padding, name=name)
+    # x1 = tf.nn.relu(x)
+    # x2 = tf.nn.sigmoid(x)
+    # x = x1 * x2
+
+    pad = ksize // 2 * rate
+    input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+
+    conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    # out_f = tf.nn.leaky_relu(conv_f, alpha=0.2)
+    # out_f = tf.nn.relu(conv_f)
+    out_f = tf.nn.sigmoid(conv_f)
+
+    conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    conv_g_1 = tf.nn.relu(conv_g_1)
+    padding_g1 = tf.pad(tensor=conv_g_1, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    conv_g_2 = tf.layers.conv2d(inputs=padding_g1, filters=cnum, kernel_size=ksize, strides=1, dilation_rate=rate)
+    conv_g_2 = tf.nn.sigmoid(conv_g_2)
+    x = out_f * conv_g_2
+    return x
+
+@add_arg_scope
+# v2 2gateConv
+def v2gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    with tf.variable_scope(name):
+        pad = ksize // 2 * rate
+        input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+
+        conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        out_f = leaky_relu(conv_f)
+
+        conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g_1 = leaky_relu(conv_g_1)
+
+        conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        conv_g_2 = leaky_relu(conv_g_2)
+
+        conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1, dilation_rate=rate)
+        # conv_g_3 = tf.nn.sigmoid(conv_g_3)
+        conv_g_3 = tf.nn.relu(conv_g_3)
+        conv_g_3 = tf.clip_by_value(conv_g_3, 0., 1.)
+        x = out_f * conv_g_3
+    return x
+
+@add_arg_scope
+# v3 2gateCon
+def v3gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    # assert padding in ['SYMMETRIC', 'SAME', 'REFELECT']
+    # if padding == 'SYMMETRIC' or padding == 'REFELECT':
+    #     p = int(rate*(ksize-1)/2)
+    #     x = tf.pad(x, [[0,0], [p, p], [p, p], [0,0]], mode=padding)
+    #     padding = 'VALID'
+    # x = tf.layers.conv2d(
+    #     x, cnum, ksize, stride, dilation_rate=rate,
+    #     activation=None, padding=padding, name=name)
+    # x1 = tf.nn.relu(x)
+    # x2 = tf.nn.sigmoid(x)
+    # x = x1 * x2
+
+    pad = ksize // 2 * rate
+    input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    out_f = leaky_relu(conv_f)
+    # Soft mask gating
+    conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g_1 = leaky_relu(conv_g_1)
+    conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    conv_g_2 = leaky_relu(conv_g_2)
+    conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1, dilation_rate=rate)
+    conv_g_3 = tf.nn.sigmoid(conv_g_3)
+    # Soft guide gating
+    conv_g1_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    conv_g1_1 = tf.nn.sigmoid(conv_g1_1)
+    # Elementwise Multiply
+    x = out_f * conv_g_3 * conv_g1_1
+    return x
+
+@add_arg_scope
+# v4 Depthwise pointwise Conv
+def v4gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+
+    pad = ksize // 2 * rate
+    input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+
+    conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    out_f = tf.nn.sigmoid(conv_f)
+
+    conv_g_1 = tf.layers.separable_conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    conv_g_1 = tf.nn.relu(conv_g_1)
+    padding_g1 = tf.pad(tensor=conv_g_1, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    conv_g_2 = tf.layers.separable_conv2d(inputs=padding_g1, filters=cnum, kernel_size=ksize, strides=1, dilation_rate=rate)
+    conv_g_2 = tf.nn.sigmoid(conv_g_2)
+    x = out_f * conv_g_2
+    return x
+
+@add_arg_scope
+# v6 2gateCon
+def v6gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    with tf.variable_scope(name):
+        pad = ksize // 2 * rate
+        input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+        # feature
+        conv_f = tf.layers.separable_conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate,)
+        out_f = leaky_relu(conv_f)
+
+        # Soft mask gating
+        conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g_1 = leaky_relu(conv_g_1)
+        conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        conv_g_2 = leaky_relu(conv_g_2)
+        conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1, dilation_rate=rate)
+        conv_g_3 = leaky_relu(conv_g_3)
+        # conv_g_3 = tf.nn.sigmoid(conv_g_3)
+        # conv_g_3 = tf.nn.relu(conv_g_3)
+        # conv_g_3 = tf.clip_by_value(conv_g_3, 0., 1.)
+
+        # Soft guide gating
+        conv_g1_1 = tf.layers.separable_conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        conv_g1_1 = leaky_relu(conv_g1_1)
+        # conv_g1_1 = tf.nn.sigmoid(conv_g1_1)
+        # conv_g1_1 = tf.nn.relu(conv_g1_1)
+        # conv_g1_1 = tf.clip_by_value(conv_g1_1, 0., 1.)
+
+        # concat the gating
+        conv_g = tf.concat([conv_g_3, conv_g1_1], axis=3)
+        conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        # conv_g = tf.nn.sigmoid(conv_g)
+        conv_g = tf.nn.relu(conv_g)
+        conv_g = tf.clip_by_value(conv_g, 0., 1.)
+        # Elementwise Multiply
+        x = out_f * conv_g
+    return x
+
+@add_arg_scope
+# v6 2gateCon
+def v6gen_gate_conv_2(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    with tf.name_scope(name="stage2"):
+        pad = ksize // 2 * rate
+        input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+        # feature
+        conv_f = tf.layers.separable_conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        out_f = leaky_relu(conv_f)
+        # Soft mask gating
+        conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g_1 = leaky_relu(conv_g_1)
+        conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        conv_g_2 = leaky_relu(conv_g_2)
+        conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1, dilation_rate=rate)
+        conv_g_3 = tf.nn.sigmoid(conv_g_3)
+        # Soft guide gating
+        conv_g1_1 = tf.layers.separable_conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        conv_g1_1 = tf.nn.sigmoid(conv_g1_1)
+        # concat the gating
+        conv_g = tf.concat([conv_g_3, conv_g1_1], axis=3)
+        conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g = tf.nn.sigmoid(conv_g)
+        # Elementwise Multiply
+        x = out_f * conv_g
+    return x
+
+@add_arg_scope
+# v6 2gateCon
+def normal_gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    if stride == 2:
+        if ksize == 5:
+            x = tf.pad(tensor=x, paddings=[[0, 0], [2, 1], [2, 1], [0, 0]], mode="CONSTANT")
+        if ksize == 3:
+            x = tf.pad(tensor=x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode="CONSTANT")
+        padding = 'valid'
+    else:
+        padding = 'same'
+    with tf.variable_scope(name_or_scope=name):
+        # feature
+        conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding=padding)
+        out_f = tf.nn.sigmoid(conv_f)
+        # Soft mask gating
+        conv_g_1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding=padding)
+        conv_g = tf.nn.sigmoid(conv_g_1)
+        # Elementwise Multiply
+        x = out_f * conv_g
+    return x
+
+@add_arg_scope
+# gateCon
+def _gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv', padding='same',training=True):
+
+    with tf.variable_scope(name):
+        pad = ksize // 2 * rate
+        input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="REFLECT")
+        # feature
+        conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize,
+            strides=stride, dilation_rate=rate)
+        conv_f = tf.nn.leaky_relu(conv_f)
+
+        # Mask gating 1
+        conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1,
+            strides=1, dilation_rate=rate)
+        conv_g_1 = tf.nn.leaky_relu(conv_g_1)
+
+        conv_g_2 = tf.layers.separable_conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize,
+            strides=stride, dilation_rate=rate)
+        conv_g_2 = tf.nn.leaky_relu(conv_g_2)
+
+        conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1,
+            strides=1, dilation_rate=rate)
+        conv_g_3 = tf.nn.sigmoid(conv_g_3)
+
+        # Mask gating 2
+        pad = 3 // 2
+        conv_g1_1 = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="REFLECT")
+        conv_g1_1 = tf.layers.separable_conv2d(inputs=conv_g1_1, filters=cnum, kernel_size=3,
+            strides=stride, dilation_rate=1)
+        conv_g1_1 = tf.nn.leaky_relu(conv_g1_1)
+
+        # concat the gating
+        conv_g = tf.concat([conv_g_3, conv_g1_1], axis=3)
+        conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g = tf.nn.sigmoid(conv_g)
+        # Elementwise Multiply
+        x = conv_f * conv_g
+    return x
+
+@add_arg_scope
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+
+    if stride == 2:
+        if ksize == 5:
+            input = tf.pad(tensor=x, paddings=[[0, 0], [2, 1], [2, 1], [0, 0]], mode="CONSTANT")
+        if ksize == 3:
+            input = tf.pad(tensor=x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode="CONSTANT")
+        input1 = tf.pad(tensor=x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode="CONSTANT")
+        padding = 'valid'
+    else:
+        padding = 'same'
+        input = x
+        input1 = x
+    with tf.variable_scope(name):
+
+        # feature
+        conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, padding=padding)
+        out_f = tf.nn.leaky_relu(conv_f)
+
+        # Soft mask gating
+        conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1)
+        conv_g_1 = tf.nn.leaky_relu(conv_g_1)
+        conv_g_2 = tf.layers.separable_conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, padding=padding)
+        conv_g_2 = tf.nn.leaky_relu(conv_g_2)
+        conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1)
+        conv_g_3 = tf.nn.sigmoid(conv_g_3)
+
+       # Soft guide gating
+        conv_g1_1 = tf.layers.separable_conv2d(inputs=input1, filters=cnum, kernel_size=3, strides=stride, padding=padding)
+        conv_g1_1 = tf.nn.sigmoid(conv_g1_1)
+
+        # concat the gating
+        conv_g = tf.concat([conv_g_3, conv_g1_1], axis=3)
+        conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g = tf.nn.sigmoid(conv_g)
+
+        # Elementwise Multiply
+        x = out_f * conv_g
+        return x
+
+
+@add_arg_scope
+def resnetblock(x, cnum, reuse = tf.AUTO_REUSE, name = 'res'):
+    with tf.variable_scope(name_or_scope=name, reuse=reuse):
+        x_1 = gen_gate_conv(x, cnum, 3, name='conv1')
+        # x_1 = tf.layers.batch_normalization(x_1, name='bn1')
+        x_1 = tf.nn.relu(x_1)
+        x_1 = gen_gate_conv(x_1, cnum, 3, name='conv2')
+        # x_1 = tf.layers.batch_normalization(x_1, name='bn2')
+        out = x + x_1
+        return out
+
+@add_arg_scope
+def gate_res_block(x, cnum, name):
+    with tf.variable_scope(name_or_scope=name):
+        x_1 = gen_gate_conv(x, cnum = cnum, ksize=3, name='conv1')
+        x_1 = tf.layers.batch_normalization(x_1)
+        x_1 = leaky_relu(x_1)
+
+        x_1 = gen_gate_conv(x_1, cnum = cnum, ksize=3, name='conv2')
+        x_1 = tf.layers.batch_normalization(x_1)
+
+        out = x + x_1
+        out = leaky_relu(out)
+        return out
+
+@add_arg_scope
+# v7 2gateCon
+def v7gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    with tf.variable_scope(name):
+        # feature
+        conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+        out_f = leaky_relu(conv_f)
+
+        # Soft 5*5 gating
+        conv_g5 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g5 = leaky_relu(conv_g5)
+        conv_g5 = tf.layers.separable_conv2d(inputs=conv_g5, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+        conv_g5 = tf.nn.sigmoid(conv_g5)
+
+        # Soft 3*3 gating
+        conv_g3 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g3 = leaky_relu(conv_g3)
+        conv_g3 = tf.layers.separable_conv2d(inputs=conv_g3, filters=cnum, kernel_size=3, padding='same',  strides=stride, dilation_rate=rate)
+        conv_g3 = tf.nn.sigmoid(conv_g3)
+
+        # Soft 1*1 gating
+        conv_g1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g1 = leaky_relu(conv_g1)
+        conv_g1 = tf.layers.separable_conv2d(inputs=conv_g1, filters=cnum, kernel_size=1, padding='same',  strides=stride, dilation_rate=1)
+        conv_g1 = tf.nn.tanh(conv_g1)
+
+        # concat the gating
+        conv_g = tf.concat([conv_g5, conv_g3], axis=3)
+        conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+        conv_g = tf.nn.sigmoid(conv_g)
+        # conv_g = leaky_relu(conv_g)
+        # conv_g = clip_by_value(conv_g, 0., 1.)
+
+        # Elementwise Multiply
+        with tf.variable_scope(name_or_scope='mul'):
+            res_mul = tf.multiply(out_f, conv_g)
+        with tf.variable_scope(name_or_scope='add'):
+            x = tf.add(res_mul, conv_g1)
+    return x
+
+
+@add_arg_scope
+# v6 2gateCon
+def _gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    pad = ksize // 2 * rate
+    input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    conv_f = tf.layers.separable_conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    out_f = leaky_relu(conv_f)
+    # Soft mask gating
+    conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g_1 = leaky_relu(conv_g_1)
+    conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    conv_g_2 = leaky_relu(conv_g_2)
+    conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1, dilation_rate=rate)
+    conv_g_3 = tf.nn.sigmoid(conv_g_3)
+    # Soft guide gating
+    conv_g1_1 = tf.layers.separable_conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+    conv_g1_1 = tf.nn.sigmoid(conv_g1_1)
+    # concat the gating
+    conv_g = tf.concat([conv_g_3, conv_g1_1], axis=3)
+    conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = tf.nn.sigmoid(conv_g)
+    # Elementwise Multiply
+    x = out_f * conv_g
+    return x
+
+@add_arg_scope
+def gen_1gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    with tf.name_scope(name=name):
+        pad = ksize // 2 * rate
+        input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+
+        conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        # out_f = tf.nn.elu(conv_f, alpha=0.2)
+        # out_f = tf.nn.relu(conv_f)
+        # out_f = tf.nn.elu(conv_f)
+        out_f = leaky_relu(conv_f)
+
+        conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        conv_g_1 = tf.nn.sigmoid(conv_g_1)
+        x = out_f * conv_g_1
+    return x
+
+def gen_result_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.relu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    with tf.name_scope(name=name):
+        pad = ksize // 2 * rate
+        input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+
+        conv_f = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        # out_f = tf.nn.elu(conv_f, alpha=0.2)
+        # out_f = tf.nn.elu(conv_f)
+        out_f = leaky_relu(conv_f)
+
+        conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=ksize, strides=stride, dilation_rate=rate)
+        conv_g_1 = tf.nn.sigmoid(conv_g_1)
+    return out_f, conv_g_1
+
+@add_arg_scope
+def gen_resize(x, cnum, name='upsample', padding='same', training=True):
+    """Define deconv for generator.
+    The deconv is defined to be a x2 resize_nearest_neighbor operation with
+    additional gen_conv operation.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        name: Name of layers.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    with tf.variable_scope(name):
+        x = resize(x, func=tf.image.resize_bilinear)
+        x = tf.layers.conv2d(
+            x, cnum, 3, 1, dilation_rate=1,
+            padding=padding, name=name)
+        x = leaky_relu(x)
+    return x
+
+
+@add_arg_scope
+def gen_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=tf.nn.elu, training=True):
+    """Define conv for generator.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        Rate: Rate for or dilated conv.
+        name: Name of layers.
+        padding: Default to SYMMETRIC.
+        activation: Activation function after convolution.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    assert padding in ['SYMMETRIC', 'SAME', 'REFELECT']
+    if padding == 'SYMMETRIC' or padding == 'REFELECT':
+        p = int(rate*(ksize-1)/2)
+        x = tf.pad(x, [[0,0], [p, p], [p, p], [0,0]], mode=padding)
+        padding = 'VALID'
+    x = tf.layers.conv2d(
+        x, cnum, ksize, stride, dilation_rate=rate,
+        padding=padding, name=name)
+    x = leaky_relu(x)
+    return x
+
+
+@add_arg_scope
+def gen_deconv(x, cnum, kernel_size = 3, scale_size = 2, stride = 2, name='upsample', pad='SAME', training=True):
+    """Define deconv for generator.
+    The deconv is defined to be a x2 resize_nearest_neighbor operation with
+    additional gen_conv operation.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        name: Name of layers.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    b, w, h, c = x.shape[0], x.shape[1], x.shape[2], x.shape[3]
+    b, w, h, c = int(b), int(w), int(h), int(c)
+    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
+        filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+        bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+        data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], pad)
+        data = tf.nn.bias_add(data, bias)
+        data = tf.nn.relu(data)
+        # data = tf.sigmoid(data)
+        # data = leaky_relu(data, alpha=0.1)
+
+        filt_gate = tf.get_variable('conv2d_transpose_gate/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+        bias_gate = tf.get_variable('conv2d_transpose_gate/bias', [cnum], tf.float32)
+        data_gate = tf.nn.conv2d_transpose(x, filt_gate, [b, scale_size * w, scale_size * h, c], [1, stride, stride, 1], pad)
+        data_gate = tf.nn.bias_add(data_gate, bias_gate)
+        data_gate = tf.sigmoid(data_gate)
+
+        x = data * data_gate
+        return x
+
+@add_arg_scope
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, name='upsample', padding='SAME', training=True):
+    """Define deconv for generator.
+    The deconv is defined to be a x2 resize_nearest_neighbor operation with
+    additional gen_conv operation.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        name: Name of layers.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    b, w, h, c = x.shape[0], x.shape[1], x.shape[2], x.shape[3]
+    b, w, h, c = int(b), int(w), int(h), int(c)
+    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
+        #filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+        #bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+        #data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], padding)
+        #data = tf.nn.bias_add(data, bias)
+        #data = tf.sigmoid(data)
+
+        data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, 'same')
+        data = leaky_relu(data)
+        return data
+
+@add_arg_scope
+def gen_normal_deconv_pad(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, name='upsample', padding='SAME', training=True):
+    """Define deconv for generator.
+    The deconv is defined to be a x2 resize_nearest_neighbor operation with
+    additional gen_conv operation.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        name: Name of layers.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    b, w, h, c = x.shape[0], x.shape[1], x.shape[2], x.shape[3]
+    b, w, h, c = int(b), int(w), int(h), int(c)
+    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
+        #filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+        #bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+        #data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], padding)
+        #data = tf.nn.bias_add(data, bias)
+        #data = tf.sigmoid(data)
+
+        data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, 'same')
+        data = tf.nn.leaky_relu(data)
+        return data
+
+@add_arg_scope
+def dis_conv(x, cnum, ksize=5, stride=2, name='conv', training=True):
+    """Define conv for discriminator.
+    Activation is set to leaky_relu.
+
+    Args:
+        x: Input.
+        cnum: Channel number.
+        ksize: Kernel size.
+        Stride: Convolution stride.
+        name: Name of layers.
+        training: If current graph is for training or inference, used for bn.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    x = tf.layers.conv2d(x, cnum, ksize, stride, 'SAME', name=name)
+    x = tf.nn.leaky_relu(x)
+    return x
+
+def contextual_attention(f, b, mask=None, ksize=3, stride=1, rate=1,
+                         fuse_k=3, softmax_scale=10., training=True, fuse=True):
+    """ Contextual attention layer implementation.
+
+    Contextual attention is first introduced in publication:
+        Generative Image Inpainting with Contextual Attention, Yu et al.
+
+    Args:
+        x: Input feature to match (foreground).
+        t: Input feature for match (background).
+        mask: Input mask for t, indicating patches not available.
+        ksize: Kernel size for contextual attention.
+        stride: Stride for extracting patches from t.
+        rate: Dilation for matching.
+        softmax_scale: Scaled softmax for attention.
+        training: Indicating if current graph is training or inference.
+
+    Returns:
+        tf.Tensor: output
+
+    """
+    # expand mask dim numbers
+    # while cnum%2 == 0 and cnum != 0:
+    #     mask = tf.concat([mask, mask], axis=3)
+    #     cnum = cnum/2
+
+    # get shapes
+    raw_int_fs = f.get_shape().as_list()
+    raw_int_bs = b.get_shape().as_list()
+    raw_int_m = mask.get_shape().as_list()
+
+    # extract patches from background with stride and rate
+    kernel = 2*rate
+    raw_w = tf.extract_image_patches(
+        b, [1,kernel,kernel,1], [1,rate*stride,rate*stride,1], [1,1,1,1], padding='SAME')
+    raw_w = tf.reshape(raw_w, [raw_int_bs[0], -1, kernel, kernel, raw_int_bs[3]])
+    raw_w = tf.transpose(raw_w, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw (B,4,4,128,32*32)
+    raw_w_groups = tf.split(raw_w, raw_int_bs[0], axis=0)
+
+    # downscaling foreground option: downscaling both foreground and
+    # background for matching and use original background for reconstruction.
+    f = resize(f, scale=1./rate, func=tf.image.resize_nearest_neighbor)
+    b = resize(b, scale=1./rate, func=tf.image.resize_nearest_neighbor)
+    mask = resize(mask, scale=1. / rate, func=tf.image.resize_nearest_neighbor)
+
+    # get downsacling foreground shapes
+    fs = f.get_shape().as_list()
+    f_groups = tf.split(f, fs[0], axis=0)  # from t(H*W*C) to w(b*k*k*c*h*w)
+
+    # get downscaling background and extract patches
+    bs = b.get_shape().as_list()
+    w = tf.extract_image_patches(
+        b, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')
+    w = tf.reshape(w, [fs[0], -1, ksize, ksize, fs[3]])
+    w = tf.transpose(w, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw (B,3,3,128,32*32)
+    w_groups = tf.split(w, bs[0], axis=0)
+
+    # process mask
+    m = tf.extract_image_patches(
+        mask, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')
+    m = tf.reshape(m, [raw_int_m[0], -1, ksize, ksize, raw_int_m[3]])
+    m = tf.transpose(m, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw (B,3,3,1,32*32)
+    m = m[0]
+    # mm = tf.cast(tf.equal(m,  0.), tf.float32) #(3,3,1,32*32)
+    mm = tf.cast(tf.equal(tf.reduce_mean(m, axis=[0, 1, 2], keep_dims=True), 0.), tf.float32) #(1,1,1,32*32)
+
+    y = []
+    k = fuse_k
+    scale = softmax_scale
+    fuse_weight = tf.reshape(tf.eye(k), [k, k, 1, 1])
+
+    for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):
+        # conv for compare
+        wi = wi[0]
+        wi_normed = wi / tf.maximum(tf.sqrt(tf.reduce_sum(tf.square(wi), axis=[0,1,2])), 1e-4)
+        yi = tf.nn.conv2d(xi, wi_normed, strides=[1,1,1,1], padding="SAME") # yi => (B=1, H=32, W=32. C=32*32)
+
+        # conv implementation for fuse scores to encourage large patches
+        if fuse:
+            yi = tf.reshape(yi, [1, fs[1]*fs[2], bs[1]*bs[2], 1]) # make all of depth to spatial resolution, (B=1, H=32*32, W=32*32, I=1)
+            yi = tf.nn.conv2d(yi, fuse_weight, strides=[1,1,1,1], padding='SAME') # (B=1, H=32*32, W=32*32, C=1)
+            yi = tf.reshape(yi, [1, fs[1], fs[2], bs[1], bs[2]]) # (B=1, 32, 32, 32, 32)
+            yi = tf.transpose(yi, [0, 2, 1, 4, 3])
+            yi = tf.reshape(yi, [1, fs[1]*fs[2], bs[1]*bs[2], 1])
+            yi = tf.nn.conv2d(yi, fuse_weight, strides=[1,1,1,1], padding='SAME')
+            yi = tf.reshape(yi, [1, fs[2], fs[1], bs[2], bs[1]])
+            yi = tf.transpose(yi, [0, 2, 1, 4, 3])
+        yi = tf.reshape(yi, [1, fs[1], fs[2], bs[1]*bs[2]]) # (B=1, H=32, W=32, C=32*32)
+
+        # softmax to match
+        yi *=  mm  # mask => (1, 1, 1, 32*32)
+        yi = yi*scale
+        yi = tf.nn.softmax(yi, 3)
+        yi *=  mm  # mask
+
+        # offset = tf.argmax(yi, axis=3, output_type=tf.int32)
+        # offset = tf.stack([offset // fs[2], offset % fs[2]], axis=-1)
+        # deconv for patch pasting
+        # 3.1 paste center
+        wi_center = raw_wi[0]
+        # output_shape = tf.concat([[1], raw_int_fs[1:]], axis=0)
+        yi = tf.nn.conv2d_transpose(yi, wi_center, tf.concat([[1], raw_int_fs[1:]], axis=0), strides=[1,rate,rate,1]) / 4. # (B=1, H=64, W=64, C=128)
+        y.append(yi)
+        # offsets.append(offset)
+    y = tf.concat(y, axis=0)
+    y.set_shape(raw_int_fs)
+    # offsets = tf.concat(offsets, axis=0)
+    # offsets.set_shape(int_bs[:3] + [2])
+    # case1: visualize optical flow: minus current position
+    # h_add = tf.tile(tf.reshape(tf.range(bs[1]), [1, bs[1], 1, 1]), [bs[0], 1, bs[2], 1])
+    # w_add = tf.tile(tf.reshape(tf.range(bs[2]), [1, 1, bs[2], 1]), [bs[0], bs[1], 1, 1])
+    # offsets = offsets - tf.concat([h_add, w_add], axis=3)
+    # to flow image
+    # flow = flow_to_image_tf(offsets)
+    # # case2: visualize which pixels are attended
+    # flow = highlight_flow_tf(offsets * tf.cast(mask, tf.int32))
+    # if rate != 1:
+    #     flow = resize(flow, scale=rate, func=tf.image.resize_nearest_neighbor)
+    return y #, flow
+
+def test_contextual_attention(args):
+    """Test contextual attention layer with 3-channel image input
+    (instead of n-channel feature).
+
+    """
+    import cv2
+    import os
+    # run on cpu
+    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+
+    rate = 2
+    stride = 1
+    grid = rate*stride
+
+    b = cv2.imread(args.imageA)
+    b = cv2.resize(b, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_CUBIC)
+    h, w, _ = b.shape
+    b = b[:h//grid*grid, :w//grid*grid, :]
+    b = np.expand_dims(b, 0)
+    logger.info('Size of imageA: {}'.format(b.shape))
+
+    f = cv2.imread(args.imageB)
+    h, w, _ = f.shape
+    f = f[:h//grid*grid, :w//grid*grid, :]
+    f = np.expand_dims(f, 0)
+    logger.info('Size of imageB: {}'.format(f.shape))
+
+    with tf.Session() as sess:
+        bt = tf.constant(b, dtype=tf.float32)
+        ft = tf.constant(f, dtype=tf.float32)
+
+        yt, flow = contextual_attention(
+            ft, bt, stride=stride, rate=rate,
+            training=False, fuse=False)
+        y = sess.run(yt)
+        cv2.imwrite(args.imageOut, y[0])
+
+
+def make_color_wheel():
+    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)
+    ncols = RY + YG + GC + CB + BM + MR
+    colorwheel = np.zeros([ncols, 3])
+    col = 0
+    # RY
+    colorwheel[0:RY, 0] = 255
+    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))
+    col += RY
+    # YG
+    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))
+    colorwheel[col:col+YG, 1] = 255
+    col += YG
+    # GC
+    colorwheel[col:col+GC, 1] = 255
+    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))
+    col += GC
+    # CB
+    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))
+    colorwheel[col:col+CB, 2] = 255
+    col += CB
+    # BM
+    colorwheel[col:col+BM, 2] = 255
+    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))
+    col += + BM
+    # MR
+    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))
+    colorwheel[col:col+MR, 0] = 255
+    return colorwheel
+
+
+COLORWHEEL = make_color_wheel()
+
+
+def compute_color(u,v):
+    h, w = u.shape
+    img = np.zeros([h, w, 3])
+    nanIdx = np.isnan(u) | np.isnan(v)
+    u[nanIdx] = 0
+    v[nanIdx] = 0
+    # colorwheel = COLORWHEEL
+    colorwheel = make_color_wheel()
+    ncols = np.size(colorwheel, 0)
+    rad = np.sqrt(u**2+v**2)
+    a = np.arctan2(-v, -u) / np.pi
+    fk = (a+1) / 2 * (ncols - 1) + 1
+    k0 = np.floor(fk).astype(int)
+    k1 = k0 + 1
+    k1[k1 == ncols+1] = 1
+    f = fk - k0
+    for i in range(np.size(colorwheel,1)):
+        tmp = colorwheel[:, i]
+        col0 = tmp[k0-1] / 255
+        col1 = tmp[k1-1] / 255
+        col = (1-f) * col0 + f * col1
+        idx = rad <= 1
+        col[idx] = 1-rad[idx]*(1-col[idx])
+        notidx = np.logical_not(idx)
+        col[notidx] *= 0.75
+        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))
+    return img
+
+
+
+def flow_to_image(flow):
+    """Transfer flow map to image.
+    Part of code forked from flownet.
+    """
+    out = []
+    maxu = -999.
+    maxv = -999.
+    minu = 999.
+    minv = 999.
+    maxrad = -1
+    for i in range(flow.shape[0]):
+        u = flow[i, :, :, 0]
+        v = flow[i, :, :, 1]
+        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)
+        u[idxunknow] = 0
+        v[idxunknow] = 0
+        maxu = max(maxu, np.max(u))
+        minu = min(minu, np.min(u))
+        maxv = max(maxv, np.max(v))
+        minv = min(minv, np.min(v))
+        rad = np.sqrt(u ** 2 + v ** 2)
+        maxrad = max(maxrad, np.max(rad))
+        u = u/(maxrad + np.finfo(float).eps)
+        v = v/(maxrad + np.finfo(float).eps)
+        img = compute_color(u, v)
+        out.append(img)
+    return np.float32(np.uint8(out))
+
+
+def flow_to_image_tf(flow, name='flow_to_image'):
+    """Tensorflow ops for computing flow to image.
+    """
+    with tf.variable_scope(name), tf.device('/cpu:0'):
+        img = tf.py_func(flow_to_image, [flow], tf.float32, stateful=False)
+        img.set_shape(flow.get_shape().as_list()[0:-1]+[3])
+        img = img / 127.5 - 1.
+        return img
+
+
+def highlight_flow(flow):
+    """Convert flow into middlebury color code image.
+    """
+    out = []
+    s = flow.shape
+    for i in range(flow.shape[0]):
+        img = np.ones((s[1], s[2], 3)) * 144.
+        u = flow[i, :, :, 0]
+        v = flow[i, :, :, 1]
+        for h in range(s[1]):
+            for w in range(s[1]):
+                ui = u[h,w]
+                vi = v[h,w]
+                img[ui, vi, :] = 255.
+        out.append(img)
+    return np.float32(np.uint8(out))
+
+
+def highlight_flow_tf(flow, name='flow_to_image'):
+    """Tensorflow ops for highlight flow.
+    """
+    with tf.variable_scope(name), tf.device('/cpu:0'):
+        img = tf.py_func(highlight_flow, [flow], tf.float32, stateful=False)
+        img.set_shape(flow.get_shape().as_list()[0:-1]+[3])
+        img = img / 127.5 - 1.
+        return img
+
+
+def image2edge(image):
+    """Convert image to edges.
+    """
+    out = []
+    for i in range(image.shape[0]):
+        img = cv2.Laplacian(image[i, :, :, :], cv2.CV_64F, ksize=3, scale=2)
+        out.append(img)
+    return np.float32(np.uint8(out))
+
+def gaussian_blur(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    pad_w = 0
+    padded_img = tf.pad(image, [[0, 0], [pad_w, pad_w], [pad_w, pad_w], [0, 0]], mode='CONSTANT')
+    for channel_idx in range(cdim):
+        data_c = padded_img[:, :, :, channel_idx:(channel_idx + 1)]
+        # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+        g = kernel.reshape((kernel_size, kernel_size, 1, 1))
+        data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+        # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+        data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+        outputs.append(data_c)
+    k = kernel.reshape((kernel_size, kernel_size,1,1))
+    # output =
+
+    return tf.concat(outputs,axis=3)
+
+if __name__ == "__main__":
+    import argparse
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--imageA', default='', type=str, help='Image A as background patches to reconstruct image B.')
+    parser.add_argument('--imageB', default='', type=str, help='Image B is reconstructed with image A.')
+    parser.add_argument('--imageOut', default='result.png', type=str, help='Image B is reconstructed with image A.')
+    args = parser.parse_args()
+    test_contextual_attention(args)
Index: filelist_gen.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- filelist_gen.py	(date 1578357929000)
+++ filelist_gen.py	(date 1578357929000)
@@ -0,0 +1,114 @@
+import argparse
+import os
+from random import shuffle
+import datetime
+
+parser = argparse.ArgumentParser()
+
+#parser.add_argument('--folder_path', default='./training_data', type=str,
+#/common-data/109.105.1.121/data/data_for_training/ffhq_db_new/images1024x1024
+parser.add_argument('--folder_path', default='/mnt/disk/li.zuo/data/longmao_faces', type=str,
+                    help='The folder path')
+parser.add_argument('--file_tpye', default='jpg', type=str,
+                    help='image file type')
+parser.add_argument('--train_filename', default='longmao_faces.flist', type=str,
+                    help='The train filename.')
+parser.add_argument('--is_shuffled', default='0', type=int,
+                    help='Needed to be shuffled')
+
+if __name__ == "__main__":
+
+
+    args = parser.parse_args()
+    training_dirs = args.folder_path
+
+    a = ['airplane_cabin','airport_terminal','amusement_arcade','aquarium','archaelogical_excavation',
+                 'archive','arena','army_base','art_school','art_studio','assembly_line','athletic_field',
+                  'auditorium','auto_factory','auto_showroom']
+
+    b = ['ball_pit','ballroom','bar','baseball_field','basketball_court','bazaar','beauty_salon','beer_garden',
+         'beer_hall','biology_laboratory','boat_deck','bookstore','booth','bowling_alley','boxing_ring','bullring',
+         'bus_interior','bus_station','butchers_shop']
+    c = ['cafeteria','campsite','candy_store','carrousel','church','classroom','closet','clothing_store',
+         'cockpit','coffee_shop','conference_center','conference_room','construction_site']
+    d = ['delicatessen','department_store','diner','dining_hall','discotheque','dorm_room']
+    e = ['engine_room','excavation']
+    f = ['fire_station','flea_market','florist_shop','food_court','football_field']
+    g = ['general_store','gift_shop','greenhouse','gymnasium']
+    h = ['hardware_store','hospital','hospital_room']
+    i = ['ice_cream_parlor','ice_skating_rink','igloo']
+    j = ['jewelry_shop','junkyard']
+    k = ['kennel','kindergarden_classroom']
+    l = ['landfill','laundromat','lecture_room','legislative_chamber','library','locker_room']
+    m = ['market','martial_arts_gym','mezzanine','movie_theater','music_studio']
+    n = ['natural_history_museum','nursing_home']
+    o = ['office','office_cubicles','operating_room','orchestra_pit']
+    p = ['pantry','parking_lot','pet_shop','pharmacy','physics_laboratory','picnic_area','pizzeria','playground',
+         'playroom','plaza','pub']
+    r = ['racecourse','raceway','raft','reading_room','reception','recreation_room','repair_shop','restaurant',
+         'restaurant_kitchen','restaurant_patio','riding_arena']
+    s= ['sandbox','science_museum','server_room','shoe_shop','shopping_mall','ski_resort','ski_slope','slum',
+        'soccer_field','stable','stadium','stage','storage_room','subway_station','supermarket','sushi_bar',
+        'swimming_hole']
+    t = ['television_studio','ticket_booth','toyshop','tree_farm','trench']
+    v = ['veterinarians_office','volleyball_court']
+    w = ['waiting_room','water_park']
+    y = ['yard']
+    listDelet = a+b+c+d+e+f+g+h+i+j+k+l+m+n+o+p+r+s+t+v+w+y
+    training_file_names = []
+    count = 0
+    starttime = datetime.datetime.now()
+    img_w = os.walk(training_dirs, topdown=False)
+    print('Total delet list len: ', len(listDelet))
+    deletCount = 0
+    # for path, d, filelist in img_w:
+    #     flag = True
+    #     listPath = path.split('/')
+    #     if listPath[-1] in listDelet:
+    #         # print('Delet: ', listPath[-1])
+    #         flag = False
+    #         listDelet.remove(listPath[-1])
+    #         deletCount = deletCount + 1
+    #     elif listPath[-2] in listDelet:
+    #         # print('Delet: ', listPath[-2])
+    #         flag = False
+    #         listDelet.remove(listPath[-2])
+    #         deletCount = deletCount + 1
+    #     else:
+    #         flag = True
+    #
+    #     if flag:
+    #         for filename in filelist:
+    #             if filename.endswith(args.file_tpye):
+    #                 training_file_names.append(os.path.join(path, filename))
+    #                 count = count + 1
+
+    for path, d, filelist in img_w:
+        for filename in filelist:
+            if filename.endswith(args.file_tpye):
+               training_file_names.append(os.path.join(path, filename))
+
+    endtime = datetime.datetime.now()
+    print('Runing time: ', (endtime - starttime).seconds)
+    print('Total image number: ',count)
+    print('Deleted list len: ',deletCount)
+    # exit()
+    # shuffle file names if set
+    if args.is_shuffled == 1:
+        shuffle(training_file_names)
+        # shuffle(validation_file_names)
+
+    # make output file if not existed
+    if not os.path.exists(args.train_filename):
+        os.mknod(args.train_filename)
+
+
+    # write to file
+    training_file_names[-1] = training_file_names[-1]+ ("\n")
+    fo = open(args.train_filename, "a")
+    fo.write("\n".join(training_file_names))
+    fo.close()
+
+
+    # print process
+    print("Written file is: ", args.train_filename, ", is_shuffle: ", args.is_shuffled)
\ No newline at end of file
Index: batch_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- batch_test.py	(date 1576309950000)
+++ batch_test.py	(date 1576309950000)
@@ -0,0 +1,162 @@
+import os
+import time
+import pandas as pds
+os.environ["CUDA_VISIBLE_DEVICES"] = "1"
+import argparse
+import cv2
+import numpy as np
+import tensorflow as tf
+
+parser = argparse.ArgumentParser(description='training code')
+parser.add_argument('--test_path', type=str, default="/mnt/disk/mm.bai/inpaiting/paperModel/quantityCode/ffhq/",
+                    help='test_data_path')
+
+parser.add_argument('--mask_path', type=str, default="/mnt/disk/mm.bai/inpaiting/paperModel/quantityCode/ffhqmask/",
+                    help='mask_path')
+
+parser.add_argument('--landmark', type=str, default="/mnt/disk/mm.bai/inpaiting/paperModel/quantityCode/ffhqldmk/",
+                    help='result_path')
+
+parser.add_argument('--pbdir', type=str, default="/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/testcc-t2.pb",
+                    help='model_path')
+
+parser.add_argument('--file_out', type=str, default="/mnt/disk/mm.bai/inpaiting/paperModel/quantityCode/our_cc_ffhq/",
+                    help='result_path')
+
+parser.add_argument('--width', type=int, default=512, help='images width')
+parser.add_argument('--height', type=int, default=512, help='images height')
+args = parser.parse_args()
+
+def landmarkMask(label_dir):
+    try:
+        label = pds.read_csv(label_dir, header=None, sep=',')
+    except:
+        return 1
+    else:
+        label = label.values
+        landmark = []
+        for i in range(0, 68):
+            x = label[0][i * 2]
+            y = label[0][i * 2 + 1]
+            landmark.append([x, y])
+        label = np.array(landmark, dtype=int)
+
+        output = np.ones(shape=[1024, 1024, 1], dtype=np.uint8)
+
+        b = label[36:41, :]
+        s = []
+        s.append(b)
+        s = np.array(s)
+        cv2.fillPoly(output, s, (0, 0, 1))  # left eye
+
+        b = label[42:47, :]
+        s = []
+        s.append(b)
+        s = np.array(s)
+        cv2.fillPoly(output, s, (0, 0, 1))  # right eye
+
+        b = label[48:60, :]
+        s = []
+        s.append(b)
+        s = np.array(s)
+        cv2.fillPoly(output, s, (0, 0, 1))  # mouth
+
+        b = label[17:21, :]
+        s = []
+        s.append(b)
+        s = np.array(s)
+        cv2.fillPoly(output, s, (0, 0, 1))  # left brow
+
+        b = label[22:26, :]
+        s = []
+        s.append(b)
+        s = np.array(s)
+        cv2.fillPoly(output, s, (0, 0, 1))  # left brow
+
+        # b = label[27:30, :]
+        b_1 = label[30:35, :]
+        s = []
+        # s.append(b)
+        s.append(b_1)
+        s = np.array(s)
+        cv2.fillPoly(output, s, (0, 0, 1))  # nose
+        return output
+
+def readImg1(image, mask, landmark):
+    image = cv2.imread(image)
+    mask = cv2.imread(mask)
+    ladmk = landmarkMask(landmark)
+
+    image = cv2.resize(image, (args.height, args.width))
+    mask = cv2.resize(mask, (args.height, args.width))
+    ladmk = cv2.resize(ladmk, (args.height, args.width))
+    h, w, _ = image.shape
+    grid = 8
+    image = image[:h // grid * grid, :w // grid * grid, :]
+    mask = mask[:h // grid * grid, :w // grid * grid, :]
+
+    image = (image / 127.5 - 1.).astype(np.float32)
+    mask = (mask > 127.5).astype(np.float32)
+    ladmk = np.expand_dims(ladmk, 2)
+
+    mask = mask*ladmk
+
+    image = np.expand_dims(image, 0)
+    mask = np.expand_dims(mask, 0)
+    image = image * (1. - mask)
+
+    input_image = np.concatenate([image, mask[:, :, :, 0:1]], axis=3)
+    return input_image
+
+def readImg2(image, mask):
+    image = cv2.imread(image)
+    mask = cv2.imread(mask)
+
+    image = cv2.resize(image, (args.height, args.width))
+    mask = cv2.resize(mask, (args.height, args.width))
+    h, w, _ = image.shape
+    grid = 8
+    image = image[:h // grid * grid, :w // grid * grid, :]
+    mask = mask[:h // grid * grid, :w // grid * grid, :]
+
+    image = (image / 127.5 - 1.).astype(np.float32)
+    mask = (mask > 127.5).astype(np.float32)
+
+    image = np.expand_dims(image, 0)
+    mask = np.expand_dims(mask, 0)
+    image = image * (1. - mask)
+
+    input_image = np.concatenate([image, mask[:, :, :, 0:1]], axis=3)
+    return input_image
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+    if not os.path.exists(args.file_out):
+        os.mkdir(args.file_out)
+
+    with tf.Graph().as_default():
+        output_graph_def = tf.GraphDef()
+        with open(args.pbdir, "rb") as f:
+            output_graph_def.ParseFromString(f.read())
+            _ = tf.import_graph_def(output_graph_def, name="")
+        config = tf.ConfigProto(allow_soft_placement=True)
+        with tf.Session(config=config) as sess:
+            sess.run(tf.global_variables_initializer())
+            input_image_tensor1 = sess.graph.get_tensor_by_name("input:0")
+            output = sess.graph.get_tensor_by_name("output:0")
+            output = tf.clip_by_value(output, -1., 1.)
+            B,G,R, _mask = tf.split(input_image_tensor1, axis=3, num_or_size_splits=4)
+            img = tf.concat([B,G,R], axis=3)
+            output_tensor = img*(1.-_mask) + output*_mask
+            output_tensor = (output_tensor + 1.) * 127.5
+            output_tensor = tf.saturate_cast(output_tensor, tf.uint8)
+            for i in range(0, 100):
+                img_path = args.test_path + '{}.jpg'.format(i)
+                mask_path = args.mask_path + '{}msk.jpg'.format(i)
+                landmark = args.landmark +  '{}.txt'.format(i)
+                test_dt = readImg1(img_path, mask_path, landmark)
+                # test_dt = readImg2(img_path, mask_path)
+                result = sess.run([output_tensor], feed_dict={input_image_tensor1: test_dt})
+                img_output = result[0][-1]
+                cv2.imwrite(args.file_out + str(i)+'.jpg', img_output)
+                print('save {}.jpg'.format(i))
\ No newline at end of file
Index: inpaint_gan_loss.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_gan_loss.py	(date 1578357929000)
+++ inpaint_gan_loss.py	(date 1578357929000)
@@ -0,0 +1,279 @@
+import tensorflow as tf
+from inpaint_layer import *
+
+class InpaintGAN_net():
+    def __init__(self):
+        super().__init__()
+
+    def build_maskD_L(self, input_r, input_f, mask, reuse):
+        with tf.variable_scope('maskD_L', reuse=reuse):
+            ones_s = tf.ones_like(input_r)[:, :, :, 0:1]
+            # mask = mask * ones_real
+            x_r = tf.concat([input_r, ones_s, mask], axis=3)
+            x_f = tf.concat([input_f, ones_s, mask], axis=3)
+            x = tf.concat([x_r, x_f], axis=0)
+            nc = 64
+            x = maskConv(x, nc, ksize=3, stride=2)
+            x = maskConv(x, 2 * nc, ksize=3, stride=2)
+            x = maskConv(x, 4 * nc, ksize=3, stride=2)
+            x = maskConv(x, 8 * nc, ksize=3, stride=1)
+            x = maskConv(x, 1, ksize=3, stride=1)
+            x = tf.nn.sigmoid(x)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+            return real, fake
+
+
+
+    def build_maskD_G(self, input_r, input_f, mask, reuse):
+        with tf.variable_scope('maskD_G', reuse=reuse):
+            ones_s = tf.ones_like(input_r)[:, :, :, 0:1]
+            # mask = mask * ones_real
+            x_r = tf.concat([input_r, ones_s, mask], axis=3)
+            x_f = tf.concat([input_f, ones_s, mask], axis=3)
+            x = tf.concat([x_r, x_f], axis=0)
+            nc = 32
+            x = maskConv(x, nc, ksize=3, stride=2)
+            x = maskConv(x, 2 * nc, ksize=3, stride=2)
+            x = maskConv(x, 4 * nc, ksize=3, stride=1)
+            x = maskConv(x, 8 * nc, ksize=3, stride=1)
+            x = maskConv(x, 1, ksize=3, stride=1)
+            x = tf.nn.sigmoid(x)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+            return real, fake
+
+
+    def build_maskDC_discriminator(self, realG, fakeG, realL, fakeL, mask, reuse=False, training=True):
+        with tf.variable_scope('Dnet_G', reuse=reuse):
+            g_pos, g_neg = self.build_maskD_G(realG, fakeG, mask, reuse=tf.AUTO_REUSE)
+        with tf.variable_scope('Dnet_L', reuse=reuse):
+            l_pos, l_neg = self.build_maskD_L(realL, fakeL, mask, reuse=tf.AUTO_REUSE)
+            return l_pos, l_neg, g_pos, g_neg
+
+    def build_SN_G_L_discriminator(self, realG, fakeG, realL, fakeL, mask, reuse=False, training=True):
+        with tf.variable_scope('Dnet_G', reuse=reuse):
+            g_pos, g_neg = self.build_SN_G(realG, fakeG, reuse=tf.AUTO_REUSE)
+        with tf.variable_scope('Dnet_L', reuse=reuse):
+            l_pos, l_neg = self.build_SN_L(realL, fakeL, reuse=tf.AUTO_REUSE)
+            return l_pos, l_neg, g_pos, g_neg
+
+    def build_maskD_L_guide(self, input_r, input_f, mask, guide, reuse=False):
+        with tf.variable_scope('maskD_L_guide', reuse=reuse):
+            ones_s = tf.ones_like(input_r)[:, :, :, 0:1]
+            # mask = mask * ones_real
+            x_r = tf.concat([input_r, ones_s, mask, guide], axis=3)
+            x_f = tf.concat([input_f, ones_s, mask, guide], axis=3)
+            x = tf.concat([x_r, x_f], axis=0)
+            nc = 32
+            x = maskConv(x, nc, ksize=3, stride=2)
+            x = maskConv(x, 2 * nc, ksize=3, stride=2)
+            x = maskConv(x, 4 * nc, ksize=3, stride=1)
+            x = maskConv(x, 8 * nc, ksize=3, stride=1)
+            x = maskConv(x, 1, ksize=3, stride=1)
+            # x = tf.nn.sigmoid(x)
+            x = leaky_relu(x)
+            x = tf.clip_by_value(x, -1., 1.)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+            return real, fake
+
+
+    def build_maskD_G_guide(self, input_r, input_f, mask, guide, reuse=False):
+        with tf.variable_scope('maskD_G', reuse=reuse):
+            ones_s = tf.ones_like(input_r)[:, :, :, 0:1]
+            # mask = mask * ones_real
+            x_r = tf.concat([input_r, ones_s, mask, guide], axis=3)
+            x_f = tf.concat([input_f, ones_s, mask, guide], axis=3)
+            x = tf.concat([x_r, x_f], axis=0)
+            nc = 32
+            x = maskConv(x, nc, ksize=3, stride=2)
+            x = maskConv(x, 2 * nc, ksize=3, stride=2)
+            x = maskConv(x, 4 * nc, ksize=3, stride=1)
+            x = maskConv(x, 8 * nc, ksize=3, stride=1)
+            x = maskConv(x, 1, ksize=3, stride=1)
+            # x = tf.nn.sigmoid(x)
+            x = leaky_relu(x)
+            x = tf.clip_by_value(x, -1., 1.)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+            return real, fake
+
+    def build_SN_G(self, x_r, x_f, reuse):
+        with tf.variable_scope('SN_G', reuse=reuse):
+            x = tf.concat([x_r, x_f], axis=0)
+            ch = 64
+            x = SNconv(x, channels=ch, kernel=5, stride=2, pad=2, scope='conv_0')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=2*ch, kernel=5, stride=2, pad=2, scope='conv_1')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=4*ch, kernel=3, stride=2, pad=2, scope='conv_2')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=8*ch, kernel=3, stride=1, pad=2, scope='conv_3')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=3, kernel=3, stride=1, pad=1, scope='conv_4')
+            # x = tf.nn.sigmoid(x)
+            # x = leaky_relu(x)
+            x = tf.nn.tanh(x)
+            # x = tf.clip_by_value(x, -1., 1.)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+        return real, fake
+
+    def build_SN_L(self, x_r, x_f, reuse):
+        with tf.variable_scope('SN_L', reuse=reuse):
+            x = tf.concat([x_r, x_f], axis=0)
+            ch = 16
+            x = SNconv(x, channels=ch, kernel=3, stride=2, pad=2, scope='conv_0')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=2 * ch, kernel=3, stride=2, pad=2, scope='conv_1')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=4 * ch, kernel=3, stride=1, pad=1, scope='conv_2')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=8 * ch, kernel=3, stride=1, pad=1, scope='conv_3')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=1, kernel=3, stride=1, pad=1, scope='conv_4')
+            # x = tf.nn.sigmoid(x)
+            x = leaky_relu(x)
+            x = tf.clip_by_value(x, -1., 1.)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+        return real, fake
+
+    def build_SN_DC_rg_pair(self, rg, rl, fl, reuse):
+        with tf.variable_scope('DC_pair_rg', reuse=reuse):
+            # mask = mask * ones_real
+            x_r = tf.concat([rg, rl], axis=3)
+            x_f = tf.concat([rg, fl], axis=3)
+            x = tf.concat([x_r, x_f], axis=0)
+            ch = 32
+            x = SNconv(x, channels=ch, kernel=3, stride=2, pad=2, scope='conv_0')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=2 * ch, kernel=3, stride=2, pad=2, scope='conv_1')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=4 * ch, kernel=3, stride=2, pad=2, scope='conv_2')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=8 * ch, kernel=3, stride=1, pad=1, scope='conv_3')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=1, kernel=3, stride=1, pad=1, scope='conv_4')
+            # x = tf.nn.sigmoid(x)
+            x = leaky_relu(x)
+            x = tf.clip_by_value(x, -1., 1.)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+            return real, fake
+
+    def build_SN_DC_rl_pair(self, rl, rg, fg, reuse):
+        with tf.variable_scope('DC_pair_rg1', reuse=reuse):
+            # mask = mask * ones_real
+            x_r = tf.concat([rg, rl], axis=3)
+            x_f = tf.concat([fg, rl], axis=3)
+            x = tf.concat([x_r, x_f], axis=0)
+            ch = 32
+            x = SNconv(x, channels=ch, kernel=7, stride=2, pad=2, scope='conv_0')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=2 * ch, kernel=5, stride=2, pad=2, scope='conv_1')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=4 * ch, kernel=5, stride=2, pad=2, scope='conv_2')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=8 * ch, kernel=3, stride=1, pad=2, scope='conv_3')
+            x = leaky_relu(x)
+            x = SNconv(x, channels=3, kernel=3, stride=1, pad=1, scope='conv_4')
+            # x = tf.nn.sigmoid(x)
+            # x = leaky_relu(x)
+            # x = tf.clip_by_value(x, -1., 1.)
+            x = tf.nn.tanh(x)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+            return real, fake
+
+    def build_SN_pair_discriminator(self, realG, fakeG, realL, fakeL, reuse=False):
+        with tf.variable_scope('Dnet_RG', reuse=reuse):
+            rg_pair_pos, rg_pair_neg = self.build_SN_DC_rg_pair(realG, realL, fakeL, reuse=tf.AUTO_REUSE)
+        with tf.variable_scope('Dnet_RL', reuse=reuse):
+            rl_pair_pos, rl_pair_neg = self.build_SN_DC_rl_pair(fakeG, realL, fakeL, reuse=tf.AUTO_REUSE)
+            return rg_pair_pos, rg_pair_neg, rl_pair_pos, rl_pair_neg
+
+    def build_SN_pair_discriminator(self, realG, realL, fakeL, reuse=False):
+        with tf.variable_scope('Dnet_RG', reuse=reuse):
+            rg_pair_pos, rg_pair_neg = self.build_SN_G(realG, realG,  reuse=tf.AUTO_REUSE)
+        with tf.variable_scope('Dnet_RL', reuse=reuse):
+            rl_pair_pos, rl_pair_neg = self.build_SN_L(realL, fakeL, reuse=tf.AUTO_REUSE)
+            return rg_pair_pos, rg_pair_neg, rl_pair_pos, rl_pair_neg
+
+    def build_SN_discriminator(self, realL, fakeL, reuse=False):
+        with tf.variable_scope('Dnet_RL', reuse=reuse):
+            rl_pair_pos, rl_pair_neg = self.build_SN_L(realL, fakeL, reuse=tf.AUTO_REUSE)
+            return rl_pair_pos, rl_pair_neg
+
+
+
+    def build_SN_G_discriminator(self, realG, fakeG, name='Dnet_G' ,reuse=False):
+        with tf.variable_scope(name_or_scope=name, reuse=reuse):
+            g_pos, g_neg = self.build_SN_G(realG, fakeG,  reuse=tf.AUTO_REUSE)
+        return g_pos, g_neg
+
+    def build_SN_L_discriminator(self, realL, fakeL, name='Dnet_L' ,reuse=False):
+        with tf.variable_scope(name_or_scope=name, reuse=reuse):
+            l_pos, l_neg = self.build_SN_L(realL, fakeL,  reuse=tf.AUTO_REUSE)
+        return l_pos, l_neg
+
+    def build_SN_pair_RG_discriminator(self, realG, realL, fakeL, name='Dnet_RG',reuse=False):
+        with tf.variable_scope(name_or_scope=name, reuse=reuse):
+            rl_pair_pos, rl_pair_neg = self.build_SN_DC_rg_pair(realG, realL, fakeL, reuse=tf.AUTO_REUSE)
+            return rl_pair_pos, rl_pair_neg
+
+    def build_SN_pair_RL_discriminator(self, realL, realG, fakeG, name='Dnet_RL',reuse=False):
+        with tf.variable_scope(name_or_scope=name, reuse=reuse):
+            rg_pair_pos, rg_pair_neg = self.build_SN_DC_rl_pair(realL, realG, fakeG, reuse=tf.AUTO_REUSE)
+            return rg_pair_pos, rg_pair_neg
+
+    def batch_norm(slef, x, is_training=True, scope='batch_norm'):
+        return tf.contrib.layers.batch_norm(x,
+                                            decay=0.9, epsilon=1e-05,
+                                            center=True, scale=True, updates_collections=None,
+                                            is_training=is_training, scope=scope)
+
+    def maskD_loss(self, pos, neg, name='maskD_loss'):
+        with tf.variable_scope(name):
+            # d_loss = tf.maximum(neg, 0) - neg*pos + tf.log( (1+tf.exp(-tf.abs(neg))) )
+            d_loss = - (tf.reduce_mean(tf.log(pos + epsilon)) +
+                        tf.reduce_mean(tf.log(1 - neg + epsilon)))
+            g_loss = tf.reduce_mean(-tf.log(neg + epsilon))
+        return g_loss, d_loss
+
+    # To Hinge loss, the range of network output must be [-1, 1]
+    def Hinge_loss(self, pos, neg, name='Hinge_loss'):
+        with tf.variable_scope(name):
+            g_loss = - tf.reduce_mean(neg) + 1
+            d_loss = tf.reduce_mean(tf.nn.relu(1.0 - pos)) + tf.reduce_mean(tf.nn.relu(1.0 + neg))
+        return g_loss, d_loss
+
+    def gan_dcgan_loss(self, pos, neg, name='gan_dcgan_loss'):
+        with tf.variable_scope(name):
+            fake_logit = tf.nn.sigmoid(neg)
+            real_logit = tf.nn.sigmoid(pos)
+            d_loss = - (tf.reduce_mean(tf.log(real_logit + epsilon)) +
+                        tf.reduce_mean(tf.log(1 - fake_logit + epsilon)))
+            g_loss = - tf.reduce_mean(tf.log(fake_logit + epsilon))
+        return g_loss, d_loss
+
+    def build_edge_discriminator(self, real, fake, name='Dnet_dege',reuse=False):
+        with tf.variable_scope(name_or_scope=name, reuse=reuse):
+            pos, posf, neg, negf = self.build_edge_Dnet(real, fake, reuse=tf.AUTO_REUSE)
+            return pos, posf, neg, negf
+
+    def build_edge_Dnet(self, x_r, x_f, name='edge_D', reuse = False):
+        with tf.variable_scope(name_or_scope=name, reuse=reuse):
+            x = tf.concat([x_r, x_f], axis=0)
+            ch = 64
+            x = SNconv(x, channels=ch, kernel=4, stride=2, pad=1, scope='conv_0')
+            conv1 = leaky_relu(x)
+            x = SNconv(conv1, channels=2 * ch, kernel=4, stride=2, pad=1, scope='conv_1')
+            conv2 = leaky_relu(x)
+            x = SNconv(conv2, channels=4 * ch, kernel=4, stride=2, pad=1, scope='conv_2')
+            conv3 = leaky_relu(x)
+            x = SNconv(conv3, channels=8 * ch, kernel=4, stride=1, pad=1, scope='conv_3')
+            conv4 = leaky_relu(x)
+            conv5 = SNconv(conv4, channels=1, kernel=4, stride=1, pad=1, scope='conv_4')
+            x = tf.nn.sigmoid(x)
+            # x = tf.clip_by_value(conv5, -1., 1.)
+            real, fake = tf.split(x, num_or_size_splits=2, axis=0)
+            rconv1, fconv1 = tf.split(conv1, num_or_size_splits=2, axis=0)
+            rconv2, fconv2 = tf.split(conv2, num_or_size_splits=2, axis=0)
+            rconv3, fconv3 = tf.split(conv3, num_or_size_splits=2, axis=0)
+            rconv4, fconv4 = tf.split(conv4, num_or_size_splits=2, axis=0)
+            rconv5, fconv5 = tf.split(conv5, num_or_size_splits=2, axis=0)
+            return real, [rconv1, rconv2, rconv3, rconv4, rconv5], fake, [fconv1, fconv2, fconv3, fconv4, fconv5]
\ No newline at end of file
Index: inpainting_model_pb_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpainting_model_pb_test.py	(date 1578385853000)
+++ inpainting_model_pb_test.py	(date 1578385853000)
@@ -0,0 +1,105 @@
+import os
+
+from skimage.feature import canny
+
+os.environ["CUDA_VISIBLE_DEVICE"] = "0"
+import argparse
+import cv2
+import numpy as np
+import tensorflow as tf
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--image', default='image/test.png', type=str,
+                    help='The filename of image to be completed.')
+parser.add_argument('--mask', default='image/test-mask.jpg', type=str,
+                    help='The filename of mask, value 255 indicates mask.')
+parser.add_argument('--guide', default='image/guide.jpg', type=str,
+                    help='The filename of guidelines, value 0 indicates guide line.')
+parser.add_argument('--output', default='/mnt/disk/mm.bai/inpaiting/dsp-face/test8/image/result-paper/fcfed_paper.jpg', type=str,
+                    help='Where to write output.')
+parser.add_argument('--pbdir', default='/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/ffhqpaper-convset3-eden.pb', type=str,
+                    help='Where to import pb model.')
+
+
+def getNoise(offset_h, offset_w):
+    noiseMask = np.random.rand(height, width)
+    if offset_h:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    else:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=2)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=2)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    return noiseMask1, noiseMask2
+
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+
+    height = 512
+    width = height
+    channel = 3
+    noise = np.random.rand(height, width)
+
+    image = cv2.imread(args.image)
+    mask = cv2.imread(args.mask)
+
+    image = cv2.resize(image, (height,width))
+    mask = cv2.resize(mask, (height, width))
+
+    h, w, _ = image.shape
+    grid = 8
+    image = image[:h // grid * grid, :w // grid * grid, :]
+    mask = mask[:h // grid * grid, :w // grid * grid, :]
+    print('Shape of image: {}'.format(image.shape))
+
+    image = (image / 127.5 - 1.).astype(np.float32)
+    mask = (mask > 127.5).astype(np.float32)
+
+    image = np.expand_dims(image, 0)
+    mask = np.expand_dims(mask, 0)
+
+    noiseMask1, noiseMask2 = getNoise(2, 1)
+    noiseMask1_1, noiseMask2_1 = getNoise(None, 2)
+
+    one_s = np.ones_like(mask)
+    image_original = image
+    image = image * (1. - mask)
+
+    # input_image = np.concatenate([image, one_s[:,:,:,0:1], mask[:,:,:,0:1], noise], axis=3)
+    input_image = np.concatenate([image, mask[:, :, :, 0:1]], axis=3)
+
+
+    with tf.Graph().as_default():
+        output_graph_def = tf.GraphDef()
+        with open(args.pbdir, "rb") as f:
+            output_graph_def.ParseFromString(f.read())
+            _ = tf.import_graph_def(output_graph_def, name="")
+        config = tf.ConfigProto(allow_soft_placement=True)
+        with tf.Session(config=config) as sess:
+
+            sess.run(tf.global_variables_initializer())
+            graph = tf.get_default_graph()
+
+            input_image_tensor1 = sess.graph.get_tensor_by_name("input:0")
+
+            layer2 = sess.graph.get_tensor_by_name("inpaint_net/batch_normalization/gamma/read:0")
+            # layer2 = tf.transpose(layer2, (3,2,0,1))
+
+            layer3 = sess.graph.get_tensor_by_name("inpaint_net/batch_normalization/FusedBatchNorm:0")
+            layer3 = tf.transpose(layer3, (0, 3, 1, 2))
+
+            gamma = sess.graph.get_tensor_by_name("inpaint_net/batch_normalization/gamma/read:0")
+            beta = sess.graph.get_tensor_by_name("inpaint_net/batch_normalization/beta/read:0")
+            mean = sess.graph.get_tensor_by_name("inpaint_net/batch_normalization/moving_mean/read:0")
+            variance = sess.graph.get_tensor_by_name("inpaint_net/batch_normalization/moving_variance/read:0")
+            #
+            # result = sess.run([gamma,beta,mean,variance], feed_dict={input_image_tensor1: input_image})#,
+            testres = sess.run([layer3], feed_dict={input_image_tensor1: input_image})
+            print(testres)
+
+            # for i in result:
+            #     print(i)
Index: print_pd_node_name.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- print_pd_node_name.py	(date 1578366123000)
+++ print_pd_node_name.py	(date 1578366123000)
@@ -0,0 +1,15 @@
+import tensorflow as tf
+from tensorflow.python.platform import gfile
+
+
+GRAPH_PB_PATH = '/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/ffhqpaper-convset3-eden.pb' #path to your .pb file
+with tf.Session() as sess:
+    print("load graph")
+    with gfile.FastGFile(GRAPH_PB_PATH,'rb') as f:
+        graph_def = tf.GraphDef()
+        # Note: one of the following two lines work if required libraries are available
+        #text_format.Merge(f.read(), graph_def)
+        graph_def.ParseFromString(f.read())
+        tf.import_graph_def(graph_def, name='')
+        for i,n in enumerate(graph_def.node):
+            print("Name {} of the node - {}".format(i, n.name))
Index: inpaint_test_demo_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_test_demo_model.py	(date 1578363852000)
+++ inpaint_test_demo_model.py	(date 1578363852000)
@@ -0,0 +1,176 @@
+import argparse
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = '0'
+import random
+import cv2
+import numpy as np
+import tensorflow as tf
+from inpaint_model import InpaintCAModel, logging
+logger = logging.getLogger()
+from tensorflow.python.framework import graph_util
+
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--image', default='image/image.jpg', type=str,
+                    help='The filename of image to be completed.')
+parser.add_argument('--mask', default='image/mask.jpg', type=str,
+                    help='The filename of mask, value 255 indicates mask.')
+parser.add_argument('--guide', default='image/guide.jpg', type=str,
+                    help='The filename of guidelines, value 0 indicates guide line.')
+parser.add_argument('--output', default='image/output_test.jpg', type=str,
+                    help='Where to write output.')
+parser.add_argument('--checkpoint_dir', default=
+                    'model/convset3-eden',
+                    type=str,
+                    help='The directory of tensorflow checkpoint.')
+parser.add_argument('--save',
+                    default='/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/ffhqpaper-convset3-eden.pb',
+                    type=str
+                    )
+
+def gaussian_blur(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    pad_w = 0
+    padded = tf.pad(image, [[0, 0], [pad_w, pad_w], [pad_w, pad_w], [0, 0]], mode='CONSTANT')
+    for channel_idx in range(cdim):
+        data_c = padded[:, :, :, channel_idx:(channel_idx + 1)]
+        # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+        g = kernel.reshape((kernel_size, kernel_size, 1, 1))
+        data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+        # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+        data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+        outputs.append(data_c)
+    return tf.concat(outputs,axis=3)
+
+def getNoise(offset_h, offset_w):
+        noiseMask = np.random.rand(height, width)
+        if offset_h and offset_w:
+            noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+        elif offset_h:
+            noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+        elif offset_w:
+            noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+        # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+        noiseMask2 = 1 - noiseMask1
+        noiseMask1 = np.expand_dims(noiseMask1, axis=2)
+        noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=2)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+        return noiseMask1, noiseMask2
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+
+    height = 512
+    #
+    width = height
+    # channel = 3
+    #
+    model = InpaintCAModel()
+
+    noise1 = np.random.rand(height, width)
+    noise1 = np.expand_dims(noise1, axis=2)
+    noise1 = np.expand_dims(noise1, axis=0)
+
+    noise2 = np.random.rand(height, width)
+    noise2 = np.expand_dims(noise2, axis=2)
+    noise2 = np.expand_dims(noise2, axis=0)
+
+    noise3 = noise1[:, :256, :256, :]
+    noise4 = noise2[:, :256, 256:, :]
+
+    noiseMask1, noiseMask2 = getNoise(2,1)
+    noiseMask1_1, noiseMask2_1 = getNoise(None, 2)
+
+
+    # 5*5 gauss filter
+    gauss_filter_5 = np.array(
+        # [1, 4, 7, 4, 1, 4, 16, 26, 16, 4, 7, 26, 41, 26, 7, 4, 16, 26, 16, 4, 1, 4, 7, 4, 1]) / 273.0
+        # [1, 2, 3, 2, 1, 2, 4, 5, 4, 2, 3, 5, 160, 5, 3, 2, 4, 5, 4, 2, 1, 2, 3, 2, 1]) / 228.0
+        [1, 2, 3, 2, 1, 2, 3, 4, 3, 2, 3, 4, 160, 4, 3, 2, 3, 4, 3, 2, 1, 2, 3, 2, 1]) / 220.0
+    gauss_filter = gauss_filter_5.astype(dtype=np.float32)
+
+    # 3*3 gauss filter
+    gauss_filter_3 = np.array(
+        [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+    gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+    # gauss_filter.reshape((5, 5, 1, 1))
+
+    sess_config = tf.ConfigProto()
+    pb_file_path = args.save
+
+    with tf.Session(config=sess_config) as sess:
+        input = tf.placeholder(tf.float32, shape=[1, height, width, 4], name='input')
+        # noise3 = tf.placeholder(tf.float32, shape=[1, height/2, width/2, 1], name='noise1')
+        # noise2 = tf.placeholder(tf.float32, shape=[1, height, width, 1], name='noise2')
+        # B,G,R, imageGray, one_s, mask = tf.split(input, num_or_size_splits=5, axis=3)
+        # B, G, R, imageGray, one_s, mask, edge = tf.split(input, num_or_size_splits=7, axis=3)
+        # image = tf.concat([B,G,R], axis=3)
+        # x1_input = tf.concat([imageGray, mask, edge], axis=3)
+        # output = model.build_gate_res_net(input, is_test=True)
+        # x1 = tf.cast(x1[:, :, :, :] > 0, tf.float32)
+        # x2_input = tf.concat([image, one_s, mask, x1], axis=3)
+        output = model.build_inpaint_net(
+            input, noise1=noise1, noise2=noise2,
+            noise3=noise3, noise4 = noise4,
+            is_test=True)
+
+        # output = tf.multiply(output, 1.0, name='output')
+
+        # x1 = output[:, :-2, :-1, :]
+        # # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+        # x2 = output[:, 2:, 1:, :]
+        # # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+        # with tf.variable_scope(name_or_scope='x1Multi'):
+        #     x1 = x1 * noiseMask1
+        # with tf.variable_scope(name_or_scope='x2Multi'):
+        #     x2 = x2 * noiseMask2
+        # # with tf.variable_scope(name_or_scope='x1Multix2'):
+        # output = tf.add(x1, x2, name='x1Multix2')
+        # output = tf.pad(output, paddings=[[0, 0], [1, 1], [1, 0], [0, 0]], mode="CONSTANT", name='output_1')
+        #
+        # x1 = output[:, :, :-2, :]
+        # # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+        # x2 = output[:, :, 2:, :]
+        # # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+        # with tf.variable_scope(name_or_scope='x1Multi_1'):
+        #     x1 = x1 * noiseMask1_1
+        # with tf.variable_scope(name_or_scope='x2Multi_1'):
+        #     x2 = x2 * noiseMask2_1
+        # # with tf.variable_scope(name_or_scope='x1Multix2'):
+        # output = tf.add(x1, x2, name='x1Multix2_1')
+        # output = tf.pad(output, paddings=[[0, 0], [0, 0], [0, 2], [0, 0]], mode="CONSTANT", name='output_2')
+
+        # Gauss blur
+        # output = gaussian_blur(output, gauss_filter_3, 3)
+        output = tf.add(output, 0., name='output')
+
+        # B,G,R,_one,_mask,_guideline = tf.split(input, num_or_size_splits=7, axis=3)
+        # _imageGray, _mask, _guideline = tf.split(input, num_or_size_splits=3, axis=3)
+        # img = tf.concat([B,G,R], axis=3)
+        # output = image*(1.-mask) + output*mask
+        # output = output*_mask + _guideline*(1. - _mask)
+        # output = (output + 1.) * 127.5
+        # output = tf.reverse(output, [-1])
+        # output = tf.saturate_cast(output, tf.uint8)
+        # load pretrained model
+        vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+        assign_ops = []
+        for var in vars_list:
+            vname = var.name
+            logger.info(vname)
+            from_name = vname
+            var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)
+            assign_ops.append(tf.assign(var, var_value))
+
+        sess.run(assign_ops)
+        logger.info('Model loaded.')
+        # result = sess.run(output, feed_dict={input: input_data})
+
+        constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['output'])
+        # constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['inpaint_net/post/conv9-2/BiasAdd'])
+        with tf.gfile.FastGFile(pb_file_path, mode='wb') as f:
+            f.write(constant_graph.SerializeToString())
+        # cv2.imwrite(args.output, result[0][:, :, ::-1])
\ No newline at end of file
Index: inpaint.yml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint.yml	(date 1578446975000)
+++ inpaint.yml	(date 1578446975000)
@@ -0,0 +1,74 @@
+# parameters
+
+# Inpainting G net loss
+X2_L1_LOSS_ALPHA: 5
+L_MASK_PER_PIXEL_ALPHA: 5
+L_UNMASK_PER_PIXEL_ALPHA: 2
+PRC_LOSS_ALPHA: 0.03
+STYLE_LOSS_ALPHA: 60
+TV_LOSS_ALPHA: 1
+G_LOSS_PAIR_ALPHA: 0.8
+G_LOSS_G_ALPHA: 0.8
+
+D_LOSS_G_ALPHA: 0.5
+D_LOSS_PAIR_ALPHA: 0.5
+
+# training
+MODEL_RESTORE: 'model/convset3-eden'
+MODEL_SAVEDIR: 'model/convset3-eden-split'
+MAX_TO_KEEP_MODEL: 20
+BATCH_SIZE: 8
+NUM_GPUS: 1
+GPU_ID: [1]  # -1 indicate select any available one, otherwise select gpu ID, e.g. [0,1,3]
+TRAIN_SPE: 5000
+MAX_ITERS: 1000000000
+VIZ_MAX_OUT: 3
+GRADS_SUMMARY: True
+GRADIENT_CLIP: False
+GRADIENT_CLIP_VALUE: 0.1
+VAL_PSTEPS: 1000
+
+DATASET: 'face'  # 'tmnist', 'dtd', 'places2', 'celeba', 'imagenet', 'cityscapes'
+DATASET_GUIDE: 'places2'
+RANDOM_CROP: False
+VAL: False
+LOG_DIR: guide_line
+USER_GUIDELINE: 1
+RANDOM_GUIDE_INPUT: True
+RESIZE_IMG: [512,512]
+RESIZE_GUIDE: [512,512]
+GAN: 'maskD' #'maskD', 'DCGAN', 'lsgan', 'wgan_gp', 'one_wgan_gp', 'RaSGAN'
+
+DATA_FLIST:
+  # https://github.com/JiahuiYu/progressive_growing_of_gans_tf
+  celebahq: [
+    'data/celeba_hq/train_shuffled.flist',
+    'data/celeba_hq/train_shuffled.flist'
+  ]
+  # http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html, please to use : True
+  celeba: [
+    'data/celeba/train_shuffled.flist',
+    'data/celeba/train_shuffled.flist'
+  ]
+  # http://places2.csail.mit.edu/, please download the high-resolution dataset and use RANDOM_CROP: True
+  places2: [
+    '/data/zhiweige/tf_places2_filelist/train_shuffled.flist',
+    '/data/data_for_training/new_places2/validation_shuffled.flist',
+  ]
+  # http://www.image-net.org/, please use RANDOM_CROP: True
+  imagenet: [
+    'data/imagenet/train_shuffled.flist',
+    'data/imagenet/static_view.flist',
+  ]
+  face: [
+    '/home/samsung/Data/li.zuo/ffhq-dataset/list.txt',
+  ]
+
+STATIC_VIEW_SIZE: 30
+INPUT_IMG_SHAPES: [256, 512, 3]
+IMG_SHAPES: [256, 256, 3]
+MASK_SHAPES: [256, 256, 3]
+
+# loss legacy
+VGG_MODEL_FILE: '/home/samsung/Data/li.zuo/models/vgg16.npy'
+VGG19_MODEL_FILE: '/home/samsung/Data/li.zuo/models/vgg19.npy'
\ No newline at end of file
Index: inpaint_model_cnt_flops.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_model_cnt_flops.py	(date 1577781374000)
+++ inpaint_model_cnt_flops.py	(date 1577781374000)
@@ -0,0 +1,44 @@
+from tensorflow.python.framework import graph_util
+import tensorflow as tf
+
+import tensorflow as tf
+def stats_graph(graph):
+    flops = tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.float_operation())
+    params = tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())
+    print('FLOPs: {};    Trainable params: {}'.format(flops.total_float_ops, params.total_parameters))
+
+def load_pb(pb):
+    with tf.gfile.GFile(pb, "rb") as f:
+        graph_def = tf.GraphDef()
+        graph_def.ParseFromString(f.read())
+    with tf.Graph().as_default() as graph:
+        tf.import_graph_def(graph_def, name='')
+        return graph
+
+
+with tf.Graph().as_default() as graph:
+    # ***** (1) Create Graph *****
+    A = tf.Variable(initial_value=tf.random_normal([25, 16]))
+    B = tf.Variable(initial_value=tf.random_normal([16, 9]))
+    C = tf.matmul(A, B, name='output')
+
+    print('stats before freezing')
+    stats_graph(graph)
+    with tf.Session() as sess:
+        sess.run(tf.global_variables_initializer())
+        # ***** (2) freeze graph *****
+        output_graph = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), ['output'])
+        with tf.gfile.GFile('graph.pb', "wb") as f:
+            f.write(output_graph.SerializeToString())
+
+#RegionWise: /mnt/disk/mm.bai/inpaiting/paperModel/region_wise_inpainting/Region-wise-Inpainting-master/model.pb
+#EdgeConnect: /mnt/disk/mm.bai/inpaiting/paperModel/edge-connect-master/model_Edge0718.pb
+#Contextual: /mnt/disk/mm.bai/inpaiting/paperModel/generative_inpainting_master/model.pb
+#GatedConv:  /mnt/disk/mm.bai/inpaiting/paperModel/gate_inpainting_guideline/model.pb
+#Ours: /mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/test10-t18.pb
+
+# ***** (3) Load frozen graph *****
+graph = load_pb('/mnt/disk/mm.bai/inpaiting/paperModel/gate_inpainting_guideline/model.pb')
+
+print('stats after freezing')
+stats_graph(graph)
\ No newline at end of file
Index: run.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- run.sh	(date 1572395093000)
+++ run.sh	(date 1572395093000)
@@ -0,0 +1,5 @@
+#!/bin/bash
+source ~/.bashrc
+cd /data/mm.bai/inpainting/145_model_copy/EDEN_zoo/DSP-test8
+ls `pwd`/run.sh
+/data/mm.bai/anaconda3/bin/python inpaint_train.py 2>&1|tee neuralgym_logs/log.txt
\ No newline at end of file
Index: inpaint_test_demo_shape.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_test_demo_shape.py	(date 1572395093000)
+++ inpaint_test_demo_shape.py	(date 1572395093000)
@@ -0,0 +1,46 @@
+from PyQt5.QtCore import Qt
+
+class Point:
+    X = 0
+    Y = 0
+
+    def __init__(self):
+        self.X = 0
+        self.Y = 0
+
+    def __init__(self, nX, nY):
+        self.X = nX
+        self.Y = nY
+
+    def Set(self,nX, nY):
+        self.X = nX
+        self.Y = nY
+
+
+class Shape:
+    Location = Point(0,0)
+    Width = 0.0
+    Colour = Qt.white
+    ShapeNumber = 0
+
+    def __init__(self, L, W, C, S):
+        self.Location = L
+        self.Width = W
+        self.Colour = C
+        self.ShapeNumber = S
+
+class Shapes:
+    __Shapes = []
+
+    def __init__(self):
+        self.__Shapes = []
+
+    def NumberOfShapes(self):
+        return len(self.__Shapes)
+
+    def NewShape(self,L,W,C,S):
+        Sh = Shape(L,W,C,S)
+        self.__Shapes.append(Sh)
+
+    def GetShape(self, Index):
+        return self.__Shapes[Index]
\ No newline at end of file
Index: inpaint_test_demo_guideline.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_test_demo_guideline.py	(date 1578363930000)
+++ inpaint_test_demo_guideline.py	(date 1578363930000)
@@ -0,0 +1,351 @@
+# -*- coding: utf-8 -*-
+
+"""
+Demo for Inpainting
+
+In this code, we draw mask on input image using mouse interaction
+and run inpainting inference code with mask image.
+
+Author: Sungwon Kim
+"""
+import os
+import sys
+from pathlib import Path
+from PyQt5.QtCore import Qt
+from PyQt5.QtWidgets import QGroupBox, QWidget, QHBoxLayout, QApplication, QLabel, QGridLayout, QPushButton, QFileDialog, QTextEdit, QRadioButton
+from PyQt5.QtGui import QPixmap, QPainter, QPen
+from inpaint_test_demo_shape import *
+from datetime import datetime
+# os.environ["CUDA_DEVICE_ORDER"] = "0,1"
+
+from subprocess import Popen, PIPE, STDOUT
+
+
+class Example(QWidget):
+
+    def __init__(self):
+        super().__init__()
+        self.title = 'Inpaint Demo - User Interaction'
+        self.savePath = "./image/mask/"
+        self.resultPath = "./image/result-test_eden/"
+        self.image_name = "./image/test.png"
+        self.model_name = '/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/ffhqpaper-convset3-eden.pb'
+        self.cmd = "python inpaint_test_demo_pb.py " \
+                   "--image=%s --mask=%s --output=%s --pbdir=%s"
+        try:
+                os.makedirs(self.savePath)
+        except OSError:
+                pass
+        try:
+                os.makedirs(self.resultPath)
+        except OSError:
+                pass
+
+        self.pixmap_input = QPixmap(self.image_name)
+        self.pixmap_input_guidline = QPixmap(self.image_name)
+        self.pixmap_mask = self.pixmap_input.copy()
+        self.pixmap_mask_guidline = self.pixmap_input.copy()
+        self.image_width = self.pixmap_input.width()
+        self.image_height = self.pixmap_input.height()
+        self.hole = Qt.white # Qt.white / Qt.black
+
+        self.input_label = QLabel(self)
+        self.output_label = QLabel(self)
+        self.btn_load = QPushButton("Load Image...")
+        self.text_edit = QTextEdit()
+        self.btn_run = QPushButton("Run Inpaint...")
+
+        self.IsPainting = False
+        self.ShapeNum = 0
+        self.MouseLoc = Point(0, 0)
+        self.LastPos = Point(0, 0)
+        self.DrawingShapes = Shapes()
+        self.DrawingShapes_guidline = Shapes()
+        self.CurrentWidth = 30
+        self.CurrentColour = self.hole
+
+        self.window_left = 300
+        self.window_top = 300
+        self.window_width = 1100
+        self.window_height = 600
+        self.initUI()
+
+    def initUI(self):
+        self.main_grid = QGridLayout()
+        #self.main_grid.addWidget(self.createModelConfigGroup(), 0, 0)
+        #self.main_grid.addWidget(self.createImageGroup(), 1, 0)
+        self.setLayout(self.createImageGroup())
+        #self.setLayout(self.main_grid)
+
+        self.setGeometry(self.window_left, self.window_top, self.window_width, self.window_height)
+        self.setWindowTitle(self.title)
+
+        self.show()
+
+    def createModelConfigGroup(self):
+        groupBox = QGroupBox("Hole Color is...")
+
+        radio_white = QRadioButton("&White")
+        radio_black = QRadioButton("&Black")
+        radio_white.setFixedWidth(70)
+
+        radio_white.setChecked(True)
+
+        radio_white.clicked.connect(self.setWhite)
+        radio_black.clicked.connect(self.setBlack)
+
+        vbox = QHBoxLayout()
+        vbox.addWidget(radio_white, Qt.AlignLeft)
+        vbox.addWidget(radio_black, Qt.AlignLeft)
+        groupBox.setLayout(vbox)
+
+        radio_white.click()
+        return groupBox
+
+    def setBlack(self):
+        self.hole = Qt.black
+        self.CurrentColour = self.hole
+        self.resetMask()
+
+    def setWhite(self):
+        self.hole = Qt.white
+        self.CurrentColour = self.hole
+        self.resetMask()
+
+    def createImageGroup(self):
+        self.image_groupBox = QGroupBox("Images ...")
+
+        self.input_label.setPixmap(self.pixmap_input)
+        self.input_label.setPixmap(self.pixmap_input_guidline)
+
+        if(self.hole == Qt.white):
+            self.pixmap_mask.fill(Qt.black)
+            self.pixmap_mask_guidline.fill(Qt.black)
+        else:
+            self.pixmap_mask.fill(Qt.white)
+            self.pixmap_mask_guidline.fill(Qt.white)
+
+        self.btn_load.clicked.connect(self.getfile)
+
+        self.text_edit.setText(self.image_name)
+        self.text_edit.setMaximumHeight(30)
+
+        self.btn_run.clicked.connect(self.runInpaint)
+
+        self.btn_load.setFixedWidth(self.image_width * 0.9)
+        self.btn_run.setFixedWidth(self.image_width * 0.9)
+        self.text_edit.setFixedWidth(self.image_width * 0.9)
+
+        grid = QGridLayout()
+        grid.addWidget(self.input_label, 0, 0, Qt.AlignTop)
+        grid.addWidget(self.output_label, 0, 1, Qt.AlignTop)
+        grid.addWidget(self.btn_load, 1, 0, Qt.AlignHCenter)
+        grid.addWidget(self.text_edit, 2, 0, Qt.AlignHCenter)
+        grid.addWidget(self.btn_run, 1, 1, Qt.AlignHCenter)
+
+        self.image_groupBox.setLayout(grid)
+        return grid
+        return self.image_groupBox
+
+    def mousePressEvent(self, QMouseEvent):
+        #left button
+        if(QMouseEvent.buttons() == Qt.LeftButton):
+            self.margin_x = self.image_groupBox.geometry().x() + self.input_label.geometry().x()
+            self.margin_y = self.image_groupBox.geometry().y() + self.input_label.geometry().y()
+            click_point = Point(QMouseEvent.x() - self.margin_x, QMouseEvent.y() - self.margin_y)
+
+            # Prevent click out of Image
+            if(0 <= click_point.X < self.image_width):
+                if(0 <= click_point.Y < self.image_height):
+                    self.IsPainting = True
+                    self.ShapeNum += 1
+                    self.LastPos = Point(0, 0)
+        #right button
+        elif(QMouseEvent.buttons() == Qt.RightButton):
+            self.margin_x = self.image_groupBox.geometry().x() + self.input_label.geometry().x()
+            self.margin_y = self.image_groupBox.geometry().y() + self.input_label.geometry().y()
+            click_point = Point(QMouseEvent.x() - self.margin_x, QMouseEvent.y() - self.margin_y)
+
+            # Prevent click out of Image
+            if (0 <= click_point.X < self.image_width):
+                if (0 <= click_point.Y < self.image_height):
+                    self.IsPainting = True
+                    self.ShapeNum += 1
+                    self.LastPos = Point(0, 0)
+
+
+    def mouseMoveEvent(self, QMouseEvent):
+        if(QMouseEvent.buttons() == Qt.LeftButton):
+            if (self.IsPainting == True):
+                self.MouseLoc = Point(QMouseEvent.x() - self.margin_x, QMouseEvent.y() - self.margin_y)
+
+                # If the mouse moves out of Image, ShapeNum++
+                if(self.MouseLoc.X < 0 or self.MouseLoc.X >= self.image_width):
+                    self.ShapeNum += 1
+                elif(self.MouseLoc.Y < 0 or self.MouseLoc.Y >= self.image_height):
+                    self.ShapeNum += 1
+                else:
+                    if ((self.LastPos.X != self.MouseLoc.X) and (self.LastPos.Y != self.MouseLoc.Y)):
+                        self.LastPos = self.MouseLoc
+                        self.DrawingShapes.NewShape(self.LastPos, self.CurrentWidth, self.CurrentColour, self.ShapeNum)
+
+                    # self.updateImage_guidline()
+                    self.updateImage()
+        elif (QMouseEvent.buttons() == Qt.RightButton):
+            if (self.IsPainting == True):
+                self.MouseLoc = Point(QMouseEvent.x() - self.margin_x, QMouseEvent.y() - self.margin_y)
+
+                # If the mouse moves out of Image, ShapeNum++
+                if (self.MouseLoc.X < 0 or self.MouseLoc.X >= self.image_width):
+                    self.ShapeNum += 1
+                elif (self.MouseLoc.Y < 0 or self.MouseLoc.Y >= self.image_height):
+                    self.ShapeNum += 1
+                else:
+                    if ((self.LastPos.X != self.MouseLoc.X) and (self.LastPos.Y != self.MouseLoc.Y)):
+                        self.LastPos = self.MouseLoc
+                        self.DrawingShapes_guidline.NewShape(self.LastPos, self.CurrentWidth, self.CurrentColour, self.ShapeNum)
+                    self.updateImage()
+                    # self.updateImage_guidline()
+    def mouseReleaseEvent(self, QMouseEvent):
+        if (self.IsPainting == True):
+            self.IsPainting = False
+
+    def updateImage(self):
+        painter_input = QPainter()
+        painter_input.begin(self.pixmap_input)
+        self.drawLines(painter_input)
+        painter_input.end()
+
+        painter_input.begin(self.pixmap_input)
+        self.drawLines_guidline(painter_input, Colour='black')
+        painter_input.end()
+
+        painter_mask = QPainter()
+        painter_mask.begin(self.pixmap_mask)
+        self.drawLines(painter_mask)
+        painter_mask.end()
+
+        painter_mask = QPainter()
+        painter_mask.begin(self.pixmap_mask_guidline)
+        self.drawLines_guidline(painter_mask)
+        painter_mask.end()
+
+        self.input_label.setPixmap(self.pixmap_input)
+
+    def drawLines(self, painter):
+        painter.setRenderHint(QPainter.Antialiasing)
+
+        for i in range(self.DrawingShapes.NumberOfShapes() - 1):
+            T = self.DrawingShapes.GetShape(i)
+            T1 = self.DrawingShapes.GetShape(i + 1)
+            if (T.ShapeNumber == T1.ShapeNumber):
+                pen = QPen(T.Colour, T.Width / 2, Qt.SolidLine, Qt.RoundCap, Qt.RoundJoin)
+                painter.setPen(pen)
+                painter.drawLine(T.Location.X, T.Location.Y, T1.Location.X, T1.Location.Y)
+
+    def updateImage_guidline(self):
+        painter_input = QPainter()
+        painter_input.begin(self.pixmap_input_guidline)
+        self.drawLines_guidline(painter_input, Colour='black')
+        painter_input.end()
+
+        painter_mask = QPainter()
+        painter_mask.begin(self.pixmap_mask_guidline)
+        self.drawLines_guidline(painter_mask)
+        painter_mask.end()
+
+        self.input_label.setPixmap(self.pixmap_input_guidline)
+
+    def drawLines_guidline(self, painter, Colour='white'):
+        painter.setRenderHint(QPainter.Antialiasing)
+
+        for i in range(self.DrawingShapes_guidline.NumberOfShapes() - 1):
+            T = self.DrawingShapes_guidline.GetShape(i)
+            T1 = self.DrawingShapes_guidline.GetShape(i + 1)
+            if (T.ShapeNumber == T1.ShapeNumber):
+                if Colour == 'black':
+                    pen = QPen(Qt.black, T.Width / 10, Qt.SolidLine, Qt.RoundCap, Qt.RoundJoin)
+                else:
+                    pen = QPen(T.Colour, T.Width / 10, Qt.SolidLine, Qt.RoundCap, Qt.RoundJoin)
+                painter.setPen(pen)
+                painter.drawLine(T.Location.X, T.Location.Y, T1.Location.X, T1.Location.Y)
+
+    def getfile(self):
+        file_name = QFileDialog.getOpenFileName(self, 'Open file',
+                                            './', "Image files (*.jpg *.png)")
+        print(file_name[0])
+        if file_name[0]:
+            self.image_name = file_name[0]
+            self.pixmap_input = QPixmap(self.image_name)
+            self.resetMask()
+            #print(self.pixmap_input.width(), self.pixmap_input.height())
+            self.image_width = self.pixmap_input.width()
+            self.image_height = self.pixmap_input.height()
+            self.input_label.setPixmap(self.pixmap_input)
+            self.output_label.clear()
+            self.btn_load.setFixedWidth(self.image_width * 0.9)
+            self.btn_run.setFixedWidth(self.image_width * 0.9)
+            self.text_edit.setFixedWidth(self.image_width * 0.9)
+            self.text_edit.setText(self.image_name)
+
+    def runInpaint(self):
+        # Save Mask Image
+        self.saveMask()
+        print("Save Mask Image to PATH")
+
+        #TODO: Run Network Inference
+        self.runInference()
+        out_name = self.resultPath + Path(self.image_name).name
+        self.pixmap_output = QPixmap(out_name)
+        self.output_label.setPixmap(self.pixmap_output)
+
+    def saveMask(self):
+        #save_str = datetime.now().strftime('%Y%m%d_%H%M%S')
+        #print(save_str)
+        #self.output_label.setPixmap(self.pixmap_mask)
+        #print(self.savePath + save_str + '_mask.png')
+        # save_name = self.savePath+save_str + '_mask.png'
+        save_name = self.savePath+Path(self.image_name).name
+        save_name_guidline = self.savePath+'guideline'+Path(self.image_name).name
+        self.pixmap_mask.save(save_name)
+        self.pixmap_mask_guidline.save(save_name_guidline)
+
+    def resetMask(self):
+        self.DrawingShapes = Shapes()
+        self.DrawingShapes_guidline = Shapes()
+        self.ShapeNum = 0
+        self.updateImage()
+        self.pixmap_input = QPixmap(self.image_name)
+        self.input_label.setPixmap(self.pixmap_input)
+
+        self.pixmap_mask = self.pixmap_input.copy()
+        self.pixmap_mask_guidline = self.pixmap_input.copy()
+        self.output_label.clear()
+        if (self.hole == Qt.white):
+            self.pixmap_mask.fill(Qt.black)
+            self.pixmap_mask_guidline.fill(Qt.black)
+        else:
+            self.pixmap_mask.fill(Qt.white)
+            self.pixmap_mask_guidline.fill(Qt.white)
+
+    def runInference(self):
+        model_name = self.model_name
+        mask_name = self.savePath+Path(self.image_name).name
+        # guide_name = self.savePath+'guideline'+Path(self.image_name).name
+        #if os.path.exist(
+        image_name = self.image_name
+        out_name = self.resultPath + Path(self.image_name).name
+        cmd = self.cmd%(image_name, mask_name, out_name, model_name)
+        print(cmd)
+        p = Popen(cmd, shell=True, stdout=PIPE, stderr=STDOUT)
+        while True:
+            line = p.stdout.readline()
+            if not line:
+                break
+            else:
+                print(line)
+
+if __name__ == '__main__':
+    app = QApplication(sys.argv)
+    ex = Example()
+    sys.exit(app.exec_())
Index: copy.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- copy.sh	(date 1572395093000)
+++ copy.sh	(date 1572395093000)
@@ -0,0 +1,4 @@
+cp /data/mm.bai/inpainting/145_model_copy/EDEN_zoo/DSP-test1/model/model_4/snap-52000.data-00000-of-00001 model/model_1/
+cp /data/mm.bai/inpainting/145_model_copy/EDEN_zoo/DSP-test1/model/model_4/snap-52000.index model/model_1/
+cp /data/mm.bai/inpainting/145_model_copy/EDEN_zoo/DSP-test1/model/model_4/snap-52000.meta model/model_1/
+cp /data/mm.bai/inpainting/145_model_copy/EDEN_zoo/DSP-test1/model/model_4/checkpoint model/model_1/
\ No newline at end of file
Index: inpaint_test_demo_model_depth.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_test_demo_model_depth.py	(date 1578357929000)
+++ inpaint_test_demo_model_depth.py	(date 1578357929000)
@@ -0,0 +1,308 @@
+import argparse
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = '0'
+import random
+import cv2
+import numpy as np
+import tensorflow as tf
+from inpaint_model import InpaintCAModel, logging
+logger = logging.getLogger()
+from tensorflow.python.framework import graph_util
+
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--image', default='image/image.jpg', type=str,
+                    help='The filename of image to be completed.')
+parser.add_argument('--mask', default='image/mask.jpg', type=str,
+                    help='The filename of mask, value 255 indicates mask.')
+parser.add_argument('--guide', default='image/guide.jpg', type=str,
+                    help='The filename of guidelines, value 0 indicates guide line.')
+parser.add_argument('--output', default='image/output_test.jpg', type=str,
+                    help='Where to write output.')
+parser.add_argument('--checkpoint_dir', default=
+                    'model/model_15_deletNoise_layers',
+                    type=str,
+                    help='The directory of tensorflow checkpoint.')
+
+def gaussian_blur(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    # outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    # pad_w = 0
+    # padded = tf.pad(image, [[0, 0], [pad_w, pad_w], [pad_w, pad_w], [0, 0]], mode='CONSTANT')
+    # for channel_idx in range(cdim):
+    #     data_c = padded[:, :, :, channel_idx:(channel_idx + 1)]
+    #     g = kernel.reshape((kernel_size, kernel_size, 1, 1))
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     outputs.append(data_c)
+    # return tf.concat(outputs,axis=3)
+
+    # channel B
+    with tf.name_scope(name='b'):
+        data_b = image[:, :, :, 0:1]
+        g_b = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_b = tf.constant(value=g_b, shape=[kernel_size,kernel_size, 1, 1], dtype=tf.float32,
+                          name='const_b1')
+        data_b = tf.nn.conv2d(data_b, g_b, [1, 1, 1, 1], 'SAME', name='convB_1')
+
+        g_b_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_b1/bias')
+        data_b = tf.nn.bias_add(data_b, g_b_bias)
+
+        g_b_1 = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_b_1 = tf.constant(value=g_b_1, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                            name='const_b2')
+        data_b = tf.nn.conv2d(data_b, g_b_1, [1, 1, 1, 1], 'SAME', name='convB_2')
+
+        g_b_1_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_b2/bias')
+        data_b = tf.nn.bias_add(data_b, g_b_1_bias)
+
+        data_b = tf.pad(data_b, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
+        k1 = np.array([1.0]).astype(dtype=np.float32)
+        k1 = k1.reshape([1, 1, 1, 1])
+        k1 = tf.constant(value=k1, shape=[1,1,1,1], dtype=tf.float32,
+                         name='fcn_b')
+        data_b = tf.nn.conv2d(data_b, k1, [1,1,1,1], 'VALID', name='fcn_pad_b')
+
+        data_b_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='fcn_b/bias')
+        data_b = tf.nn.bias_add(data_b, data_b_bias)
+
+    # channel G
+    with tf.name_scope(name='g'):
+        data_g = image[:, :, :, 1:2]
+        g_g = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_g = tf.constant(value=g_g, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                          name='const_g1')
+        data_g = tf.nn.conv2d(data_g, g_g, [1, 1, 1, 1], 'SAME', name='convG_1')
+
+        g_g_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_g1/bias')
+        data_g = tf.nn.bias_add(data_g, g_g_bias)
+
+        g_g_1 = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_g_1 = tf.constant(value=g_g_1, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                            name='const_g2')
+        data_g = tf.nn.conv2d(data_g, g_g_1, [1, 1, 1, 1], 'SAME', name='convG_2')
+
+        g_g_1_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_g2/bias')
+        data_g = tf.nn.bias_add(data_g, g_g_1_bias)
+
+        data_g = tf.pad(data_g, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
+        k1 = np.array([1.0]).astype(dtype=np.float32)
+        k1 = k1.reshape([1, 1, 1, 1])
+        k1 = tf.constant(value=k1, shape=[1, 1, 1, 1], dtype=tf.float32,
+                         name='fcn_g')
+        data_g = tf.nn.conv2d(data_g, k1, [1, 1, 1, 1], 'VALID', name='fcn_pad_g')
+
+        data_g_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='fcn_g/bias')
+        data_g = tf.nn.bias_add(data_g, data_g_bias)
+
+    # channel R
+    with tf.name_scope(name='r'):
+        data_r = image[:, :, :, 2:3]
+        g_r = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_r = tf.constant(value=g_r, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                          name='const_r1')
+        data_r = tf.nn.conv2d(data_r, g_r, [1, 1, 1, 1], 'SAME', name='convR_1')
+
+        g_r_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_r1/bias')
+        data_r = tf.nn.bias_add(data_r, g_r_bias)
+
+        g_r_1 = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_r_1 = tf.constant(value=g_r_1, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                            name='const_r2')
+        data_r = tf.nn.conv2d(data_r, g_r_1, [1, 1, 1, 1], 'SAME', name='convR_2')
+
+        g_r_1_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_r2/bias')
+        data_r = tf.nn.bias_add(data_r, g_r_1_bias)
+
+        data_r = tf.pad(data_r, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
+        k1 = np.array([1.0]).astype(dtype=np.float32)
+        k1 = k1.reshape([1, 1, 1, 1])
+        k1 = tf.constant(value=k1, shape=[1, 1, 1, 1], dtype=tf.float32,
+                         name='fcn_r')
+        data_r = tf.nn.conv2d(data_r, k1, [1, 1, 1, 1], 'VALID', name='fcn_pad_r')
+
+        gdata_r_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='fcn_r/bias')
+        data_r = tf.nn.bias_add(data_r, gdata_r_bias)
+
+    output = tf.concat([data_b, data_g, data_r], axis=3)
+    return output
+
+def gaussian_blur_depth(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    # outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    image = tf.pad(image, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
+    k_dt = kernel.reshape((kernel_size, kernel_size, 1, 1))
+    k_dt = np.concatenate([k_dt,k_dt,k_dt], axis=2)
+    k_dt = tf.constant(value=k_dt, shape=[kernel_size, kernel_size, 3, 1], dtype=tf.float32, name='gauss_kernel')
+    x1 = tf.nn.depthwise_conv2d(image, k_dt, [1, 1, 1, 1], 'SAME', name='gauss_blur1')
+    x2 = tf.nn.depthwise_conv2d(x1, k_dt, [1, 1, 1, 1], 'SAME', name='gauss_blur2')
+    return x2
+
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+
+    height = 512
+    #
+    width = height
+    # channel = 3
+    #
+    model = InpaintCAModel()
+    # image = cv2.imread(args.image)
+    # mask = cv2.imread(args.mask)
+    # guide = cv2.imread(args.guide)
+    # # imageGray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
+    # # guide = canny(imageGray, sigma=random.randint(1, 3)).astype(np.float32)
+    #
+    #
+    # image = cv2.resize(image, (height,width))
+    # # imageGray = cv2.resize(imageGray, (height, width))
+    # # imageGray = np.expand_dims(imageGray, 2)
+    # mask = cv2.resize(mask, (height, width))
+    # guide = cv2.resize(guide, (height, width))
+    # # guide = np.expand_dims(guide, 2)
+    #
+    # h, w, _ = image.shape
+    # grid = 8
+    # image = image[:h // grid * grid, :w // grid * grid, :]
+    # # imageGray = imageGray[:h // grid * grid, :w // grid * grid, :]
+    # mask = mask[:h // grid * grid, :w // grid * grid, :]
+    # guide = guide[:h // grid * grid, :w // grid * grid, :]
+    # print('Shape of image: {}'.format(image.shape))
+    #
+    # image = (image / 127.5 - 1.).astype(np.float32)
+    # # imageGray = (imageGray / 127.5 - 1.).astype(np.float32)
+    # mask = (mask > 127.5).astype(np.float32)
+    # guide = (guide > 0).astype(np.float32)
+    #
+    # image = np.expand_dims(image, 0)
+    # # imageGray = np.expand_dims(imageGray, 0)
+    # mask = np.expand_dims(mask, 0)
+    # guide = np.expand_dims(guide, 0)
+    #
+    # mask = mask[:, :, :, 0:1]
+    # guide = guide[:, :, :, 0:1]
+    # one_s = np.ones_like(mask)
+    #
+    # image = image * (1. - mask)
+    # # imageGray = imageGray * (1. - mask)
+    # guide = guide * (1. - mask)
+    #
+    #
+    # # input_data = np.concatenate([imageGray, mask, guide], axis=3)
+    # input_data = np.concatenate([image, one_s, mask, guide], axis=3)
+    noise = np.random.rand(height, width)
+    noise = np.expand_dims(noise, axis=2)
+    noise = np.expand_dims(noise, axis=0)
+
+    def getNoise(offset_h, offset_w):
+        noiseMask = np.random.rand(height, width)
+        if offset_h and offset_w:
+            noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+        elif offset_h:
+            noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+        elif offset_w:
+            noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+        # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+        noiseMask2 = 1 - noiseMask1
+        noiseMask1 = np.expand_dims(noiseMask1, axis=2)
+        noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=2)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+        return noiseMask1, noiseMask2
+
+    noiseMask1, noiseMask2 = getNoise(2,1)
+    noiseMask1_1, noiseMask2_1 = getNoise(2, 2)
+
+    # 5*5 gauss filter
+    gauss_filter_5 = np.array(
+        # [1, 4, 7, 4, 1, 4, 16, 26, 16, 4, 7, 26, 41, 26, 7, 4, 16, 26, 16, 4, 1, 4, 7, 4, 1]) / 273.0
+        # [1, 2, 3, 2, 1, 2, 4, 5, 4, 2, 3, 5, 160, 5, 3, 2, 4, 5, 4, 2, 1, 2, 3, 2, 1]) / 228.0
+        [1, 2, 3, 2, 1, 2, 3, 4, 3, 2, 3, 4, 150, 4, 3, 2, 3, 4, 3, 2, 1, 2, 3, 2, 1]) / 210.0
+    gauss_filter = gauss_filter_5.astype(dtype=np.float32)
+
+    # 3*3 gauss filter1
+    gauss_filter_3 = np.array(
+        [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+
+    gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+    # gauss_filter.reshape((5, 5, 1, 1))
+
+    sess_config = tf.ConfigProto()
+    pb_file_path = '/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/test_eden.pb'
+
+    with tf.Session(config=sess_config) as sess:
+        input = tf.placeholder(tf.float32, shape=[1, height, width, 4], name='input')
+        # B,G,R, imageGray, one_s, mask = tf.split(input, num_or_size_splits=5, axis=3)
+        # B, G, R, imageGray, one_s, mask, edge = tf.split(input, num_or_size_splits=7, axis=3)
+        # image = tf.concat([B,G,R], axis=3)
+        # x1_input = tf.concat([imageGray, mask, edge], axis=3)
+        # output = model.build_gate_res_net(input, is_test=True)
+        # x1 = tf.cast(x1[:, :, :, :] > 0, tf.float32)
+        # x2_input = tf.concat([image, one_s, mask, x1], axis=3)
+        output = model.build_inpaint_net(input, noise=noise, is_test=True)
+
+        # output = tf.multiply(output, 1.0, name='output')
+        with tf.name_scope(name='crop1'):
+            x1 = output[:, :-2, :-1, :]
+        # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+        with tf.name_scope(name='crop2'):
+            x2 = output[:, 2:, 1:, :]
+        # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+        with tf.name_scope(name='x1_Mul_Noise1'):
+            x1 = x1 * noiseMask1
+        with tf.name_scope(name='x2_Mul_Noise2'):
+            x2 = x2 * noiseMask2
+        # with tf.variable_scope(name_or_scope='x1Multix2'):
+
+        output = tf.add(x1, x2, name='x1_add_x2')
+        # output = tf.pad(output, paddings=[[0, 0], [1, 1], [1, 0], [0, 0]], mode="CONSTANT", name='output_1')
+
+        with tf.name_scope(name='crop3'):
+            x3 = output[:, :, :-1, :]
+        # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+        with tf.name_scope(name='crop4'):
+            x4 = output[:, :, 1:, :]
+        # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+        with tf.name_scope(name='x3_Mul_noise3'):
+            x3 = x3 * noiseMask1_1
+        with tf.name_scope(name='x4_Mul_noise4'):
+            x4 = x4 * noiseMask2_1
+        # with tf.variable_scope(name_or_scope='x1Multix2'):
+        output = tf.add(x3, x4, name='x3_add_x4')
+        # output = tf.pad(output, paddings=[[0, 0], [0, 0], [0, 2], [0, 0]], mode="CONSTANT", name='output_2')
+
+        # Gauss blur
+        with tf.variable_scope(name_or_scope='gauss_blur'):
+            output = gaussian_blur_depth(output, gauss_filter_3, 3)
+        output = tf.add(output, 0., name='output')
+
+        # B,G,R,_one,_mask,_guideline = tf.split(input, num_or_size_splits=7, axis=3)
+        # _imageGray, _mask, _guideline = tf.split(input, num_or_size_splits=3, axis=3)
+        # img = tf.concat([B,G,R], axis=3)
+        # output = image*(1.-mask) + output*mask
+        # output = output*_mask + _guideline*(1. - _mask)
+        # output = (output + 1.) * 127.5
+        # output = tf.reverse(output, [-1])
+        # output = tf.saturate_cast(output, tf.uint8)
+        # load pretrained model
+        vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+        assign_ops = []
+        for var in vars_list:
+            vname = var.name
+            logger.info(vname)
+            from_name = vname
+            var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)
+            assign_ops.append(tf.assign(var, var_value))
+
+        sess.run(assign_ops)
+        logger.info('Model loaded.')
+        # result = sess.run(output, feed_dict={input: input_data})
+
+        constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['output'])
+        # constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['inpaint_net/post/conv9-2/BiasAdd'])
+        with tf.gfile.FastGFile(pb_file_path, mode='wb') as f:
+            f.write(constant_graph.SerializeToString())
+        # cv2.imwrite(args.output, result[0][:, :, ::-1])
\ No newline at end of file
Index: vgg16.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- vgg16.py	(date 1573192737000)
+++ vgg16.py	(date 1573192737000)
@@ -0,0 +1,99 @@
+import inspect
+import os
+
+import numpy as np
+import tensorflow as tf
+# import time
+import logging
+logger = logging.getLogger()
+
+VGG_MEAN = [103.939, 116.779, 123.68]
+
+
+class Vgg16:
+    def __init__(self, vgg16_npy_path=None):
+        if vgg16_npy_path is None:
+            path = inspect.getfile(Vgg16)
+            path = os.path.abspath(os.path.join(path, os.pardir))
+            path = os.path.join(path, "vgg16.npy")
+            vgg16_npy_path = path
+            print(path)
+
+        self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()
+        self.filt = {}
+        self.conv_biases = {}
+        logger.info("vgg16_npy_path file loaded")
+
+    def build(self, bgr, reuse=False):
+        """
+        load variable from npy to build the VGG
+        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]
+        """
+        # start_time = time.time()
+        logger.info("build vgg model started")
+        with tf.variable_scope("vgg16"):
+            self.conv1_1 = self.conv_layer(bgr, "conv1_1", reuse=reuse)
+            self.conv1_2 = self.conv_layer(self.conv1_1, "conv1_2", reuse=reuse)
+            self.pool1 = self.max_pool(self.conv1_2, 'pool1')
+
+            self.conv2_1 = self.conv_layer(self.pool1, "conv2_1", reuse=reuse)
+            self.conv2_2 = self.conv_layer(self.conv2_1, "conv2_2", reuse=reuse)
+            self.pool2 = self.max_pool(self.conv2_2, 'pool2')
+
+            self.conv3_1 = self.conv_layer(self.pool2, "conv3_1", reuse=reuse)
+            self.conv3_2 = self.conv_layer(self.conv3_1, "conv3_2", reuse=reuse)
+            self.conv3_3 = self.conv_layer(self.conv3_2, "conv3_3", reuse=reuse)
+            self.pool3 = self.max_pool(self.conv3_3, 'pool3')
+
+        self.data_dict = None
+        logger.info("build model finished")
+        return self.pool1, self.pool2, self.pool3
+
+    def _build(self, bgr, reuse=False):
+        """
+        load variable from npy to build the VGG
+        rgb: rgb image [batch, height, width, 3] values scaled [0, 1]
+        """
+        # start_time = time.time()
+        logger.info("build vgg model started")
+        with tf.variable_scope("vgg16"):
+            self.conv1_1 = self.conv_layer(bgr, "conv1_1", reuse=reuse)
+            self.conv1_2 = self.conv_layer(self.conv1_1, "conv1_2", reuse=reuse)
+            self.pool1 = self.max_pool(self.conv1_2, 'pool1')
+
+            self.conv2_1 = self.conv_layer(self.pool1, "conv2_1", reuse=reuse)
+            self.conv2_2 = self.conv_layer(self.conv2_1, "conv2_2", reuse=reuse)
+            self.pool2 = self.max_pool(self.conv2_2, 'pool2')
+
+            self.conv3_1 = self.conv_layer(self.pool2, "conv3_1", reuse=reuse)
+            self.conv3_2 = self.conv_layer(self.conv3_1, "conv3_2", reuse=reuse)
+            self.conv3_3 = self.conv_layer(self.conv3_2, "conv3_3", reuse=reuse)
+            self.pool3 = self.max_pool(self.conv3_3, 'pool3')
+
+        self.data_dict = None
+        logger.info("build model finished")
+        return self.pool1, self.pool2, self.pool3
+
+    def max_pool(self, bottom, name):
+        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)
+
+    def conv_layer(self, bottom, name, reuse=False):
+        with tf.variable_scope(name):
+            # if reuse:
+            #     # tf.get_variable_scope().reuse_variables()
+            #     conv = tf.nn.conv2d(bottom, self.filt[name], [1, 1, 1, 1], padding='SAME')
+            #     bias = tf.nn.bias_add(conv, self.conv_biases[name])
+            # else:
+            self.filt[name] = self.get_conv_filter(name)
+            conv = tf.nn.conv2d(bottom, self.filt[name], [1, 1, 1, 1], padding='SAME')
+            self.conv_biases[name] = self.get_bias(name)
+            bias = tf.nn.bias_add(conv, self.conv_biases[name])
+            relu = tf.nn.relu(bias)
+            return relu
+
+    def get_conv_filter(self, name):
+        return tf.constant(self.data_dict[name][0], name="filter")
+
+    def get_bias(self, name):
+        return tf.constant(self.data_dict[name][1], name="biases")
+
Index: inpaint_test_demo_model_featuremaps.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_test_demo_model_featuremaps.py	(date 1574929609000)
+++ inpaint_test_demo_model_featuremaps.py	(date 1574929609000)
@@ -0,0 +1,183 @@
+import argparse
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = '0'
+import random
+import cv2
+import numpy as np
+import tensorflow as tf
+from inpaint_model import InpaintCAModel, logging
+logger = logging.getLogger()
+from tensorflow.python.framework import graph_util
+
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--image', default='image/test.png', type=str,
+                    help='The filename of image to be completed.')
+parser.add_argument('--mask', default='image/mask/test.png', type=str,
+                    help='The filename of mask, value 255 indicates mask.')
+parser.add_argument('--output', default='image/result-test_eden/test-our.jpg', type=str,
+                    help='Where to write output.')
+parser.add_argument('--checkpoint_dir', default=
+                    'model/convset3',
+                    type=str,
+                    help='The directory of tensorflow checkpoint.')
+
+
+def gaussian_blur(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    pad_w = 0
+    padded = tf.pad(image, [[0, 0], [pad_w, pad_w], [pad_w, pad_w], [0, 0]], mode='CONSTANT')
+    for channel_idx in range(cdim):
+        data_c = padded[:, :, :, channel_idx:(channel_idx + 1)]
+        # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+        g = kernel.reshape((kernel_size, kernel_size, 1, 1))
+        data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+        # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+        data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+        outputs.append(data_c)
+    return tf.concat(outputs,axis=3)
+
+def getNoise(offset_h, offset_w):
+        noiseMask = np.random.rand(height, width)
+        if offset_h and offset_w:
+            noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+        elif offset_h:
+            noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+        elif offset_w:
+            noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+        # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+        noiseMask2 = 1 - noiseMask1
+        noiseMask1 = np.expand_dims(noiseMask1, axis=2)
+        noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=2)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+        return noiseMask1, noiseMask2
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+
+    height = 256
+    width = height
+    channel = 3
+
+    model = InpaintCAModel()
+    image = cv2.imread(args.image)
+    mask = cv2.imread(args.mask)
+
+    image = cv2.resize(image, (height,width))
+    mask = cv2.resize(mask, (height, width))
+
+    h, w, _ = image.shape
+    grid = 8
+    image = image[:h // grid * grid, :w // grid * grid, :]
+    mask = mask[:h // grid * grid, :w // grid * grid, :]
+    print('Shape of image: {}'.format(image.shape))
+
+    image = (image / 127.5 - 1.).astype(np.float32)
+    mask = (mask > 127.5).astype(np.float32)
+
+    image = np.expand_dims(image, 0)
+    mask = np.expand_dims(mask, 0)
+
+    mask = mask[:, :, :, 0:1]
+
+    image = image * (1. - mask)
+
+    input_data = np.concatenate([image, mask], axis=3)
+
+    noise1 = np.random.rand(height, width)
+    noise1 = np.expand_dims(noise1, axis=2)
+    noise1 = np.expand_dims(noise1, axis=0)
+
+    noise2 = np.random.rand(height, width)
+    noise2 = np.expand_dims(noise2, axis=2)
+    noise2 = np.expand_dims(noise2, axis=0)
+
+    noise3 = noise1[:, :127, :127, :]
+    noise4 = noise2[:, :256, 256:, :]
+
+    noiseMask1, noiseMask2 = getNoise(2,1)
+    noiseMask1_1, noiseMask2_1 = getNoise(None, 2)
+
+    sess_config = tf.ConfigProto()
+    with tf.Session(config=sess_config) as sess:
+        input = tf.placeholder(tf.float32, shape=[1, height, width, 4], name='input')
+        # input = tf.placeholder(tf.float32, shape=[1, height, width*3, 3], name='input')
+        output, x1, x2, x3, x4, x1Deconv, x2Deconv, x3Deconv, x4Deconv \
+            = model.build_inpaint_net(input, noise3= noise3 ,is_test=True)
+        # output = tf.clip_by_value(output, -1, 1)
+        # R,B,G,_one,_mask,_guide = tf.split(input,axis=3, num_or_size_splits=4)
+        # img = tf.concat([R,B,G], axis=3)
+        # output = img*(1.-_mask) + output*_mask
+        output = (output + 1.) * 127.5
+        output = tf.reverse(output, [-1])
+        output = tf.saturate_cast(output, tf.uint8)
+        # load pretrained model
+        vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+        assign_ops = []
+        for var in vars_list:
+            vname = var.name
+            from_name = vname
+            var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)
+            assign_ops.append(tf.assign(var, var_value))
+        sess.run(assign_ops)
+        print('Model loaded.')
+        res, x1, x2, x3, x4, x1Deconv, x2Deconv, x3Deconv, x4Deconv = \
+            sess.run([output, x1, x2, x3, x4, x1Deconv, x2Deconv, x3Deconv, x4Deconv],
+                                      feed_dict={input: input_data})
+        cv2.imwrite(args.output, res[0][:, :, ::-1])
+
+        # x1 = x1[0][:,:,0:3]
+        # x1 = (x1 - np.min(x1))/(np.max(x1) - np.min(x1))*255
+        # cv2.imwrite('image/x1-our.jpg', x1)
+        #
+        # x2 = x2[0][:, :, 0:3]
+        # x2 = (x2 - np.min(x2)) / (np.max(x2) - np.min(x2)) * 255
+        # cv2.imwrite('image/x2-our.jpg', x2)
+        #
+        # x3 = x3[0][:, :, 0:3]
+        # x3 = (x3 - np.min(x3)) / (np.max(x3) - np.min(x3)) * 255
+        # cv2.imwrite('image/x3-our.jpg', x3)
+        #
+        # x4 = x4[0][:, :, 0:3]
+        # x4 = (x4 - np.min(x4)) / (np.max(x4) - np.min(x4)) * 255
+        # cv2.imwrite('image/x4-our.jpg', x4)
+        #
+        # randomx = randomx[0][:, :, 0:3]
+        # randomx = (randomx - np.min(randomx)) / (np.max(randomx) - np.min(randomx)) * 255
+        # cv2.imwrite('image/randomx-our.jpg', randomx)
+
+        # index = 1
+        # for i in (x1, x2, x3, x4):
+        #     i = i[0][:,:,0:3]
+        #     i = (i - np.min(i)) / (np.max(i) - np.min(i)) * 255
+        #     dir = 'image/x' + str(index) + '-deconv.jpg'
+        #     index = index + 1
+        #     cv2.imwrite(dir, i)
+
+        f = open('image/f.txt', 'w')
+        name = ['x1', 'x1Deconv', 'x2', 'x2Deconv', 'x3', 'x3Deconv', 'x4', 'x4Deconv']
+        index = 0
+        for i in (x1, x1Deconv, x2, x2Deconv, x3, x3Deconv, x4, x4Deconv):
+            shapelist = np.shape(i)
+            h = shapelist[1]
+            w = shapelist[2]
+
+
+            for j in range(0,9):
+                tmp = i[0][:, :, j:j+1]
+                tmp = np.reshape(tmp,(h, w))
+                f.write(name[index])
+                f.write(' featuremap: {} \n'.format(j))
+
+                for m in range(0,h):
+                    for n in range(0,w):
+                        num = str(tmp[m][n])
+                        f.write(num)
+                        f.write(',')
+                    f.write('\n')
+                f.write('\n')
+            index = index + 1
+                # np.savetxt('image/featuremap.txt', tmp, fmt='%.f', delimiter=",")
\ No newline at end of file
Index: vgg19.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- vgg19.py	(date 1572395093000)
+++ vgg19.py	(date 1572395093000)
@@ -0,0 +1,192 @@
+import os
+import tensorflow as tf
+import numpy as np
+import inspect
+import logging
+logger = logging.getLogger()
+
+
+VGG_MEAN = [103.939, 116.779, 123.68]
+
+
+class Vgg19:
+    def __init__(self, vgg19_npy_path=None):
+        if vgg19_npy_path is None:
+            path = inspect.getfile(Vgg19)
+            path = os.path.abspath(os.path.join(path, os.pardir))
+            path = os.path.join(path, "vgg19.npy")
+            vgg19_npy_path = path
+            print(vgg19_npy_path)
+
+        self.data_dict = np.load(vgg19_npy_path, encoding='latin1').item()
+        # self.filt = {}
+        # self.conv_biases = {}
+        logger.info("vgg19_npy_path file loaded")
+
+    def build(self, rgb, reuse=tf.AUTO_REUSE):
+
+        logger.info("build vgg model started")
+        # rgb_scaled = rgb * 255.0
+        rgb_scaled = (rgb+1.0) * 127.5
+
+        # Convert RGB to BGR
+        # red, green, blue\
+        blue, green, red = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)
+        # assert red.get_shape().as_list()[1:] == [512, 512, 1]
+        # assert green.get_shape().as_list()[1:] == [512, 512, 1]
+        # assert blue.get_shape().as_list()[1:] == [512, 512, 1]
+        bgr = tf.concat(axis=3, values=[
+            blue - VGG_MEAN[0],
+            green - VGG_MEAN[1],
+            red - VGG_MEAN[2],
+        ])
+        assert bgr.get_shape().as_list()[1:] == [256, 256, 3]
+        with tf.variable_scope("vgg19", reuse=reuse):
+            self.conv1_1 = self.conv_layer(bgr, "conv1_1")
+            self.conv1_2 = self.conv_layer(self.conv1_1, "conv1_2")
+            self.pool1 = self.max_pool(self.conv1_2, 'pool1')
+
+            self.conv2_1 = self.conv_layer(self.pool1, "conv2_1")
+            self.conv2_2 = self.conv_layer(self.conv2_1, "conv2_2")
+            self.pool2 = self.max_pool(self.conv2_2, 'pool2')
+
+            self.conv3_1 = self.conv_layer(self.pool2, "conv3_1")
+            self.conv3_2 = self.conv_layer(self.conv3_1, "conv3_2")
+            self.conv3_3 = self.conv_layer(self.conv3_2, "conv3_3")
+            self.conv3_4 = self.conv_layer(self.conv3_3, "conv3_4")
+            self.pool3 = self.max_pool(self.conv3_4, 'pool3')
+
+            self.conv4_1 = self.conv_layer(self.pool3, "conv4_1")
+            self.conv4_2 = self.conv_layer(self.conv4_1, "conv4_2")
+            self.conv4_3 = self.conv_layer(self.conv4_2, "conv4_3")
+            self.conv4_4 = self.conv_layer(self.conv4_3, "conv4_4")
+            self.pool4 = self.max_pool(self.conv4_4, 'pool4')
+
+            self.conv5_1 = self.conv_layer(self.pool4, "conv5_1")
+            self.conv5_2 = self.conv_layer(self.conv5_1, "conv5_2")
+            self.conv5_3 = self.conv_layer(self.conv5_2, "conv5_3")
+            self.conv5_4 = self.conv_layer(self.conv5_3, "conv5_4")
+            # self.pool5 = self.max_pool(self.conv5_4, 'pool5')
+        self.data_dict = None
+        logger.info("build model finished")
+        return self.conv1_2, self.conv2_2, self.conv3_4, self.conv4_4, self.conv5_4
+
+    def avg_pool(self, bottom, name):
+        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)
+
+    def max_pool(self, bottom, name):
+        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)
+
+    def conv_layer(self, bottom, name):
+        with tf.variable_scope(name):
+            filt = self.get_conv_filter(name)
+
+            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')
+
+            conv_biases = self.get_bias(name)
+            bias = tf.nn.bias_add(conv, conv_biases)
+
+            relu = tf.nn.relu(bias)
+            return relu
+
+    def fc_layer(self, bottom, name):
+        with tf.variable_scope(name):
+            shape = bottom.get_shape().as_list()
+            dim = 1
+            for d in shape[1:]:
+                dim *= d
+            x = tf.reshape(bottom, [-1, dim])
+
+            weights = self.get_fc_weight(name)
+            biases = self.get_bias(name)
+
+            # Fully connected layer. Note that the '+' operation automatically
+            # broadcasts the biases.
+            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)
+
+            return fc
+
+    def get_conv_filter(self, name):
+        return tf.constant(self.data_dict[name][0], name="filter")
+
+    def get_bias(self, name):
+        return tf.constant(self.data_dict[name][1], name="biases")
+
+    def get_fc_weight(self, name):
+        return tf.constant(self.data_dict[name][0], name="weights")
+
+
+import inspect
+import os
+
+import numpy as np
+import tensorflow as tf
+# import time
+import logging
+logger = logging.getLogger()
+
+VGG_MEAN = [103.939, 116.779, 123.68]
+
+
+class Vgg16:
+    def __init__(self, vgg16_npy_path=None):
+        if vgg16_npy_path is None:
+            path = inspect.getfile(Vgg16)
+            path = os.path.abspath(os.path.join(path, os.pardir))
+            path = os.path.join(path, "vgg16.npy")
+            vgg16_npy_path = path
+            print(path)
+
+        self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()
+        self.filt = {}
+        self.conv_biases = {}
+        logger.info("vgg16_npy_path file loaded")
+
+    def build(self, bgr, reuse=False):
+        """
+        load variable from npy to build the VGG
+        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]
+        """
+        # start_time = time.time()
+        logger.info("build vgg model started")
+        with tf.variable_scope("vgg16"):
+            self.conv1_1 = self.conv_layer(bgr, "conv1_1", reuse=reuse)
+            self.conv1_2 = self.conv_layer(self.conv1_1, "conv1_2", reuse=reuse)
+            self.pool1 = self.max_pool(self.conv1_2, 'pool1')
+
+            self.conv2_1 = self.conv_layer(self.pool1, "conv2_1", reuse=reuse)
+            self.conv2_2 = self.conv_layer(self.conv2_1, "conv2_2", reuse=reuse)
+            self.pool2 = self.max_pool(self.conv2_2, 'pool2')
+
+            self.conv3_1 = self.conv_layer(self.pool2, "conv3_1", reuse=reuse)
+            self.conv3_2 = self.conv_layer(self.conv3_1, "conv3_2", reuse=reuse)
+            self.conv3_3 = self.conv_layer(self.conv3_2, "conv3_3", reuse=reuse)
+            # self.pool3 = self.max_pool(self.conv3_3, 'pool3')
+
+        self.data_dict = None
+        logger.info("build model finished")
+        return self.conv1_2, self.conv2_2, self.conv3_3
+
+    def max_pool(self, bottom, name):
+        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)
+
+    def conv_layer(self, bottom, name, reuse=False):
+        with tf.variable_scope(name):
+            # if reuse:
+            #     # tf.get_variable_scope().reuse_variables()
+            #     conv = tf.nn.conv2d(bottom, self.filt[name], [1, 1, 1, 1], padding='SAME')
+            #     bias = tf.nn.bias_add(conv, self.conv_biases[name])
+            # else:
+            self.filt[name] = self.get_conv_filter(name)
+            conv = tf.nn.conv2d(bottom, self.filt[name], [1, 1, 1, 1], padding='SAME')
+            self.conv_biases[name] = self.get_bias(name)
+            bias = tf.nn.bias_add(conv, self.conv_biases[name])
+            relu = tf.nn.relu(bias)
+            return relu
+
+    def get_conv_filter(self, name):
+        return tf.constant(self.data_dict[name][0], name="filter")
+
+    def get_bias(self, name):
+        return tf.constant(self.data_dict[name][1], name="biases")
+
Index: inpaint_test_demo_pb.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_test_demo_pb.py	(date 1578357929000)
+++ inpaint_test_demo_pb.py	(date 1578357929000)
@@ -0,0 +1,246 @@
+import os
+
+from skimage.feature import canny
+
+os.environ["CUDA_VISIBLE_DEVICE"] = "0"
+import argparse
+import cv2
+import numpy as np
+import tensorflow as tf
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--image', default='/mnt/disk/mm.bai/inpaiting/dsp-face/test8/image/fcfed5c2baf56ffa866e293812b8d894.jpg', type=str,
+                    help='The filename of image to be completed.')
+parser.add_argument('--mask', default='/mnt/disk/mm.bai/inpaiting/dsp-face/test8/image/mask/fcfed5c2baf56ffa866e293812b8d894.jpg', type=str,
+                    help='The filename of mask, value 255 indicates mask.')
+parser.add_argument('--guide', default='image/guide.jpg', type=str,
+                    help='The filename of guidelines, value 0 indicates guide line.')
+parser.add_argument('--output', default='/mnt/disk/mm.bai/inpaiting/dsp-face/test8/image/result-paper/fcfed_paper.jpg', type=str,
+                    help='Where to write output.')
+parser.add_argument('--pbdir', default='/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/ffhqpaper-resize-2500.pb.pb', type=str,
+                    help='Where to import pb model.')
+
+
+def getNoise(offset_h, offset_w):
+    noiseMask = np.random.rand(height, width)
+    if offset_h:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    else:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=2)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=2)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    return noiseMask1, noiseMask2
+
+def gaussian_blur(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    pad_w = 0
+    padded = tf.pad(image, [[0, 0], [pad_w, pad_w], [pad_w, pad_w], [0, 0]], mode='CONSTANT')
+    # for channel_idx in range(cdim):
+    #     data_c = padded[:, :, :, channel_idx:(channel_idx + 1)]
+    #     # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+    #     g = kernel.reshape((kernel_size, kernel_size, 1, 1))
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     # g = tf.reshape(kernel, [kernel_size, kernel_size, 1, 1])
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     outputs.append(data_c)
+
+    k = kernel.reshape((kernel_size, kernel_size, 1, 1))
+    output = tf.nn.depthwise_conv2d(padded, k, [1,1,1,1], 'SAME')
+    return output
+
+def bilateralfilter(image, texture, sigma_s, sigma_r):
+    r = int(np.ceil(3 * sigma_s))
+    # Image padding
+    if image.ndim == 3:
+        h, w, ch = image.shape
+        I = np.pad(image, ((r, r), (r, r), (0, 0)), 'symmetric').astype(np.float32)
+    elif image.ndim == 2:
+        h, w = image.shape
+        I = np.pad(image, ((r, r), (r, r)), 'symmetric').astype(np.float32)
+    else:
+        print('Input image is not valid!')
+        return image
+    # Check texture size and do padding
+    if texture.ndim == 3:
+        ht, wt, cht = texture.shape
+        if ht != h or wt != w:
+            print('The guidance image is not aligned with input image!')
+            return image
+        T = np.pad(texture, ((r, r), (r, r), (0, 0)), 'symmetric').astype(np.int32)
+    elif texture.ndim == 2:
+        ht, wt = texture.shape
+        if ht != h or wt != w:
+            print('The guidance image is not aligned with input image!')
+            return image
+        T = np.pad(texture, ((r, r), (r, r)), 'symmetric').astype(np.int32)
+    # Pre-compute
+    output = np.zeros_like(image)
+    scaleFactor_s = 1 / (2 * sigma_s * sigma_s)
+    scaleFactor_r = 1 / (2 * sigma_r * sigma_r)
+    # A lookup table for range kernel
+    LUT = np.exp(-np.arange(256) * np.arange(256) * scaleFactor_r)
+    # Generate a spatial Gaussian function
+    x, y = np.meshgrid(np.arange(2 * r + 1) - r, np.arange(2 * r + 1) - r)
+    kernel_s = np.exp(-(x * x + y * y) * scaleFactor_s)
+    # Main body
+    if I.ndim == 2 and T.ndim == 2:     # I1T1 filter
+        for y in range(r, r + h):
+            for x in range(r, r + w):
+                wgt = LUT[np.abs(T[y - r:y + r + 1, x - r:x + r + 1] - T[y, x])] * kernel_s
+                output[y - r, x - r] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1]) / np.sum(wgt)
+    elif I.ndim == 3 and T.ndim == 2:     # I3T1 filter
+        for y in range(r, r + h):
+            for x in range(r, r + w):
+                wgt = LUT[abs(T[y - r:y + r + 1, x - r:x + r + 1] - T[y, x])] * kernel_s
+                wacc = np.sum(wgt)
+                output[y - r, x - r, 0] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1, 0]) / wacc
+                output[y - r, x - r, 1] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1, 1]) / wacc
+                output[y - r, x - r, 2] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1, 2]) / wacc
+    elif I.ndim == 3 and T.ndim == 3:     # I3T3 filter
+        for y in range(r, r + h):
+            for x in range(r, r + w):
+                wgt = LUT[abs(T[y - r:y + r + 1, x - r:x + r + 1, 0] - T[y, x, 0])] * \
+                      LUT[abs(T[y - r:y + r + 1, x - r:x + r + 1, 1] - T[y, x, 1])] * \
+                      LUT[abs(T[y - r:y + r + 1, x - r:x + r + 1, 2] - T[y, x, 2])] * \
+                      kernel_s
+                wacc = np.sum(wgt)
+                output[y - r, x - r, 0] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1, 0]) / wacc
+                output[y - r, x - r, 1] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1, 1]) / wacc
+                output[y - r, x - r, 2] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1, 2]) / wacc
+    elif I.ndim == 2 and T.ndim == 3:     # I1T3 filter
+        for y in range(r, r + h):
+            for x in range(r, r + w):
+                wgt = LUT[abs(T[y - r:y + r + 1, x - r:x + r + 1, 0] - T[y, x, 0])] * \
+                      LUT[abs(T[y - r:y + r + 1, x - r:x + r + 1, 1] - T[y, x, 1])] * \
+                      LUT[abs(T[y - r:y + r + 1, x - r:x + r + 1, 2] - T[y, x, 2])] * \
+                      kernel_s
+                output[y - r, x - r] = np.sum(wgt * I[y - r:y + r + 1, x - r:x + r + 1]) / np.sum(wgt)
+    else:
+        print('Something wrong!')
+        return image
+
+    # return np.clip(output, 0, 255)
+    return output
+
+def bilateral_blur(output):
+
+    return
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+
+    height = 512
+    width = height
+    channel = 3
+    noise = np.random.rand(height, width)
+
+    image = cv2.imread(args.image)
+    mask = cv2.imread(args.mask)
+
+    image = cv2.resize(image, (height,width))
+    mask = cv2.resize(mask, (height, width))
+
+    h, w, _ = image.shape
+    grid = 8
+    image = image[:h // grid * grid, :w // grid * grid, :]
+    mask = mask[:h // grid * grid, :w // grid * grid, :]
+    print('Shape of image: {}'.format(image.shape))
+
+    image = (image / 127.5 - 1.).astype(np.float32)
+    mask = (mask > 127.5).astype(np.float32)
+
+    image = np.expand_dims(image, 0)
+    mask = np.expand_dims(mask, 0)
+
+    noiseMask1, noiseMask2 = getNoise(2, 1)
+    noiseMask1_1, noiseMask2_1 = getNoise(None, 2)
+
+    one_s = np.ones_like(mask)
+    image_original = image
+    image = image * (1. - mask)
+
+    # input_image = np.concatenate([image, one_s[:,:,:,0:1], mask[:,:,:,0:1], noise], axis=3)
+    input_image = np.concatenate([image, mask[:, :, :, 0:1]], axis=3)
+
+    # 3*3 gauss filter
+    gauss_filter_3 = np.array(
+        [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+
+    gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+
+    with tf.Graph().as_default():
+        output_graph_def = tf.GraphDef()
+        with open(args.pbdir, "rb") as f:
+            output_graph_def.ParseFromString(f.read())
+            _ = tf.import_graph_def(output_graph_def, name="")
+        config = tf.ConfigProto(allow_soft_placement=True)
+        with tf.Session(config=config) as sess:
+            sess.run(tf.global_variables_initializer())
+            input_image_tensor1 = sess.graph.get_tensor_by_name("input:0")
+            # input_image_tensor2 = sess.graph.get_tensor_by_name("input2:0")
+            # model_otput = sess.graph.get_tensor_by_name("inpaint_net/post/conv9-2/BiasAdd:0")#
+            output = sess.graph.get_tensor_by_name("output:0")
+            # output = tf.clip_by_value(model_otput, -1., 1.)
+            # output_tensor = tf.tanh(model_otput)
+            post_processing = 0
+            if post_processing:
+                x1 = output[:, :-2, :-1, :]
+                # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+                x2 = output[:, 2:, 1:, :]
+                # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+                with tf.variable_scope(name_or_scope='x1Multi'):
+                    x1 = x1 * noiseMask1
+                with tf.variable_scope(name_or_scope='x2Multi'):
+                    x2 = x2 * noiseMask2
+                # with tf.variable_scope(name_or_scope='x1Multix2'):
+                output = tf.add(x1, x2, name='x1Multix2')
+                output = tf.pad(output, paddings=[[0, 0], [1, 1], [1, 0], [0, 0]], mode="CONSTANT", name='output_1')
+
+                x1 = output[:, :, :-2, :]
+                # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+                x2 = output[:, :, 2:, :]
+                # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+                with tf.variable_scope(name_or_scope='x1Multi_1'):
+                    x1 = x1 * noiseMask1_1
+                with tf.variable_scope(name_or_scope='x2Multi_1'):
+                    x2 = x2 * noiseMask2_1
+                # with tf.variable_scope(name_or_scope='x1Multix2'):
+                output = tf.add(x1, x2, name='x1Multix2_1')
+                output = tf.pad(output, paddings=[[0, 0], [0, 0], [0, 2], [0, 0]], mode="CONSTANT", name='output_2')
+            blur = 0
+            if blur:
+                # Bilateral blur
+                # output = bilateral_blur(output)
+                # Gauss blur
+                output = gaussian_blur(output, gauss_filter_3, 3)
+
+
+            output = tf.clip_by_value(output, -1., 1.)
+            B,G,R, _mask = tf.split(input_image_tensor1, axis=3, num_or_size_splits=4)
+            img = tf.concat([B,G,R], axis=3)
+
+            # img_mask = input_image_original*_mask
+            # gray_img = tf.image.rgb_to_grayscale(img_mask)
+
+            output_tensor = img*(1.-_mask) + output*_mask
+            # output_tensor = output*_mask*(1.-edge_mask)
+            output_tensor = (output_tensor + 1.) * 127.5
+            output_tensor = tf.saturate_cast(output_tensor, tf.uint8)
+            result = sess.run([output_tensor], feed_dict={input_image_tensor1: input_image})#,
+
+            img_output = result[0][-1]
+
+            # bilateral filter
+            # sigma_s = 1
+            # sigma_r = 0.1 * 255
+            # img_gray = cv2.cvtColor(img_output, cv2.COLOR_BGR2GRAY)
+            # img_output = bilateralfilter(img_output, img_gray, sigma_s, sigma_r)
+
+            cv2.imwrite(args.output, img_output)
+            print("test done")
\ No newline at end of file
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/vcs.xml	(date 1572590838000)
+++ .idea/vcs.xml	(date 1572590838000)
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
Index: inpaint_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_model.py	(date 1578389656000)
+++ inpaint_model.py	(date 1578389656000)
@@ -0,0 +1,565 @@
+from tensorflow.contrib.framework.python.ops import arg_scope
+from neuralgym.models import Model
+from inpaint_layer import *
+from inpaint_loss import *
+from inpaint_mask import *
+from inpaint_gan_loss import InpaintGAN_net
+
+logger = logging.getLogger()
+epsilon = (1e-14)
+
+class InpaintCAModel(Model):
+    def __init__(self):
+        super().__init__('InpaintCAModel')
+
+    def random_offset(self, x, cnum, noise1, noise2, name):
+        with tf.variable_scope(name_or_scope=name):
+            # gauss_filter_3 = np.array(
+            #     [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+            # random offset 1
+            x = tvnoise(x, noise1, 1, 1)
+            # x = tvnoise(x, noise2, None, 1)
+
+            # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+
+            # x = tf.layers.conv2d(x, cnum, 1, padding='valid', activation=tf.nn.elu, name='fcn')
+            # x = gaussian_blur(x, gauss_filter_3, 3)
+            # x = tf.layers.conv2d(x, 3, 3, 1, padding='same', name='filter')
+            return x
+
+    def offset(self, x, cnum, hoffset1, woffset1, hoffset2, woffset2, name='offset'):
+        with tf.variable_scope(name_or_scope=name):
+            # random offset 1
+            if hoffset1 != 0 and woffset1 != 0:
+                x1_1 = x[:,:-hoffset1,:-woffset1,:]
+                x1_2 = x[:,hoffset1:,woffset1:,:]
+                # x1_1 = tf.layers.separable_conv2d(x1_1, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu,name='normal_1_1')
+                x1_2 = tf.layers.separable_conv2d(x1_2, cnum, 3, 1, padding='same', activation=tf.nn.sigmoid, name='normal_1_2')
+
+            elif hoffset1 != 0 and woffset1 == 0:
+                x1_1 = x[:,:-hoffset1,:,:]
+                x1_2 = x[:,hoffset1:,:,:]
+                # x1_1 = tf.layers.conv2d(x1_1, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu,name='normal_1_1')
+                x1_2 = tf.layers.separable_conv2d(x1_2, cnum, 3, 1, padding='same', activation=tf.nn.sigmoid, name='normal_1_2')
+
+            elif hoffset1 == 0 and woffset1 != 0:
+                x1_1 = x[:,:,:-woffset1,:]
+                x1_2 = x[:,:,woffset1:,:]
+                # x1_1 = tf.layers.conv2d(x1_1, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu,name='normal_1_1')
+                x1_2 = tf.layers.separable_conv2d(x1_2, cnum, 3, 1, padding='same', activation=tf.nn.sigmoid, name='normal_1_2')
+
+            x1 = x1_1 * x1_2
+
+            if hoffset2 != 0 and woffset2 != 0:
+                x2_1 = x1[:, :-hoffset2, :-woffset2, :]
+                x2_2 = x1[:, hoffset2:, woffset2:, :]
+                # x2_1 = tf.layers.conv2d(x2_1, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='normal_2_1')
+                x2_2 = tf.layers.separable_conv2d(x2_2, cnum, 3, 1, padding='same', activation=tf.nn.sigmoid, name='normal_2_2')
+            elif hoffset2 != 0 and woffset2 == 0:
+                x2_1 = x1[:, :-hoffset2, :, :]
+                x2_2 = x1[:, hoffset2:, :, :]
+                # x2_1 = tf.layers.conv2d(x2_1, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='normal_2_1')
+                x2_2 = tf.layers.separable_conv2d(x2_2, cnum, 3, 1, padding='same', activation=tf.nn.sigmoid, name='normal_2_2')
+            elif hoffset2 == 0 and woffset2 != 0:
+                x2_1 = x1[:, :, :-woffset2, :]
+                x2_2 = x1[:, :, woffset2:, :]
+                # x2_1 = tf.layers.conv2d(x2_1, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='normal_2_1')
+                x2_2 = tf.layers.separable_conv2d(x2_2, cnum, 3, 1, padding='same', activation=tf.nn.sigmoid, name='normal_2_2')
+            x2 = x2_1 * x2_2
+
+            # hpad = (hoffset1 + hoffset2)
+            # wpad = (woffset2 + woffset1)
+
+            # x = tf.pad(x2, paddings=[[0, 0], [hpad, 0], [0, wpad], [0, 0]], mode='CONSTANT', name='padding')
+            # x = tf.layers.conv2d(x, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn')
+            return x2
+
+    def _build_inpaint_net(self, x, mask=None,
+                          noise1 = None, noise2 = None, noise3 = None, noise4 = None,
+                          is_test=False, config=None, reuse=False,
+                          training=True, padding='SAME', name='inpaint_net'):
+        """Inpaint network.
+
+        Args:
+            x: incomplete image, [-1, 1]
+            batch_mask: batch_mask region {0, 1}
+        Returns:
+            [-1, 1] as predicted image
+        """
+        if is_test:
+            # B,G,R,ones_x,mask, noise = tf.split(x, num_or_size_splits=6,axis=3)
+            # input_x = tf.concat([B,G,R,ones_x,mask], axis=3)
+            input_x = x
+        else:
+            # ones_x = tf.ones_like(x)[:, :, :, 0:1]
+            x = tf.concat([x, mask], axis=3)
+            input_x = x
+        # two stage network
+        cnum = 32
+        offset = 0
+        with tf.variable_scope(name, reuse=tf.AUTO_REUSE), \
+                arg_scope([gen_deconv, gen_gate_conv],
+                          training=training):
+            #stage 1
+            # encode
+            # 128
+            x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+            # x_1 = tf.layers.batch_normalization(x_1)
+            # 64
+            x_2 = gen_gate_conv(x_1, 2*cnum, 5, 2, name='conv2')
+            # x_2 = tf.layers.batch_normalization(x_2)
+            # 32
+            x_3 = gen_gate_conv(x_2, 4*cnum, 3, 2, name='conv3')
+            # x_3 = tf.layers.batch_normalization(x_3)
+            # 16
+            x_4 = gen_gate_conv(x_3, 4*cnum, 3, 2, name='conv4')
+            # x_4 = tf.layers.batch_normalization(x_4)
+
+            # 32
+            # x = gen_resize(x_4, 4*cnum, name='upsample1')
+            x = gen_normal_deconv(x_4, 4*cnum, name='upsample1')
+            x = tf.layers.conv2d(x, 2*cnum, 3, 1, activation=tf.nn.leaky_relu, padding='same',name='up_conv1')
+            # x = tf.layers.batch_normalization(x)
+
+            # 64
+            # x = gen_resize(x, 2*cnum, name='upsample2')
+            x = gen_normal_deconv(x, 2*cnum, name='upsample2')
+
+            if offset:
+                x = self.offset(x, 2*cnum, 1, 2, 1, 0,name='offset1')
+                x = tf.pad(x, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, 2*cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn1')
+
+            x = tf.concat([x, x_2], axis=3, name='concat8')
+            x = gen_gate_conv(x, 2*cnum, 3, 1, padding='same', name='conv7')
+            # x = tf.layers.batch_normalization(x)
+
+            # 128
+            # x = gen_resize(x, cnum, name='upsample3')
+            x = gen_normal_deconv(x, cnum, name='upsample3')
+            # x = self.random_offset(x, cnum, noise3, noise4, name='offset1')
+
+            if offset:
+                x = self.offset(x, cnum, 2, 1, 0, 1, name='offset2')
+                x = tf.pad(x, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn2')
+
+            x = tf.concat([x, x_1], axis=3, name='concat8')
+            x = gen_gate_conv(x, cnum, 3, 1, padding='same', name='conv8')
+            # x = tf.layers.batch_normalization(x)
+
+            # 256
+            # x = gen_resize(x, cnum, name='upsample4')
+            x = gen_normal_deconv(x, cnum, name='upsample4')
+            # x = self.random_offset(x, cnum,  noise1, noise2, name='offset2')
+            if offset:
+                x = self.offset(x, cnum, 1, 0, 0, 1, name='offset3')
+                x = tf.pad(x, paddings=[[0, 0], [1, 0], [0, 1], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn3')
+
+            x = tf.concat([x, input_x], axis=3, name='concat9')
+            x = tf.layers.conv2d(x, 3, 3, 1, padding='same', name='conv10')
+            if not is_test:
+                x = tf.clip_by_value(x, -1., 1., name='result')
+        return x
+
+
+    def __build_inpaint_net(self, x, mask=None, noise1 = None, noise2 = None, noise3 = None, noise4 = None,
+                          is_test=False, config=None, reuse=False,
+                          training=True, padding='SAME', name='inpaint_net'):
+        """Inpaint network.
+
+        Args:
+            x: incomplete image, [-1, 1]
+            batch_mask: batch_mask region {0, 1}
+        Returns:
+            [-1, 1] as predicted image
+        """
+        if is_test:
+            # B,G,R,ones_x,mask, noise = tf.split(x, num_or_size_splits=6,axis=3)
+            # input_x = tf.concat([B,G,R,ones_x,mask], axis=3)
+            input_x = x
+        else:
+            # ones_x = tf.ones_like(x)[:, :, :, 0:1]
+            x = tf.concat([x, mask], axis=3)
+            input_x = x
+        # two stage network
+        cnum = 32
+        with tf.variable_scope(name, reuse=reuse), \
+                arg_scope([gen_deconv, gen_gate_conv],
+                          training=training):
+            #stage 1
+            # encode
+            x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+            x_2 = gen_gate_conv(x_1, 2*cnum, 5, 2, name='conv2')
+            x_3 = gen_gate_conv(x_2, 4*cnum, 3, 2, name='conv3')
+            x_4 = gen_gate_conv(x_3, 4*cnum, 3, 2, name='conv4')
+
+            x = gen_normal_deconv(x_4, 4*cnum, name='upsample2')
+            x = tf.concat([x, x_3], axis=3, name='concat2')
+            x = gen_gate_conv(x, 2*cnum, 3, 1, padding='same',name='conv6')
+
+            x = gen_normal_deconv(x, 2*cnum, name='upsample3')
+            x = tf.concat([x, x_2], axis=3, name='concat3')
+            x = gen_gate_conv(x, cnum, 3, 1, padding='same',name='conv7')
+
+            x = gen_normal_deconv(x, cnum, name='upsample4')
+            x = tf.concat([x, x_1], axis=3, name='concat8')
+            x = tf.layers.conv2d(x, cnum, 3, 1, padding='same', name='conv8')
+
+            x = gen_normal_deconv(x, cnum, name='upsample5')
+            x = tf.concat([x, input_x], axis=3, name='concat9')
+            x = tf.layers.conv2d(x, 3, 3, 1, padding='same', name='conv9')
+
+            if not is_test:
+                x = tf.clip_by_value(x, -1., 1., name='result')
+        return x
+
+    def build_inpaint_net(self, x, mask=None,
+                          noise1 = None, noise2 = None, noise3 = None, noise4 = None,
+                          is_test=False, config=None, reuse=False,
+                          training=True, padding='SAME', name='inpaint_net'):
+        """Inpaint network.
+
+        Args:
+            x: incomplete image, [-1, 1]
+            batch_mask: batch_mask region {0, 1}
+        Returns:
+            [-1, 1] as predicted image
+        """
+        if is_test:
+            # B,G,R,ones_x,mask, noise = tf.split(x, num_or_size_splits=6,axis=3)
+            # input_x = tf.concat([B,G,R,ones_x,mask], axis=3)
+            input_x = x
+        else:
+            # ones_x = tf.ones_like(x)[:, :, :, 0:1]
+            x = tf.concat([x, mask], axis=3)
+            input_x = x
+        # two stage network
+        cnum = 32
+        offset3 = 0
+        offset2 = 0
+        offset1 = 0
+
+        randomoff2 = 1
+        randomoff3 = 0
+
+        with tf.variable_scope(name, reuse=tf.AUTO_REUSE), \
+                arg_scope([gen_deconv, gen_gate_conv, gen_normal_deconv_pad],
+                          training=training):
+            #stage 1
+            # encode
+            # 128
+            x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+            x_1 = tf.layers.batch_normalization(x_1)
+            # 64
+            x_2 = gen_gate_conv(x_1, 2*cnum, 5, 2, name='conv2')
+            x_2 = tf.layers.batch_normalization(x_2)
+            # 32
+            x_3 = gen_gate_conv(x_2, 4*cnum, 3, 2, name='conv3')
+            x_3 = tf.layers.batch_normalization(x_3)
+            # 16
+            x_4 = gen_gate_conv(x_3, 4*cnum, 3, 2, name='conv4')
+            x_4 = tf.layers.batch_normalization(x_4)
+
+            # 32
+            # x = gen_resize(x_4, 4*cnum, name='upsample1')
+            x = gen_normal_deconv(x_4, 4*cnum, name='upsample1')
+            x = tf.layers.conv2d(x, 4*cnum, 3, 1, activation=tf.nn.leaky_relu, padding='same',name='up_conv1')
+            x = tf.layers.batch_normalization(x)
+
+            # 64
+            # x = gen_resize(x, 2*cnum, name='upsample2')
+            x = gen_normal_deconv(x, 4*cnum, name='upsample2')
+
+            if offset3:
+                x = self.offset(x, 4*cnum, 2, 1, 0, 1,name='offset3')
+                x = tf.pad(x, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, 4*cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn1')
+
+            x = tf.concat([x, x_2], axis=3, name='concat8')
+            x = gen_gate_conv(x, 4*cnum, 3, 1, padding='same', name='up_conv2')
+            x = tf.layers.batch_normalization(x)
+
+            # 128
+            # x = gen_resize(x, cnum, name='upsample3')
+            x = gen_normal_deconv(x, 2*cnum, name='upsample3')
+            # x = self.random_offset(x, cnum, noise3, noise4, name='offset1')
+
+            if randomoff2:
+                x = self.random_offset(x, cnum,  noise3, noise4, name='randomoffset1')
+
+            if offset2:
+                x = self.offset(x, 2*cnum, 1, 2, 1, 0, name='offset2')
+                x = tf.pad(x, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, 2*cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn2')
+
+            if is_test:
+                x_1 = x_1[:,0:255,0:255,:]
+            else:
+                x_1 = x_1[:,0:127,0:127,:]
+
+            x = tf.concat([x, x_1], axis=3, name='concat8')
+            x = gen_gate_conv(x, cnum, 3, 1, padding='same', name='conv8')
+            x = tf.layers.batch_normalization(x)
+
+            # 256
+            # x = gen_resize(x, cnum, name='upsample4')
+            x = gen_normal_deconv(x, cnum, name='upsample4')
+
+            if randomoff3:
+                x = self.random_offset(x, cnum,  noise1, noise2, name='randomoffset1')
+
+            if offset1:
+                x = self.offset(x, cnum, 1, 0, 0, 1, name='offset1')
+                x = tf.pad(x, paddings=[[0, 0], [0, 1], [0, 1], [0, 0]], mode='CONSTANT', name='padding')
+                x = tf.layers.separable_conv2d(x, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn3')
+
+            if is_test:
+                input_x = input_x[:,0:510,0:510,:]
+            else:
+                input_x = input_x[:,0:254,0:254,:]
+
+            x = tf.concat([x, input_x], axis=3, name='concat9')
+            x = tf.layers.conv2d(x, 3, 3, 1, padding='same', name='conv10')
+            # x = tf.layers.conv2d(x, 3, 3, 1, padding='same', activation=tf.nn.elu, name='conv11')
+            x = tf.pad(x, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT', name='padding')
+
+            if not is_test:
+                x = tf.clip_by_value(x, -1., 1., name='result')
+        return x
+
+    def build_graph_with_losses(
+            self, batch_data,
+            batch_mask,
+            noise1 = None,
+            noise2 = None,
+            config=None,
+            training=True,
+            summary=False,
+            reuse=False
+    ):
+
+        # generate mask
+        batch_mask_input = tf.cast(batch_mask, tf.float32)
+
+        # image gray
+        batch_pos = batch_data / 127.5 - 1.
+        batch_incomplete = batch_pos * (1. - batch_mask)
+
+        # noise
+        noise3 = noise1[:,:128,:128,:]
+        noise4 = noise2[:,:128,128:,:]
+
+        x2 = self.build_inpaint_net(
+            batch_incomplete, batch_mask_input,
+            noise1=noise1, noise2 = noise2, noise3=noise3, noise4 = noise4,
+            is_test=False, config=config, reuse=reuse, training=training
+        )
+
+        losses = {}
+        ##########################X2 loss##################################
+        # apply batch_mask and complete image
+        batch_complete = x2 * batch_mask + batch_incomplete * (tf.subtract(1.0, batch_mask))
+
+        # l1 loss
+        x2_l1_loss = tf.reduce_mean(tf.abs(batch_pos - x2))
+
+        # l1 loss L per-pixel loss
+        lperpixel_m_loss = tf.reduce_mean(tf.multiply(tf.abs(x2 - batch_pos), batch_mask))
+        # lperpixel_unm_loss = tf.reduce_mean(tf.multiply(tf.abs(x2 - batch_pos), tf.abs(tf.subtract(1., batch_mask))))
+
+        # TV_noise loss
+
+        # tvstage1 = tvnoise(x2, noise1, 4, 2)
+        # tvstage2 = tvnoise(tvstage1, noise2, None, 2)
+
+        # tv_complete = tvstage2 * batch_mask[:,4:,4:,:] + batch_incomplete[:, 4:, 4:, :] * (tf.subtract(1.0, batch_mask[:, 4:, 4:, :]))
+
+        # noise1_1 = 1.0 - noise1
+        # x1_1 = x2[:, :-2, :-1, :]
+        # noisex1_1 = noise1[:, :-2, :-1, :]
+        # x1_1 = x1_1 * noisex1_1
+        #
+        # x1_2 = x2[:, 2:, 1:, :]
+        # noisex2_2 = noise1_1[:, 2:, 1:, :]
+        # x1_2 = x1_2 * noisex2_2
+        #
+        # tv_complete = (x1_1 + x1_2) * batch_mask[:, 2:, 1:, :] + batch_incomplete[:, 2:, 1:, :] * (tf.subtract(1.0, batch_mask[:, 2:, 1:, :]))
+
+        # tv_loss = tf.reduce_mean(tf.abs(tv_complete - batch_pos[:, 4:, 4:, :]))
+        tv_loss_col = tf.reduce_mean(tf.abs(batch_complete[:, :, 1:, :] - batch_pos[:, :, :-1, :]))
+        tv_loss_row = tf.reduce_mean(tf.abs(batch_complete[:, 1:, :, :] - batch_pos[:, :-1, :, :]))
+        tv_loss = tv_loss_col + tv_loss_row
+
+        losses['l1_loss'] = config.X2_L1_LOSS_ALPHA * x2_l1_loss  # + config.X1_L1_LOSS_ALPHA * x1_l1_loss
+        losses['lperpixel_loss'] = config.L_MASK_PER_PIXEL_ALPHA * lperpixel_m_loss
+                                   # + config.L_UNMASK_PER_PIXEL_ALPHA * lperpixel_unm_loss
+        losses['tv_loss'] = config.TV_LOSS_ALPHA * tv_loss
+
+        ##########################vgg loss##################################
+
+        gt_feat, x2_feat = extractor_feature_vgg16_net(
+            config, batch_pos, x2
+        )
+        prc_loss, style_loss = prc_and_style_loss(x2_feat, gt_feat)
+
+        losses['prc_loss'] = config.PRC_LOSS_ALPHA * prc_loss
+        losses['style_loss'] = config.STYLE_LOSS_ALPHA * style_loss
+
+        if summary:
+            scalar_summary('losses/l1_loss', losses['l1_loss'])
+            scalar_summary('losses/lperpixel_loss', losses['lperpixel_loss'])
+            scalar_summary('losses/prc_loss', losses['prc_loss'])
+            scalar_summary('losses/style_loss', losses['style_loss'])
+            scalar_summary('losses/tv_loss', losses['tv_loss'])
+
+        if summary:
+            viz_img = [batch_pos, batch_incomplete, batch_complete, x2]
+            images_summary(tf.concat(viz_img, axis=2), 'raw_incomplete_predicted_complete', config.VIZ_MAX_OUT)
+
+        ##########################X2 Dnet####################################
+        # init GAN loss
+        gan = InpaintGAN_net()
+        g_pos, g_neg = gan.build_SN_G_discriminator(
+            realG= batch_pos, fakeG=x2, name='Dnet_G'
+        )
+
+        # localD and globalD loss
+        g_loss_global, d_loss_global = gan.Hinge_loss(g_pos, g_neg, name='Dnet_G/global_gan')
+        losses['d_loss_G'] = config.D_LOSS_G_ALPHA * d_loss_global
+        losses['g_loss_G'] = config.G_LOSS_G_ALPHA * g_loss_global
+
+        if summary:
+            scalar_summary('convergence/d_loss_G', losses['d_loss_G'])
+            scalar_summary('losses/g_loss_G', losses['g_loss_G'])
+
+        pair_l_pos, pair_l_neg = gan.build_SN_pair_RL_discriminator(
+            realL=batch_pos * batch_mask,
+            realG=batch_pos,
+            fakeG=batch_complete,
+            name='Dnet_RL'
+        )
+
+        # pair RL loss
+        g_loss_pair_l, d_loss_pair_l = gan.Hinge_loss(pair_l_pos, pair_l_neg, name='Dnet_RL/pair_RL_gan')
+        losses['d_loss_pair_l'] = config.D_LOSS_PAIR_ALPHA * d_loss_pair_l
+        losses['g_loss_pair_l'] = config.G_LOSS_PAIR_ALPHA * g_loss_pair_l
+
+        if summary:
+            scalar_summary('convergence/d_loss_pair_l', losses['d_loss_pair_l'])
+            scalar_summary('losses/g_loss_pair_l', losses['g_loss_pair_l'])
+
+        # total loss for gener net
+        losses['g_loss'] = \
+            losses['l1_loss'] + \
+            losses['prc_loss'] + \
+            losses['style_loss'] + \
+            losses['g_loss_G'] + \
+            losses['lperpixel_loss'] + \
+            losses['g_loss_pair_l'] + \
+            losses['tv_loss']
+
+
+        if summary:
+            scalar_summary('losses/netG_total_loss', losses['g_loss'])
+
+        g_vars = tf.get_collection(
+            tf.GraphKeys.TRAINABLE_VARIABLES, 'inpaint_net')
+
+        Dnet_G_vars = tf.get_collection(
+            tf.GraphKeys.TRAINABLE_VARIABLES, 'Dnet_G')
+
+        Dnet_RL_vars = tf.get_collection(
+            tf.GraphKeys.TRAINABLE_VARIABLES, 'Dnet_RL')
+
+        return g_vars, Dnet_G_vars, Dnet_RL_vars, losses
+
+    def build_inpaint_resize_net(self, x, mask=None,
+                          noise1 = None, noise2 = None, noise3 = None, noise4 = None,
+                          is_test=False, config=None, reuse=False,
+                          training=True, padding='SAME', name='inpaint_net'):
+        """Inpaint network.
+
+        Args:
+            x: incomplete image, [-1, 1]
+            batch_mask: batch_mask region {0, 1}
+        Returns:
+            [-1, 1] as predicted image
+        """
+        if is_test:
+            # B,G,R,ones_x,mask, noise = tf.split(x, num_or_size_splits=6,axis=3)
+            # input_x = tf.concat([B,G,R,ones_x,mask], axis=3)
+            input_x = x
+        else:
+            # ones_x = tf.ones_like(x)[:, :, :, 0:1]
+            x = tf.concat([x, mask], axis=3)
+            input_x = x
+        # two stage network
+        cnum = 32
+        offset = 0
+        with tf.variable_scope(name, reuse=tf.AUTO_REUSE), \
+                arg_scope([gen_deconv, gen_gate_conv],
+                          training=training):
+            #stage 1
+            # encode
+            # 128
+            x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+            x_1 = tf.layers.batch_normalization(x_1)
+            # 64
+            x_2 = gen_gate_conv(x_1, 2*cnum, 5, 2, name='conv2')
+            x_2 = tf.layers.batch_normalization(x_2)
+            # 32
+            x_3 = gen_gate_conv(x_2, 4*cnum, 3, 2, name='conv3')
+            x_3 = tf.layers.batch_normalization(x_3)
+            # 16
+            x_4 = gen_gate_conv(x_3, 4*cnum, 3, 2, name='conv4')
+            x_4 = tf.layers.batch_normalization(x_4)
+
+            # 32
+            x = gen_resize(x_4, 4*cnum, name='upsample1')
+            # x = gen_normal_deconv(x_4, 4*cnum, name='upsample1')
+            x = tf.layers.conv2d(x, 2*cnum, 3, 1, activation=tf.nn.leaky_relu, padding='same',name='up_conv1')
+            x = tf.layers.batch_normalization(x)
+
+            # 64
+            x = gen_resize(x, 2*cnum, name='upsample2')
+            # x = gen_normal_deconv(x, 2*cnum, name='upsample2')
+
+            if offset:
+                x = self.offset(x, 2*cnum, 1, 0, 0, 1,name='offset1')
+                x = tf.pad(x, paddings=[[0, 0], [1, 0], [0, 1], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, 2*cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn1')
+
+            x = tf.concat([x, x_2], axis=3, name='concat8')
+            x = gen_gate_conv(x, 2*cnum, 3, 1, padding='same', name='conv7')
+            x = tf.layers.batch_normalization(x)
+
+            # 128
+            x = gen_resize(x, cnum, name='upsample3')
+            # x = gen_normal_deconv(x, cnum, name='upsample3')
+            # x = self.random_offset(x, cnum, noise3, noise4, name='offset1')
+
+            if offset:
+                x = self.offset(x, cnum, 0, 1, 1, 0, name='offset2')
+                x = tf.pad(x, paddings=[[0, 0], [0, 1], [1, 0], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn2')
+
+            x = tf.concat([x, x_1], axis=3, name='concat8')
+            x = gen_gate_conv(x, cnum, 3, 1, padding='same', name='conv8')
+            x = tf.layers.batch_normalization(x)
+
+            # 256
+            x = gen_resize(x, cnum, name='upsample4')
+            # x = gen_normal_deconv(x, cnum, name='upsample4')
+            # x = self.random_offset(x, cnum,  noise1, noise2, name='offset2')
+            if offset:
+                x = self.offset(x, cnum, 1, 0, 0, 1, name='offset3')
+                x = tf.pad(x, paddings=[[0, 0], [1, 0], [0, 1], [0, 0]], mode='REFLECT', name='padding')
+                x = tf.layers.separable_conv2d(x, cnum, 3, 1, padding='same', activation=tf.nn.leaky_relu, name='fcn3')
+
+            x = tf.concat([x, input_x], axis=3, name='concat9')
+            x = tf.layers.conv2d(x, 3, 3, 1, padding='same', name='conv10')
+            if not is_test:
+                x = tf.clip_by_value(x, -1., 1., name='result')
+        return x
Index: inpaint_train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_train.py	(date 1578357929000)
+++ inpaint_train.py	(date 1578357929000)
@@ -0,0 +1,149 @@
+import logging
+import tensorflow as tf
+import neuralgym as ng
+
+from inpaint_model import InpaintCAModel
+logger = logging.getLogger()
+
+def multigpu_graph_def(model, config, gpu_id=0, loss_type='g_loss'):
+    with tf.device('/cpu:%d' % gpu_id):
+        dataImg = data.data_pipeline(config.BATCH_SIZE)
+        images, mask, noise1, noise2 = split_channel_input_data(dataImg, 6)
+
+    # summary the GPU:0 loss
+    if gpu_id == 0 and loss_type == 'g_loss':
+        _, _, _, losses = model.build_graph_with_losses(
+            images, mask, noise1=noise1, noise2=noise2, config=config, summary=True, reuse=True)
+    else:
+        _, _, _, losses = model.build_graph_with_losses(
+            images, mask, noise1=noise1, noise2=noise2, config=config, reuse=True)
+
+    if loss_type == 'g_loss':
+        return losses
+    elif loss_type == 'Dnet_G':
+        return losses['d_loss_G']
+    elif loss_type == 'Dnet_RL':
+        return losses['d_loss_pair_l']
+    else:
+        raise ValueError('loss type is not supported.')
+
+def split_channel_input_data(imagesAndguide, channel):
+    if channel == 4:
+        b,g,r, mask = tf.split(imagesAndguide, axis=3, num_or_size_splits=4)
+        image = tf.concat([b,g,r], axis=3)
+        return image, mask
+    if channel == 6:
+        b, g, r, mask, noise1, noise2 = tf.split(imagesAndguide, axis=3, num_or_size_splits=6)
+        image = tf.concat([b, g, r], axis=3)
+        return image, mask, noise1, noise2
+
+
+if __name__ == "__main__":
+    config = ng.Config('inpaint.yml')
+    if config.GPU_ID != -1:
+        ng.set_gpus(config.GPU_ID)
+    else:
+        ng.get_gpus(config.NUM_GPUS)
+
+    # training data
+    with tf.device('/cpu:0'):
+        with open(config.DATA_FLIST[config.DATASET][0]) as f1:
+                    f1names = f1.read().splitlines()
+                    fnames = f1names
+                    data = ng.data.DataFromFNames(
+                        fnames,
+                        shapes=[256, 256, 6],
+                        resize_img=[256, 256],
+                        crop_shape=[512, 512],
+                        is_resize = True
+                    )
+                    dataImg = data.data_pipeline(config.BATCH_SIZE)
+                    images, mask, noise1, noise2 = split_channel_input_data(dataImg, 6)
+
+    # main model
+    model = InpaintCAModel()
+
+    # build model
+    g_vars, Dnet_G_vars, Dnet_L_vars, losses = \
+        model.build_graph_with_losses(images, mask, noise1=noise1, noise2=noise2, config=config)
+    g_list = tf.global_variables()
+    bn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]
+    bn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]
+    # g_vars1 = [g for g in g_list if 'inpaint_net' in g.name]
+
+    # training settings
+    d_lr = tf.get_variable('d_lr', shape=[], trainable=False, initializer=tf.constant_initializer(1e-5))
+    d_optimizer = tf.train.AdamOptimizer(d_lr, beta1=0.5, beta2=0.9)
+	
+    d_l_lr = tf.get_variable('d_lr_rl', shape=[], trainable=False, initializer=tf.constant_initializer(1e-5))
+    d_l_optimizer = tf.train.AdamOptimizer(d_l_lr, beta1=0.5, beta2=0.9)
+	
+    g_lr = tf.get_variable('g_lr', shape=[], trainable=False, initializer=tf.constant_initializer(1e-5))
+    g_optimizer = tf.train.AdamOptimizer(g_lr, beta1=0.5, beta2=0.9)
+
+    # gradient processor
+    if config.GRADIENT_CLIP:
+        gradient_processor = lambda grad_var: \
+            (tf.clip_by_average_norm(grad_var[0], config.GRADIENT_CLIP_VALUE), grad_var[1])
+    else:
+        gradient_processor = None
+
+    # log dir
+    log_prefix = config.MODEL_SAVEDIR
+
+    # train discriminator with secondary trainer, should initialize before primary trainer.
+    discriminator0_training_callback = ng.callbacks.SecondaryTrainer(
+        pstep=2,
+        align=False,
+        optimizer=d_optimizer,
+        var_list=Dnet_G_vars,
+        max_iters=1,
+        graph_def=multigpu_graph_def,
+        graph_def_kwargs={
+            'model': model, 'config': config, 'loss_type': 'Dnet_G'},
+    )
+
+    # train discriminator with secondary trainer, should initialize before primary trainer.
+    discriminator1_training_callback = ng.callbacks.SecondaryTrainer(
+        pstep=2,
+        align=False,
+        optimizer=d_l_optimizer,
+        var_list=Dnet_L_vars,
+        max_iters=1,
+        graph_def=multigpu_graph_def,
+        graph_def_kwargs={
+            'model': model, 'config': config, 'loss_type': 'Dnet_RL'}
+    )
+
+    # train generator with primary trainer
+    trainer = ng.train.MultiGPUTrainer(
+        optimizer=g_optimizer,
+        var_list=g_vars,
+        align = False,
+        max_iters=config.MAX_ITERS,
+        graph_def=multigpu_graph_def,
+        grads_summary=config.GRADS_SUMMARY,
+        gradient_processor=gradient_processor,
+        graph_def_kwargs={
+            'model': model, 'config': config, 'loss_type': 'g_loss'},
+        spe=config.TRAIN_SPE,
+        async_train=False,
+        num_gpus=config.NUM_GPUS,
+        log_dir=log_prefix,
+    )
+
+    # add all callbacks
+    trainer.add_callbacks(discriminator0_training_callback)
+    trainer.add_callbacks(discriminator1_training_callback)
+
+    # save vars
+    vars = g_vars+ bn_moving_vars+ Dnet_G_vars  + Dnet_L_vars
+    trainer.add_callbacks([
+        ng.callbacks.WeightsViewer(),
+        ng.callbacks.ModelRestorer(trainer.context['saver'], dump_prefix=config.MODEL_RESTORE+'/snap', optimistic=True),
+        ng.callbacks.ModelSaver(config.TRAIN_SPE, tf.train.Saver(vars, max_to_keep=config.MAX_TO_KEEP_MODEL),
+                                log_prefix, config.MODEL_RESTORE),
+        # ng.callbacks.SummaryWriter((config.VAL_PSTEPS//1), trainer.context['summary_writer'], tf.summary.merge_all()),
+    ])
+    # launch training
+    trainer.train()
Index: inpaint_trainset_filelist_gen.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_trainset_filelist_gen.py	(date 1572395093000)
+++ inpaint_trainset_filelist_gen.py	(date 1572395093000)
@@ -0,0 +1,136 @@
+import argparse
+import os
+from os import walk
+from random import shuffle
+
+parser = argparse.ArgumentParser()
+
+#parser.add_argument('--folder_path', default='./training_data', type=str,
+
+parser.add_argument('--folder_path', default='/data/jinbin.lin/data/new_places2/data_large/', type=str,
+                    help='The folder path')
+# /data/li.zuo/data/new_place2/places2-edge-results/hed_pretrained_bsds_multiscale/ #user guide line data
+parser.add_argument('--file_tpye', default='jpg', type=str,
+                    help='image file type')
+parser.add_argument('--train_filename', default='/data/zhiweige/tf_places2_filelist/train_shuffled.flist', type=str,
+                    help='The train filename.')
+parser.add_argument('--train_file_guide', default='/data/zhiweige/tf_places2_filelist/train_shuffled_guideline.flist', type=str,
+                    help='The validation filename.')
+'''
+parser.add_argument('--folder_path', default='/data/data_for_training/Imagenet', type=str,
+                    help='The folder path')
+parser.add_argument('--train_filename', default='/data/data_for_training/Imagenet/train_shuffled.flist', type=str,
+                    help='The train filename.')
+parser.add_argument('--validation_filename', default='/data/data_for_training/Imagenet/validation_shuffled.flist', type=str,
+                    help='The validation filename.')
+'''
+parser.add_argument('--is_shuffled', default='0', type=int,
+                    help='Needed to be shuffled')
+
+
+if __name__ == "__main__":
+
+    args = parser.parse_args()
+    str_guide = '/data/li.zuo/data/new_place2/places2-edge-results/hed_pretrained_bsds_multiscale'
+    # get the list of directories and separate them into 2 types: training and validation
+    #training_dirs = os.listdir(args.folder_path + "/train")
+    #validation_dirs = os.listdir(args.folder_path + "/val")
+    # training_dirs = args.folder_path + "/data_large"
+    # validation_dirs = args.folder_path + "/val_large"
+#    training_dirs = args.folder_path + "/train"
+#    validation_dirs = args.folder_path + "/val"
+
+    training_dirs = args.folder_path
+    # make 2 lists to save file paths
+    #training_file_names = []
+    #validation_file_names = []
+    training_file_names = []
+    img_w = os.walk(training_dirs)
+    for path, d, filelist in img_w:
+        for filename in filelist:
+            if filename.endswith(args.file_tpye):
+                # if os.path.join(path, filename) == '/data/li.zuo/data/new_place2/places2-edge-results/hed_pretrained_bsds_multiscale/00010699.jpg' or \
+                #     os.path.join(path,filename) == '/data/li.zuo/data/new_place2/places2-edge-results/hed_pretrained_bsds_multiscale/00010698.jpg' or \
+                #         os.path.join(path,
+                #                      filename) == '/data/li.zuo/data/new_place2/places2-edge-results/hed_pretrained_bsds_multiscale/00010697.jpg' :
+                #     continue
+                training_file_names.append(os.path.join(path, filename))
+                # print(training_file_names[-1])
+                # line_guide = str_guide + training_file_names[-1][44:]
+                # training_file_names.append(line_guide)
+                # print(training_file_names[-1])
+    # validation_file_names = []
+    # img_w = os.walk(validation_dirs)
+    # for path, d, filelist in img_w:
+    #     for filename in filelist:
+    #         if filename.endswith(args.file_tpye):
+    #             validation_file_names.append(os.path.join(path, filename))
+
+    '''
+    for training_dir in training_dirs:
+        training_folders = os.listdir(args.folder_path + "/data_large" + training_dir)
+        for training_folder in training_folders:
+            for dirpath, dirnames, filenames in os.walk(training_folder):
+                training_file_names.append(os.path.join(dirpath, filenames))
+    
+    # append all files into 2 lists
+    for training_dir in training_dirs:
+        # append each file into the list file names
+        training_folder = os.listdir(args.folder_path + "/train" + "/" + training_dir)
+        #for training_item in training_folder:
+#        training_files = os.listdir(args.folder_path + "/train" + "/" + training_dir + "/" + training_folder)
+        # modify to full path -> directory
+        for training_file in training_folder:
+            training_file_full = args.folder_path + "/train" + "/" + training_file #training_dir + "/" +  training_item + "/"
+            training_file_names.append(training_file_full)
+    
+    # append all files into 2 lists
+    for validation_folder in validation_dirs:
+        # append each file into the list file names
+        validation_file_names.append(args.folder_path + "/val" + "/" + validation_folder)
+        
+        validation_file_names.append(validation_item)
+        for validation_item in validation_folder:
+            # modify to full path -> directory
+            validation_item = args.folder_path + "/val_large" + "/" + validation_dir + "/" + validation_item
+            validation_file_names.append(validation_item)
+        
+    '''
+    # print all file paths
+    #for i in training_file_names:
+    #    print(i)
+    #for i in validation_file_names:
+    #    print(i)
+
+    # shuffle file names if set
+    if args.is_shuffled == 1:
+        shuffle(training_file_names)
+        # shuffle(validation_file_names)
+
+    # make output file if not existed
+    if not os.path.exists(args.train_filename):
+        os.mknod(args.train_filename)
+    #
+    if not os.path.exists(args.train_file_guide):
+        os.mknod(args.train_file_guide)
+
+    # if not os.path.exists(args.validation_filename):
+    #     os.mknod(args.validation_filename)
+
+    file_guide = []
+
+    for line in training_file_names:
+        line_guide = str_guide + line[44:]
+        file_guide.append(line_guide)
+
+    # write to file
+    fo = open(args.train_filename, "w")
+    fo.write("\n".join(training_file_names))
+    fo.close()
+
+    fo = open(args.train_file_guide, "w")
+    fo.write("\n".join(file_guide))
+    fo.close()
+
+    # print process
+    print("Written file is: ", args.train_filename, ", is_shuffle: ", args.is_shuffled)
\ No newline at end of file
Index: inpaint_test_demo_model_slic.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_test_demo_model_slic.py	(date 1572395093000)
+++ inpaint_test_demo_model_slic.py	(date 1572395093000)
@@ -0,0 +1,294 @@
+import argparse
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = '0'
+import random
+import cv2
+import numpy as np
+import tensorflow as tf
+from inpaint_model import InpaintCAModel, logging
+logger = logging.getLogger()
+from tensorflow.python.framework import graph_util
+
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--image', default='image/image.jpg', type=str,
+                    help='The filename of image to be completed.')
+parser.add_argument('--mask', default='image/mask.jpg', type=str,
+                    help='The filename of mask, value 255 indicates mask.')
+parser.add_argument('--guide', default='image/guide.jpg', type=str,
+                    help='The filename of guidelines, value 0 indicates guide line.')
+parser.add_argument('--output', default='image/output_test.jpg', type=str,
+                    help='Where to write output.')
+parser.add_argument('--checkpoint_dir', default=
+                    'model/model_18_deletNoise_layers',
+                    type=str,
+                    help='The directory of tensorflow checkpoint.')
+
+def gaussian_blur(image, kernel, kernel_size, cdim=3):
+    # kernel as placeholder variable, so it can change
+    # outputs = []
+    # pad_w = (kernel_size - 1) // 2
+    # pad_w = 0
+    # padded = tf.pad(image, [[0, 0], [pad_w, pad_w], [pad_w, pad_w], [0, 0]], mode='CONSTANT')
+    # for channel_idx in range(cdim):
+    #     data_c = padded[:, :, :, channel_idx:(channel_idx + 1)]
+    #     g = kernel.reshape((kernel_size, kernel_size, 1, 1))
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     data_c = tf.nn.conv2d(data_c, g, [1, 1, 1, 1], 'SAME')
+    #     outputs.append(data_c)
+    # return tf.concat(outputs,axis=3)
+
+    # channel B
+    with tf.name_scope(name='b'):
+        data_b = image[:, :, :, 0:1]
+        g_b = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_b = tf.constant(value=g_b, shape=[kernel_size,kernel_size, 1, 1], dtype=tf.float32,
+                          name='const_b1')
+        data_b = tf.nn.conv2d(data_b, g_b, [1, 1, 1, 1], 'SAME', name='convB_1')
+
+        # g_b_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_b1/bias')
+        # data_b = tf.nn.bias_add(data_b, g_b_bias)
+
+        g_b_1 = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_b_1 = tf.constant(value=g_b_1, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                            name='const_b2')
+        data_b = tf.nn.conv2d(data_b, g_b_1, [1, 1, 1, 1], 'SAME', name='convB_2')
+
+        # g_b_1_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_b2/bias')
+        # data_b = tf.nn.bias_add(data_b, g_b_1_bias)
+
+        data_b = tf.pad(data_b, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
+        k1 = np.array([1.0]).astype(dtype=np.float32)
+        k1 = k1.reshape([1, 1, 1, 1])
+        k1 = tf.constant(value=k1, shape=[1,1,1,1], dtype=tf.float32,
+                         name='fcn_b')
+        data_b = tf.nn.conv2d(data_b, k1, [1,1,1,1], 'VALID', name='fcn_pad_b')
+
+        # data_b_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='fcn_b/bias')
+        # data_b = tf.nn.bias_add(data_b, data_b_bias)
+
+    # channel G
+    with tf.name_scope(name='g'):
+        data_g = image[:, :, :, 1:2]
+        g_g = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_g = tf.constant(value=g_g, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                          name='const_g1')
+        data_g = tf.nn.conv2d(data_g, g_g, [1, 1, 1, 1], 'SAME', name='convG_1')
+
+        # g_g_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_g1/bias')
+        # data_g = tf.nn.bias_add(data_g, g_g_bias)
+
+        g_g_1 = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_g_1 = tf.constant(value=g_g_1, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                            name='const_g2')
+        data_g = tf.nn.conv2d(data_g, g_g_1, [1, 1, 1, 1], 'SAME', name='convG_2')
+
+        # g_g_1_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_g2/bias')
+        # data_g = tf.nn.bias_add(data_g, g_g_1_bias)
+
+        data_g = tf.pad(data_g, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
+        k1 = np.array([1.0]).astype(dtype=np.float32)
+        k1 = k1.reshape([1, 1, 1, 1])
+        k1 = tf.constant(value=k1, shape=[1, 1, 1, 1], dtype=tf.float32,
+                         name='fcn_g')
+        data_g = tf.nn.conv2d(data_g, k1, [1, 1, 1, 1], 'VALID', name='fcn_pad_g')
+
+        # data_g_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='fcn_g/bias')
+        # data_g = tf.nn.bias_add(data_g, data_g_bias)
+
+    # channel R
+    with tf.name_scope(name='r'):
+        data_r = image[:, :, :, 2:3]
+        g_r = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_r = tf.constant(value=g_r, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                          name='const_r1')
+        data_r = tf.nn.conv2d(data_r, g_r, [1, 1, 1, 1], 'SAME', name='convR_1')
+
+        # g_r_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_r1/bias')
+        # data_r = tf.nn.bias_add(data_r, g_r_bias)
+
+        g_r_1 = kernel.reshape([kernel_size, kernel_size, 1, 1])
+        g_r_1 = tf.constant(value=g_r_1, shape=[kernel_size, kernel_size, 1, 1], dtype=tf.float32,
+                            name='const_r2')
+        data_r = tf.nn.conv2d(data_r, g_r_1, [1, 1, 1, 1], 'SAME', name='convR_2')
+
+        # g_r_1_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='const_r2/bias')
+        # data_r = tf.nn.bias_add(data_r, g_r_1_bias)
+
+        data_r = tf.pad(data_r, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
+        k1 = np.array([1.0]).astype(dtype=np.float32)
+        k1 = k1.reshape([1, 1, 1, 1])
+        k1 = tf.constant(value=k1, shape=[1, 1, 1, 1], dtype=tf.float32,
+                         name='fcn_r')
+        data_r = tf.nn.conv2d(data_r, k1, [1, 1, 1, 1], 'VALID', name='fcn_pad_r')
+
+        # gdata_r_bias = tf.constant(value=0, shape=[1], dtype=tf.float32, name='fcn_r/bias')
+        # data_r = tf.nn.bias_add(data_r, gdata_r_bias)
+
+    output = tf.concat([data_b, data_g, data_r], axis=3)
+    return output
+
+if __name__ == "__main__":
+    args = parser.parse_args()
+
+    height = 512
+    #
+    width = height
+    # channel = 3
+    #
+    model = InpaintCAModel()
+    # image = cv2.imread(args.image)
+    # mask = cv2.imread(args.mask)
+    # guide = cv2.imread(args.guide)
+    # # imageGray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
+    # # guide = canny(imageGray, sigma=random.randint(1, 3)).astype(np.float32)
+    #
+    #
+    # image = cv2.resize(image, (height,width))
+    # # imageGray = cv2.resize(imageGray, (height, width))
+    # # imageGray = np.expand_dims(imageGray, 2)
+    # mask = cv2.resize(mask, (height, width))
+    # guide = cv2.resize(guide, (height, width))
+    # # guide = np.expand_dims(guide, 2)
+    #
+    # h, w, _ = image.shape
+    # grid = 8
+    # image = image[:h // grid * grid, :w // grid * grid, :]
+    # # imageGray = imageGray[:h // grid * grid, :w // grid * grid, :]
+    # mask = mask[:h // grid * grid, :w // grid * grid, :]
+    # guide = guide[:h // grid * grid, :w // grid * grid, :]
+    # print('Shape of image: {}'.format(image.shape))
+    #
+    # image = (image / 127.5 - 1.).astype(np.float32)
+    # # imageGray = (imageGray / 127.5 - 1.).astype(np.float32)
+    # mask = (mask > 127.5).astype(np.float32)
+    # guide = (guide > 0).astype(np.float32)
+    #
+    # image = np.expand_dims(image, 0)
+    # # imageGray = np.expand_dims(imageGray, 0)
+    # mask = np.expand_dims(mask, 0)
+    # guide = np.expand_dims(guide, 0)
+    #
+    # mask = mask[:, :, :, 0:1]
+    # guide = guide[:, :, :, 0:1]
+    # one_s = np.ones_like(mask)
+    #
+    # image = image * (1. - mask)
+    # # imageGray = imageGray * (1. - mask)
+    # guide = guide * (1. - mask)
+    #
+    #
+    # # input_data = np.concatenate([imageGray, mask, guide], axis=3)
+    # input_data = np.concatenate([image, one_s, mask, guide], axis=3)
+    noise = np.random.rand(height, width)
+    noise = np.expand_dims(noise, axis=2)
+    noise = np.expand_dims(noise, axis=0)
+
+    def getNoise(offset_h, offset_w):
+        noiseMask = np.random.rand(height, width)
+        if offset_h and offset_w:
+            noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+        elif offset_h:
+            noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+        elif offset_w:
+            noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+        # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+        noiseMask2 = 1 - noiseMask1
+        noiseMask1 = np.expand_dims(noiseMask1, axis=2)
+        noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=2)
+        noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+        return noiseMask1, noiseMask2
+
+    noiseMask1, noiseMask2 = getNoise(2,1)
+    noiseMask1_1, noiseMask2_1 = getNoise(2, 2)
+
+    # 5*5 gauss filter
+    gauss_filter_5 = np.array(
+        # [1, 4, 7, 4, 1, 4, 16, 26, 16, 4, 7, 26, 41, 26, 7, 4, 16, 26, 16, 4, 1, 4, 7, 4, 1]) / 273.0
+        # [1, 2, 3, 2, 1, 2, 4, 5, 4, 2, 3, 5, 160, 5, 3, 2, 4, 5, 4, 2, 1, 2, 3, 2, 1]) / 228.0
+        [1, 2, 3, 2, 1, 2, 3, 4, 3, 2, 3, 4, 150, 4, 3, 2, 3, 4, 3, 2, 1, 2, 3, 2, 1]) / 210.0
+    gauss_filter = gauss_filter_5.astype(dtype=np.float32)
+
+    # 3*3 gauss filter
+    gauss_filter_3 = np.array(
+        [1, 1, 1, 1, 60, 1, 1, 1, 1]) / 68.0
+    gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+    # gauss_filter.reshape((5, 5, 1, 1))
+
+    sess_config = tf.ConfigProto()
+    pb_file_path = '/mnt/disk/mm.bai/inpaiting/DLC_model_converter/pbmodelzoo/test-eden.pb'
+
+    with tf.Session(config=sess_config) as sess:
+        input = tf.placeholder(tf.float32, shape=[1, height, width, 4], name='input')
+        # B,G,R, imageGray, one_s, mask = tf.split(input, num_or_size_splits=5, axis=3)
+        # B, G, R, imageGray, one_s, mask, edge = tf.split(input, num_or_size_splits=7, axis=3)
+        # image = tf.concat([B,G,R], axis=3)
+        # x1_input = tf.concat([imageGray, mask, edge], axis=3)
+        # output = model.build_gate_res_net(input, is_test=True)
+        # x1 = tf.cast(x1[:, :, :, :] > 0, tf.float32)
+        # x2_input = tf.concat([image, one_s, mask, x1], axis=3)
+        output = model.build_inpaint_net(input, noise=noise, is_test=True)
+
+        # output = tf.multiply(output, 1.0, name='output')
+        with tf.name_scope(name='crop1'):
+            x1 = output[:, :-2, :-1, :]
+        # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+        with tf.name_scope(name='crop2'):
+            x2 = output[:, 2:, 1:, :]
+        # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+        with tf.name_scope(name='x1_Mul_Noise1'):
+            x1 = x1 * noiseMask1
+        with tf.name_scope(name='x2_Mul_Noise2'):
+            x2 = x2 * noiseMask2
+        # with tf.variable_scope(name_or_scope='x1Multix2'):
+
+        output = tf.add(x1, x2, name='x1_add_x2')
+        # output = tf.pad(output, paddings=[[0, 0], [1, 1], [1, 0], [0, 0]], mode="CONSTANT", name='output_1')
+
+        with tf.name_scope(name='crop3'):
+            x3 = output[:, :, :-1, :]
+        # x1 = tf.pad(output, paddings=[[0, 0], [2, 0], [2, 0], [0, 0]], mode="CONSTANT", name='x1pad')
+        with tf.name_scope(name='crop4'):
+            x4 = output[:, :, 1:, :]
+        # x2 = tf.pad(output, paddings=[[0, 0], [0, 2], [0, 2], [0, 0]], mode="CONSTANT", name='x2pad')
+        with tf.name_scope(name='x3_Mul_noise3'):
+            x3 = x3 * noiseMask1_1
+        with tf.name_scope(name='x4_Mul_noise4'):
+            x4 = x4 * noiseMask2_1
+        # with tf.variable_scope(name_or_scope='x1Multix2'):
+        output = tf.add(x3, x4, name='x3_add_x4')
+        # output = tf.pad(output, paddings=[[0, 0], [0, 0], [0, 2], [0, 0]], mode="CONSTANT", name='output_2')
+
+        # Gauss blur
+        with tf.variable_scope(name_or_scope='gauss_blur'):
+            output = gaussian_blur(output, gauss_filter_3, 3)
+        output = tf.add(output, 0., name='output')
+
+        # B,G,R,_one,_mask,_guideline = tf.split(input, num_or_size_splits=7, axis=3)
+        # _imageGray, _mask, _guideline = tf.split(input, num_or_size_splits=3, axis=3)
+        # img = tf.concat([B,G,R], axis=3)
+        # output = image*(1.-mask) + output*mask
+        # output = output*_mask + _guideline*(1. - _mask)
+        # output = (output + 1.) * 127.5
+        # output = tf.reverse(output, [-1])
+        # output = tf.saturate_cast(output, tf.uint8)
+        # load pretrained model
+        vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+        assign_ops = []
+        for var in vars_list:
+            vname = var.name
+            logger.info(vname)
+            from_name = vname
+            var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)
+            assign_ops.append(tf.assign(var, var_value))
+
+        sess.run(assign_ops)
+        logger.info('Model loaded.')
+        # result = sess.run(output, feed_dict={input: input_data})
+
+        constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['output'])
+        # constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['inpaint_net/post/conv9-2/BiasAdd'])
+        with tf.gfile.FastGFile(pb_file_path, mode='wb') as f:
+            f.write(constant_graph.SerializeToString())
+        # cv2.imwrite(args.output, result[0][:, :, ::-1])
\ No newline at end of file
Index: inpaint_mask.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpaint_mask.py	(date 1578357929000)
+++ inpaint_mask.py	(date 1578357929000)
@@ -0,0 +1,483 @@
+from queue import Queue
+from neuralgym.ops.layers import *
+from random import randint
+
+import logging
+import math
+import random
+import tensorflow as tf
+import cv2
+
+logger = logging.getLogger()
+
+def random_bbox(config):
+    """Generate a random tlhw with configuration.
+
+
+    Args:
+        config: Config should have configuration including IMG_SHAPES,
+            VERTICAL_MARGIN, HEIGHT, HORIZONTAL_MARGIN, WIDTH.
+
+    Returns:
+        tuple: (top, left, height, width)
+
+    """
+    img_shape = config.MASK_SHAPES
+    img_height = img_shape[0]
+    img_width = img_shape[1]
+    maxt = img_height - config.VERTICAL_MARGIN - config.HEIGHT
+    maxl = img_width - config.HORIZONTAL_MARGIN - config.WIDTH
+    t = tf.random_uniform(
+        [], minval=config.VERTICAL_MARGIN, maxval=maxt, dtype=tf.int32)
+    l = tf.random_uniform(
+        [], minval=config.HORIZONTAL_MARGIN, maxval=maxl, dtype=tf.int32)
+    h = tf.constant(config.HEIGHT)
+    w = tf.constant(config.WIDTH)
+    return (t, l, h, w)
+
+def pointInEllipse(x,y,xp,yp,d,D,angle):
+    #tests if a point[xp,yp] is within
+    #boundaries defined by the ellipse
+    #of center[x,y], diameter d D, and tilted at angle
+
+    cosa=math.cos(math.pi - angle)
+    sina=math.sin(math.pi - angle)
+    dd=d/2*d/2
+    DD=D/2*D/2
+
+    a =math.pow(cosa*(xp-x)+sina*(yp-y),2)
+    b =math.pow(sina*(xp-x)-cosa*(yp-y),2)
+    ellipse=(a/dd)+(b/DD)
+
+    if ellipse <= 1:
+        return True
+    else:
+        return False
+
+def get_mask_weight(img):
+
+    def exist_zero(axis_i, axis_j):
+        if axis_i-1 >= 0:
+            if img[axis_i - 1][axis_j] == 0:
+                return True
+        if axis_j-1 >= 0:
+            if img[axis_i][axis_j - 1] == 0:
+                return True
+        if axis_i + 1 < len(img):
+            if img[axis_i + 1][axis_j] == 0:
+                return True
+        if axis_j + 1 < len(img[0]):
+            if img[axis_i][axis_j + 1] == 0:
+                return True
+        return False
+
+    def get_next_near_point(weight, axis_i, axis_j, round_num):
+        if axis_i-1 >= 0:
+            if img[axis_i - 1][axis_j] == 1 and flag[axis_i - 1][axis_j] == 1:
+                queue.put(axis_i - 1)
+                queue.put(axis_j)
+                weight[axis_i - 1][axis_j] = round_num
+                flag[axis_i - 1][axis_j] = 2
+        if axis_j-1 >= 0:
+            if img[axis_i][axis_j - 1] == 1 and flag[axis_i][axis_j - 1] == 1:
+                queue.put(axis_i)
+                queue.put(axis_j - 1)
+                weight[axis_i][axis_j - 1] = round_num
+                flag[axis_i][axis_j - 1] = 2
+        if axis_i + 1 < len(img):
+            if img[axis_i + 1][axis_j] == 1 and flag[axis_i + 1][axis_j] == 1:
+                queue.put(axis_i+1)
+                queue.put(axis_j)
+                weight[axis_i+1][axis_j] = round_num
+                flag[axis_i + 1][axis_j] = 2
+        if axis_j + 1 < len(img[0]):
+            if img[axis_i][axis_j + 1] == 1 and flag[axis_i][axis_j+1] == 1:
+                queue.put(axis_i)
+                queue.put(axis_j+1)
+                weight[axis_i][axis_j+1] = round_num
+                flag[axis_i][axis_j + 1] = 2
+
+    queue = Queue()
+    flag = np.copy(img)
+    weight = np.copy(img)
+    for i in range(len(img)):
+        for j in range(len(img[0])):
+            if img[i][j] == 1 and exist_zero(i, j):
+                queue.put(i)
+                queue.put(j)
+                weight[i][j] = 1
+                flag[i][j] = 2
+    round = queue.qsize()/2
+    round_num = 1
+    while not queue.empty():
+        if(round == 0):
+            round = queue.qsize() / 2
+            round_num += 1
+        round -= 1
+        i = queue.get()
+        j = queue.get()
+        get_next_near_point(weight, i, j, round_num)
+
+
+    # gamma = 0.7 / round_num
+    # for i in range(len(img)):
+    #     for j in range(len(img[0])):
+    #         if weight[i][j] != 0:
+    #             weight[i][j] = (round_num - weight[i][j] + 1) * gamma + 0.7
+
+    gamma = 0.95
+    for i in range(len(img)):
+        for j in range(len(img[0])):
+            if weight[i][j] != 0:
+                weight[i][j] = gamma ** weight[i][j]
+
+    return weight
+
+def pointInEllipse(x,y,xp,yp,d,D,angle):
+    #tests if a point[xp,yp] is within
+    #boundaries defined by the ellipse
+    #of center[x,y], diameter d D, and tilted at angle
+
+    cosa=math.cos(math.pi - angle)
+    sina=math.sin(math.pi - angle)
+    dd=d/2*d/2
+    DD=D/2*D/2
+
+    a =math.pow(cosa*(xp-x)+sina*(yp-y),2)
+    b =math.pow(sina*(xp-x)-cosa*(yp-y),2)
+    ellipse=(a/dd)+(b/DD)
+
+    if ellipse <= 1:
+        return True
+    else:
+        return False
+
+def bbox2mask_without_value(config, name='mask'):
+    """Generate mask tensor from bbox.
+
+    Args:
+        bbox: configuration tuple, (top, left, height, width)
+        config: Config should have configuration including IMG_SHAPES,
+            MAX_DELTA_HEIGHT, MAX_DELTA_WIDTH.
+
+    Returns:
+        tf.Tensor: output with shape [1, H, W, 1]
+
+    """
+
+    def random_walk_nvidia_new(height, width):
+        """Generates a random irregular mask with lines, circles and elipses"""
+        mask = np.zeros((config.BATCH_SIZE, height, width, 1), np.float32)
+        img = np.zeros((height, width), np.float32)
+
+        # Set size scale
+        size = int((width + height) * 0.03)
+        flag = randint(0, 1)
+        if flag == 0:
+            x_center = randint(int(width / 4), int(width - 1 - width / 4))
+            y_center = randint(int(height / 4), int(height - 1 - height / 4))
+
+            S = randint(6553, 9830)  # area range 10% - 15%
+            axis_short = int(math.sqrt(S) * randint(40, 80) / 100)
+            axis_long = int(S / axis_short)
+            angle_ellipse = random.uniform(0.0, math.pi)
+            maxAngleScale = random.uniform(2.0, math.pi)  # 2.0  # 0~2*pi/4
+            radius = randint(int(axis_short / 4), int(axis_short / 2))
+            theta = maxAngleScale * 2 * math.pi * random.random()
+            x2_temp = x_center + radius * math.cos(theta)
+            y2_temp = y_center + radius * math.sin(theta)
+            for _ in range(randint(60, 150)):
+                x1 = x2_temp
+                y1 = y2_temp
+                bInEllipse = False
+                x2 = x_center
+                y2 = y_center
+                while bInEllipse == False:
+                    radius, theta = [randint(int(axis_long / 4), int(axis_long / 2)),
+                                     maxAngleScale * 2 * math.pi * random.random()]
+                    x2 = x_center + radius * math.cos(theta)
+                    y2 = y_center + radius * math.sin(theta)
+                    bInEllipse = pointInEllipse(x_center, y_center, x2, y2, axis_short, axis_long, angle_ellipse)
+                    # print('bInEllipse: {}'.format(bInEllipse))
+                x2_temp = x2
+                y2_temp = y2
+                thickness = randint(6, size)
+                cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)), (1, 1, 1), thickness)
+                cv2.circle(img, (int(x1), int(y1)), int(thickness / 2), (1, 1, 1), -1)
+                cv2.circle(img, (int(x2), int(y2)), int(thickness / 2), (1, 1, 1), -1)
+        else:
+            # Draw random lines
+            for _ in range(randint(1, 20)):
+                x1, x2 = randint(1, width), randint(1, width)
+                y1, y2 = randint(1, height), randint(1, height)
+                thickness = randint(3, size)
+                cv2.line(img, (x1, y1), (x2, y2), (1, 1, 1), thickness)
+
+            # Draw random circles
+            for _ in range(randint(1, 20)):
+                x1, y1 = randint(1, width), randint(1, height)
+                radius = randint(3, size)
+                cv2.circle(img, (x1, y1), radius, (1, 1, 1), -1)
+
+            # Draw random ellipses
+            for _ in range(randint(1, 20)):
+                x1, y1 = randint(1, width), randint(1, height)
+                s1, s2 = randint(1, width), randint(1, height)
+                a1, a2, a3 = randint(3, 180), randint(3, 180), randint(3, 180)
+                thickness = randint(3, size)
+                cv2.ellipse(img, (x1, y1), (s1, s2), a1, a2, a3, (1, 1, 1), thickness)
+        # cv2.imshow('mask', img)
+        # cv2.waitKey(0)
+        mask[:, :, :, 0] = img[:, :]
+        return mask
+
+    with tf.variable_scope(name), tf.device('/cpu:0'):
+        img_shape = config.MASK_SHAPES
+        height = img_shape[0]
+        width = img_shape[1]
+
+        mask = tf.py_func(
+            random_walk_nvidia_new,
+            [height, width],
+            tf.float32, stateful=False)
+        mask.set_shape([config.BATCH_SIZE] + [height, width] + [1])
+    return mask
+
+def bbox2mask_guide(config, name='guide'):
+
+    def random_walk_nvidia_new(height, width):
+        flag = randint(0,1)
+        if flag == 0:
+            guide = np.zeros((config.BATCH_SIZE, height, width, 1), np.float32)
+        else:
+            guide = np.ones((config.BATCH_SIZE, height, width, 1), np.float32)
+        return guide
+
+    with tf.variable_scope(name), tf.device('/cpu:0'):
+        img_shape = config.MASK_SHAPES
+        height = img_shape[0]
+        width = img_shape[1]
+
+        guide = tf.py_func(
+            random_walk_nvidia_new,
+            [height, width],
+            tf.float32, stateful=False)
+        guide.set_shape([config.BATCH_SIZE] + [height, width] + [1])
+    return guide
+
+def bbox2mask(config, name='mask'):
+
+    def random_walk_nvidia_new(height, width):
+        """Generates a random irregular mask with lines, circles and elipses"""
+        mask = np.zeros((config.BATCH_SIZE, height, width*2, 1), np.float32)
+        img = np.zeros((height, width), np.float32)
+        # img = np.zeros((height, width, channels), np.uint8)
+
+        # Set size scale
+        size = int((width + height) * 0.03)
+        if width < 64 or height < 64:
+            raise Exception("Width and Height of mask must be at least 64!")
+
+        x_center, y_center = randint(int(width / 5), int(width - 1 - width / 5)), randint(int(height / 5),
+                                                                                          int(height - 1 - height / 5))
+        bCircleOrEllipse = randint(0, 3)
+        # bCircleOrEllipse = 0
+        if bCircleOrEllipse == 0:  # circle
+            init_r = int((width / 6) * (height / 6))
+            init_r_max = int((width) * (height) / 9)
+            maxAngleScale = 0.2  # 0~2*pi/8
+            r_squared, theta = [randint(init_r, init_r_max), maxAngleScale * 2 * math.pi * random.random()]
+            x2_temp = x_center + math.sqrt(r_squared) * math.cos(theta)
+            y2_temp = y_center + math.sqrt(r_squared) * math.sin(theta)
+            for _ in range(randint(10, 120)):
+                # r_squared, theta = [randint(init_r, int(width*width/(3*3))), maxAngleScale * 2 * math.pi * random.random()]
+                # x1 = x_center + math.sqrt(r_squared)*math.cos(theta)
+                # y1 = y_center + math.sqrt(r_squared)*math.sin(theta)
+                x1 = x2_temp
+                y1 = y2_temp
+                r_squared, theta = [randint(init_r, init_r_max), maxAngleScale * 2 * math.pi * random.random()]
+                x2 = x_center + math.sqrt(r_squared) * math.cos(theta)
+                y2 = y_center + math.sqrt(r_squared) * math.sin(theta)
+                x2_temp = x2
+                y2_temp = y2
+                thickness = randint(10, size)
+                cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)), (1, 1, 1), thickness)
+                cv2.circle(img, (int(x1), int(y1)), int(thickness / 2), (1, 1, 1), -1)
+                cv2.circle(img, (int(x2), int(y2)), int(thickness / 2), (1, 1, 1), -1)
+        else:
+            S = randint(2260, 8418)  # min area 208 --- max area 12522
+            axis_short = int(math.sqrt(S) * randint(4, 8) / 10)
+            axis_long = int(S / axis_short)
+
+            # axis_short = randint(int(width/3), int(width/3))
+            # axis_long = (random.uniform(1.5, 4)) * axis_short
+            # axis_long = (random.uniform(6, 6)) * axis_short
+            angle_ellipse = random.uniform(0, math.pi)
+            init_r = int(axis_short / 4)
+            maxAngleScale = 1.0  # 0~2*pi/4
+            radius, theta = [randint(init_r, int(axis_short / 2)), maxAngleScale * 2 * math.pi * random.random()]
+            x2_temp = x_center + radius * math.cos(theta)
+            y2_temp = y_center + radius * math.sin(theta)
+            for _ in range(randint(50, 150)):
+                x1 = x2_temp
+                y1 = y2_temp
+                bInEllipse = False
+                x2 = x_center
+                y2 = y_center
+                while bInEllipse == False:
+                    radius, theta = [randint(int(axis_long / 4), int(axis_long / 2)),
+                                     maxAngleScale * 2 * math.pi * random.random()]
+                    x2 = x_center + radius * math.cos(theta)
+                    y2 = y_center + radius * math.sin(theta)
+                    bInEllipse = pointInEllipse(x_center, y_center, x2, y2, axis_short, axis_long, angle_ellipse)
+                    # print('bInEllipse: {}'.format(bInEllipse))
+                x2_temp = x2
+                y2_temp = y2
+                thickness = randint(10, size)
+                cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)), (1, 1, 1), thickness)
+                cv2.circle(img, (int(x1), int(y1)), int(thickness / 2), (1, 1, 1), -1)
+                cv2.circle(img, (int(x2), int(y2)), int(thickness / 2), (1, 1, 1), -1)
+
+        value = get_mask_weight(img)
+        mask_maskvalue = np.concatenate((img, value), axis=1)
+        mask[:, :, :, 0] = mask_maskvalue[:, :]
+        return mask
+
+    with tf.variable_scope(name), tf.device('/cpu:0'):
+        img_shape = config.MASK_SHAPES
+        height = img_shape[0]
+        width = img_shape[1]
+
+        mask = tf.py_func(
+            random_walk_nvidia_new,
+            [height, width],
+            tf.float32, stateful=False)
+        mask.set_shape([config.BATCH_SIZE] + [height, width*2] + [1])
+    return mask
+
+
+def local_patch(x, bbox):
+    """Crop local patch according to bbox.
+
+    Args:
+        x: input
+        bbox: (top, left, height, width)
+
+    Returns:
+        tf.Tensor: local patch
+
+    """
+    x = tf.image.crop_to_bounding_box(x, bbox[0], bbox[1], bbox[2], bbox[3])
+    return x
+
+
+def resize_mask_like(mask, x):
+    """Resize mask like shape of x.
+
+    Args:
+        mask: Original mask.
+        x: To shape of x.
+
+    Returns:
+        tf.Tensor: resized mask
+
+    """
+    mask_resize = resize(
+        mask, to_shape=x.get_shape().as_list()[1:3],
+        func=tf.image.resize_nearest_neighbor)
+    return mask_resize
+
+def resize_freefrom_mask_like(mask, x):
+    """Resize mask like shape of x.
+
+    Args:
+        mask: Original mask.
+        x: To shape of x.
+
+    Returns:
+        tf.Tensor: resized mask
+
+    """
+    mask_resize = resize(
+        mask, to_shape=x.get_shape().as_list()[1:3],
+        func=tf.image.resize_nearest_neighbor)
+    return mask_resize
+
+def spatial_discounting_mask(config):
+    """Generate spatial discounting mask constant.
+
+    Spatial discounting mask is first introduced in publication:
+        Generative Image Inpainting with Contextual Attention, Yu et al.
+
+    Args:
+        config: Config should have configuration including HEIGHT, WIDTH,
+            DISCOUNTED_MASK.
+
+    Returns:
+        tf.Tensor: spatial discounting mask
+
+    """
+    gamma = config.SPATIAL_DISCOUNTING_GAMMA
+    shape = [1, config.HEIGHT, config.WIDTH, 1]
+    if config.DISCOUNTED_MASK:
+        logger.info('Use spatial discounting l1 loss.')
+        mask_values = np.ones((config.HEIGHT, config.WIDTH))
+        for i in range(config.HEIGHT):
+            for j in range(config.WIDTH):
+                mask_values[i, j] = max(
+                    gamma**min(i, config.HEIGHT-i),
+                    gamma**min(j, config.WIDTH-j))
+        mask_values = np.expand_dims(mask_values, 0)
+        mask_values = np.expand_dims(mask_values, 3)
+        mask_values = mask_values
+    else:
+        mask_values = np.ones(shape)
+    return tf.constant(mask_values, dtype=tf.float32, shape=shape)
+
+def random_walk_nvidia_new(height, width):
+    """Generates a random irregular mask with lines, circles and elipses"""
+    mask = np.zeros((1, height, width, 1), np.float32)
+    img = np.zeros((height, width), np.float32)
+
+    # Set size scale
+    size = int((width + height) * 0.03)
+
+    x_center = randint(int(width / 5), int(width - 1 - width / 5))
+    y_center = randint(int(height / 5), int(height - 1 - height / 5))
+
+    S = randint(11043, 16261)  # area range 5% - 10%
+    axis_short = int(math.sqrt(S) * randint(40, 80) / 100)
+    axis_long = int(S / axis_short)
+    angle_ellipse = random.uniform(0, math.pi)
+    maxAngleScale = 1.0  # 0~2*pi/4
+    radius = randint(int(axis_short / 4), axis_short)
+    theta = maxAngleScale * 2 * math.pi * random.random()
+    x2_temp = x_center + radius * math.cos(theta)
+    y2_temp = y_center + radius * math.sin(theta)
+    for _ in range(randint(20, 150)):
+        x1 = x2_temp
+        y1 = y2_temp
+        bInEllipse = False
+        x2 = x_center
+        y2 = y_center
+        while bInEllipse == False:
+            radius, theta = [randint(int(axis_long / 4), int(axis_long / 2)),
+                             maxAngleScale * 2 * math.pi * random.random()]
+            x2 = x_center + radius * math.cos(theta)
+            y2 = y_center + radius * math.sin(theta)
+            bInEllipse = pointInEllipse(x_center, y_center, x2, y2, axis_short, axis_long, angle_ellipse)
+            # print('bInEllipse: {}'.format(bInEllipse))
+        x2_temp = x2
+        y2_temp = y2
+        thickness = randint(6, size)
+        cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)), (1, 1, 1), thickness)
+        cv2.circle(img, (int(x1), int(y1)), int(thickness / 2), (1, 1, 1), -1)
+        cv2.circle(img, (int(x2), int(y2)), int(thickness / 2), (1, 1, 1), -1)
+    cv2.imshow('mask', img)
+    cv2.waitKey(0)
+    mask[:, :, :, 0] = img[:, :]
+    return mask
+
+if __name__ == '__main__':
+    for i in range(0,100):
+        mask = random_walk_nvidia_new(256, 256)
\ No newline at end of file
