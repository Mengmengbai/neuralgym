Index: SaveProtxt_inpainting200106.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting200106.py	(date 1578655442000)
+++ SaveProtxt_inpainting200106.py	(date 1578655442000)
@@ -0,0 +1,279 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1), depthwise不能改变通道数，所以需要再来一个1*1卷积
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name, input_channel = 0):
+    if input_channel == 0:
+        input_channel = num_filter
+    convs_depthwise = convs(data=data, num_filter=input_channel, kernel=kernel, stride=stride, pad=pad, group=input_channel, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+
+def gen_gate_conv(x, inputChannel, cnum, ksize, stride=1, rate=1, name='gen_gate_conv'):
+    if stride == 1:
+        padding = ksize // 2
+        input1 = x
+        ######## feature #########
+        conv_f = conv(data=x, num_filter=cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate,
+                      name=name + '_conv_f')
+        out_f = leakyReLU(conv_f, name=name + "_conv_f_ReLU")
+
+        ######## Soft mask gating  #############
+        conv_g_1 = conv(data=x, num_filter=cnum, kernel=1, stride=1, pad=0, dilation=1, name=name + "_conv_g_1")
+        conv_g_1 = leakyReLU(conv_g_1, name=name + "_conv_g_1_ReLU")
+
+        conv_g_2 = separable_convs(data=conv_g_1, num_filter=cnum, kernel=ksize, stride=stride, pad=padding, dilation=1,
+                                   name=name + "_separable_conv_g_2", input_channel=cnum)
+        conv_g_2 = leakyReLU(conv_g_2, name=name + "_conv_g_2_ReLU")
+
+        conv_g_3 = conv(data=conv_g_2, num_filter=cnum, kernel=1, stride=1, pad=0, dilation=1, name=name + '_conv_g_3')
+        conv_g_3 = caffe.layers.Sigmoid(conv_g_3, in_place=True, name=name + "_conv_g_3_Sigmoid")
+
+        ######## Soft guide gating #############
+        conv_g1_1 = separable_convs(data=input1, num_filter=cnum, kernel=3, stride=stride, pad=padding, dilation=1,
+                                    name=name + "_separable_conv_g1_1", input_channel=inputChannel)
+        conv_g1_1 = caffe.layers.Sigmoid(conv_g1_1, in_place=True, name=name + "_conv_g1_1_Sigmoid")
+
+        # concat the gating
+        conv_g = caffe.layers.Concat(conv_g_3, conv_g1_1, axis=1, name=name + "_concat")
+        conv_g = conv(data=conv_g, num_filter=cnum, kernel=1, stride=1, pad=0, dilation=1, name=name + "_conv_g")
+        conv_g = caffe.layers.Sigmoid(conv_g, in_place=True, name=name + "_conv_g_Sigmoid")
+
+        # x = caffe.layers.Concat(out_f, conv_g, axis=1, name=name + "_concat_test")
+        # out_f, conv_g = caffe.layers.Slice(x, ntop=2, name=name + '_slice_test',
+        #                                    slice_param=dict(slice_dim=2, slice_point=[cnum]))
+
+        # Elementwise Multiply
+        x = caffe.layers.Eltwise(out_f, conv_g, operation=0, name=name + "_Eltwise")
+    else:
+        padding = ksize // 2 * rate
+        input1 = x
+        ######## feature ##########
+        conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+        out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+        ######## Soft mask gating #########
+        conv_g_1 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad=padding, dilation = 1, name = name+"_conv_g_1")
+        conv_g_1 = leakyReLU(conv_g_1, name = name + "_conv_g_1_ReLU")
+
+        conv_g_2 = separable_convs(data=conv_g_1, num_filter=cnum, kernel=ksize, stride=stride, pad=0, dilation=1, name=name+"_separable_conv_g_2", input_channel=cnum)
+        conv_g_2 = leakyReLU(conv_g_2, name=name+"_conv_g_2_ReLU")
+
+        conv_g_3 = conv(data = conv_g_2, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+'_conv_g_3')
+        conv_g_3 = caffe.layers.Sigmoid(conv_g_3, in_place = True, name = name + "_conv_g_3_Sigmoid")
+
+
+        # Soft guide gating
+        conv_g1_1 = separable_convs(data=input1, num_filter=cnum, kernel=3, stride=stride, pad=1, dilation=1, name=name+"_separable_conv_g1_1", input_channel=inputChannel)
+        conv_g1_1 = caffe.layers.Sigmoid(conv_g1_1, in_place = True, name = name + "_conv_g1_1_Sigmoid")
+
+        # concat the gating
+        conv_g = caffe.layers.Concat(conv_g_3, conv_g1_1, axis=1, name = name + "_concat")
+        conv_g = conv(data = conv_g, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g")
+        conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+        # x = caffe.layers.Concat(out_f, conv_g, axis = 1, name = name + "_concat_test")
+        # out_f, conv_g = caffe.layers.Slice(x, ntop=2, name=name+'_slice_test', slice_param=dict(slice_dim=2, slice_point=[cnum]) )
+
+        # Elementwise Multiply
+        # x = out_f * conv_g
+        x = caffe.layers.Eltwise(out_f, conv_g, operation = 0, name = name + "_Eltwise")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.leaky_relu(data)
+    data = leakyReLU(data, name = name + '_deconv_ReLU')
+    return data
+
+def _random_offset(x, noise, knum = 1, width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1, _x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1, _x2 = caffe.layers.Slice(noise1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    #需要复制与x相同的通道数
+    for i in range(5) : #32通道
+        noise1 = caffe.layers.Concat(noise1, noise1, axis = 1, name = name + "_noise1_concat_%d"%i)
+    for j in range(1, knum): # knum*cnum(=32)
+        noise1 = caffe.layers.Concat(noise1, noise1, axis = 1, name = name + "_noise1_concat_32_%d"%j)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+    # noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)#需要复制与x相同的通道数
+
+    x1, _x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, _x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    _x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    _x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi")
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    filters = knum * cnum
+    res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+    _res_r, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-r", slice_param=dict(slice_dim = 2, slice_point=[1]))
+    _res_c, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-c", slice_param=dict(slice_dim = 3, slice_point=[1]))
+    _res_r = caffe.layers.Reshape(_res_r, name = name + "_reshape_257_1")
+    _res_c = caffe.layers.Reshape(_res_c, name = name + "_reshape_1_256")
+    _x_256_1 = caffe.layers.Concat(_x3, _x5, axis = 1, name = name + "_others_256_1") # (256 * 1)
+    _x_1_255 = caffe.layers.Concat(_x4, _x6, axis = 1, name = name + "_others_1_255") # (1 * 255)
+    _x_256_1 = caffe.layers.Reshape(_x_256_1, name = name + "_reshape_256_1")
+    _x_1_255 = caffe.layers.Reshape(_x_1_255, name = name + "_reshape_1_255")
+    _x1 = caffe.layers.Reshape(_x1, name = name + "_reshape_512_257")
+    _x2 = caffe.layers.Reshape(_x2, name = name + "_reshape_257_255")
+    _x = caffe.layers.Concat(_res_r, _res_c, _x_256_1, _x_1_255, _x1, _x2, axis = 1, name = name + "_concat_others")
+    return res, _x
+
+
+def random_offset(x, noise, net, knum = 1,width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1_1, net.output_x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1_2, net.output_x2 = caffe.layers.Slice(noise1_1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+
+    noise1 = caffe.layers.Concat(noise1_2, noise1_2, axis=1, name=name + "_noise1_concat_32_1")
+    for j in range(2, knum*cnum): # knum*cnum(=32)
+        noise1 = caffe.layers.Concat(noise1, noise1_2, axis = 1, name = name + "_noise1_concat_32_%d"%j)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+
+    x1, net.output_x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, net.output_x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    net.output_x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    net.output_x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi") # x2 * (1.0 - noise1)
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    # filters = knum * cnum
+    # res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+
+    return res
+
+# caffe (N*C*H*W)  tensorflow (默认 N*H*W*C， 也支持 N*C*H*W)
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    randomoff2 = 1 # for test11
+    #输入为512*512*5
+    input_x = caffe.layers.Scale(net.data, bias_term = True, name = "input_x_Scale")
+    input_x, noise = caffe.layers.Slice(input_x, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, 4, cnum, 5, 2, name='conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = "conv1_BatchNorm")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = "conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_2 = gen_gate_conv(x_1, cnum, 2 * cnum, 5, 2, name='conv2')
+    #x_2 = tf.layers.batch_normalization(x_2)
+    # x_2 = caffe.layers.BatchNorm(x_2, use_global_stats = True, name = "conv2_BatchNorm")#测试时true，训练时false
+    # x_2 = caffe.layers.Scale(x_2, bias_term = True, name = "conv2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_3 = gen_gate_conv(x_2, 2 * cnum, 4 * cnum, 3, 2, name='conv3')
+    #x_3 = tf.layers.batch_normalization(x_3)
+    # x_3 = caffe.layers.BatchNorm(x_3, use_global_stats = True, name = "conv3_BatchNorm")#测试时true，训练时false
+    # x_3 = caffe.layers.Scale(x_3, bias_term = True, name = "conv3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 4 * cnum, 3, 2, name='conv4')
+    #x_4 = tf.layers.batch_normalization(x_4)
+    # x_4 = caffe.layers.BatchNorm(x_4, use_global_stats = True, name = "conv4_BatchNorm")#测试时true，训练时false
+    # x_4 = caffe.layers.Scale(x_4, bias_term = True, name = "conv4_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x_4, 4*cnum, name='upsample1')
+    x = conv(data = x, num_filter = 4*cnum, kernel=3, stride=1, pad=1, dilation=1, name='upsample1_conv1')
+    x = leakyReLU(x, name="up_conv1")
+    # x = tf.layers.batch_normalization(x)
+    # x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample1_BatchNorm")#测试时true，训练时false
+    # x = caffe.layers.Scale(x, bias_term = True, name = "upsample1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 4*cnum, name='upsample2')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'upsample2_concat')
+    x = gen_gate_conv(x, 6*cnum, 4*cnum, 3, 1, name='up_conv2')
+    # x = tf.layers.batch_normalization(x)
+    # x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample2_BatchNorm")#测试时true，训练时false
+    # x = caffe.layers.Scale(x, bias_term = True, name = "upsample2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 2 * cnum, name='upsample3')
+    if randomoff2:
+        x = random_offset(x, noise, net, 2, name = "upsample3_randomoff")
+
+    net.output_x_1_r, x_1 = caffe.layers.Slice(x_1, ntop=2, name="x_1_slice_res-r", slice_param=dict(slice_dim=2, slice_point=[1]))
+    net.output_x_1_c, x_1 = caffe.layers.Slice(x_1, ntop=2, name="x_1_slice_res-c", slice_param=dict(slice_dim=3, slice_point=[1]))
+
+    x = caffe.layers.Concat(x, x_1, axis=1, name='upsample3_concat')
+    x = gen_gate_conv(x, 3*cnum, cnum, 3, 1, name='upsample3_conv')
+    # x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample3_BatchNorm")#测试时true，训练时false
+    # x = caffe.layers.Scale(x, bias_term = True, name = "upsample3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+
+    net.output_input_x_r, input_x = caffe.layers.Slice(input_x, ntop=2, name="input_x_slice_res-r",
+                                               slice_param=dict(slice_dim=2, slice_point=[2]))
+    net.output_input_x_c, input_x = caffe.layers.Slice(input_x, ntop=2, name="input_x_slice_res-c",
+                                               slice_param=dict(slice_dim=3, slice_point=[2]))
+
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    net.output = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv10')
+
+
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test11_5c_test_LRE_woBN_1.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: weights_tensor2caffe_inpainting_200103.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting_200103.py	(date 1579682195000)
+++ weights_tensor2caffe_inpainting_200103.py	(date 1579682195000)
@@ -0,0 +1,255 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+def tensor2caffe(net,tensor):
+
+    ct_map={"conv1_conv_f:weights"                              :   "inpaint_net/conv1/conv2d/kernel" ,
+            "conv1_conv_f:bias"                                 :   "inpaint_net/conv1/conv2d/bias" ,
+            "conv1_conv_g_1:weights"                            :   "inpaint_net/conv1/conv2d_1/kernel" ,
+            "conv1_conv_g_1:bias"                               :   "inpaint_net/conv1/conv2d_1/bias" ,
+            "conv2_conv_f:weights"                              :   "inpaint_net/conv2/conv2d/kernel" ,
+            "conv2_conv_f:bias"                                 :   "inpaint_net/conv2/conv2d/bias" ,
+            "conv2_conv_g_1:weights"                            :   "inpaint_net/conv2/conv2d_1/kernel" ,
+            "conv2_conv_g_1:bias"                               :   "inpaint_net/conv2/conv2d_1/bias" ,
+            "conv3_conv_f:weights"                              :   "inpaint_net/conv3/conv2d/kernel" ,
+            "conv3_conv_f:bias"                                 :   "inpaint_net/conv3/conv2d/bias" ,
+            "conv3_conv_g_1:weights"                            :   "inpaint_net/conv3/conv2d_1/kernel" ,
+            "conv3_conv_g_1:bias"                               :   "inpaint_net/conv3/conv2d_1/bias" ,
+            "conv4_conv_f:weights"                              :   "inpaint_net/conv4/conv2d/kernel" ,
+            "conv4_conv_f:bias"                                 :   "inpaint_net/conv4/conv2d/bias" ,
+            "conv4_conv_g_1:weights"                            :   "inpaint_net/conv4/conv2d_1/kernel" ,
+            "conv4_conv_g_1:bias"                               :   "inpaint_net/conv4/conv2d_1/bias" ,
+            "upsample2_deconv:weights"                     :   "inpaint_net/upsample2/conv2d_transpose/kernel" ,
+            "upsample2_deconv:bias"                        :   "inpaint_net/upsample2/conv2d_transpose/bias" ,
+            "up_conv6_conv_f:weights"                                     :   "inpaint_net/conv6/conv2d/kernel" ,
+            "up_conv6_conv_f:bias"                                        :   "inpaint_net/conv6/conv2d/bias" ,
+            "up_conv6_conv_g_1:weights"                                     :   "inpaint_net/conv6/conv2d_1/kernel" ,
+            "up_conv6_conv_g_1:bias"                                        :   "inpaint_net/conv6/conv2d_1/bias" ,
+            "upsample3_deconv:weights"                     :   "inpaint_net/upsample3/conv2d_transpose/kernel" ,
+            "upsample3_deconv:bias"                        :   "inpaint_net/upsample3/conv2d_transpose/bias" ,
+            "up_conv7_conv_f:weights"                                     :   "inpaint_net/conv7/conv2d/kernel" ,
+            "up_conv7_conv_f:bias"                                        :   "inpaint_net/conv7/conv2d/bias" ,
+            "up_conv7_conv_g_1:weights"                                     :   "inpaint_net/conv7/conv2d_1/kernel" ,
+            "up_conv7_conv_g_1:bias"                                        :   "inpaint_net/conv7/conv2d_1/bias" ,
+            "upsample4_deconv:weights"                     :   "inpaint_net/upsample4/conv2d_transpose/kernel" ,
+            "upsample4_deconv:bias"                        :   "inpaint_net/upsample4/conv2d_transpose/bias" ,
+            "up_conv8_conv_f:weights"                                     :   "inpaint_net/conv8/conv2d/kernel" ,
+            "up_conv8_conv_f:bias"                                        :   "inpaint_net/conv8/conv2d/bias" ,
+            "up_conv8_conv_g_1:weights"                                     :   "inpaint_net/conv8/conv2d_1/kernel" ,
+            "up_conv8_conv_g_1:bias"                                        :   "inpaint_net/conv8/conv2d_1/bias" ,
+            "upsample5_deconv:weights"                     :   "inpaint_net/upsample5/conv2d_transpose/kernel" ,
+            "upsample5_deconv:bias"                        :   "inpaint_net/upsample5/conv2d_transpose/bias" ,
+            "conv9-2:weights"                                   :   "inpaint_net/conv9-2/kernel" ,
+            "conv9-2:bias"                                      :   "inpaint_net/conv9-2/bias" ,
+            "conv9-1:weights"                                   :   "inpaint_net/conv9-1/kernel" ,
+            "conv9-1:bias"                                      :   "inpaint_net/conv9-1/bias" ,
+            "conv9:weights"                                   :   "inpaint_net/conv9/kernel" ,
+            "conv9:bias"                                      :   "inpaint_net/conv9/bias" ,
+        }
+    
+    num=0
+    net_keys=net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s'%(i, len(net_keys), le, net_keys[i]))
+        if le==1:
+            # if(net_keys[i] == 'upsample1_deconv'): continue
+            # if(net_keys[i] == 'upsample2_deconv'): continue
+            # if(net_keys[i] == 'upsample3_deconv'): continue
+            # if(net_keys[i] == 'upsample4_deconv'): continue
+            # if(net_keys[i] == 'upsample5_deconv'): continue
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value=weights_value.transpose((3,2,0,1)) 
+            else:
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1)) 
+            try:
+                net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+            except:
+                net.params[net_keys[i]][0].data[:,:,:,:]=0
+                net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+            
+            
+            print(weights_value.shape,end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+            
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                print("scale layer!")
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+                continue
+                print("le_last = %d"%le_last)
+                # for BN layers
+                key0=net_keys[i]+':gamma'
+                value0 = ct_map[key0]
+                gamma_value=tensor[value0] 
+
+                key1=net_keys[i]+':beta'
+                value1=ct_map[key1]
+                beta_value=tensor[value1]
+
+                value2 = value0.replace('gamma', 'moving_variance')
+                mv_value=tensor[value2] 
+
+                value3 = value1.replace('beta', 'moving_mean')
+                mm_value=tensor[value3] 
+
+
+                net.params[net_keys[i]][0].data[:] = gamma_value / np.sqrt(mv_value)
+                print(gamma_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                net.params[net_keys[i]][1].data[:] = beta_value - mm_value/np.sqrt(mv_value)*gamma_value
+                print(beta_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+            else:   
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                
+                weights_value=tensor[value0]
+                if "filter"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1))
+                    
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value[:,:,:,:]
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value[:,:,:,:]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+#                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+#                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':bias'
+                value1=ct_map[key1]
+                
+                bias_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=bias_value
+                print(bias_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+    print(num)
+    print('net_keys[0]',net_keys[0])
+    net.params[net_keys[0]][0].data[:] = np.array([1./128, 1./128, 1./128, 1./255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0.])
+
+
+
+def print_caffemodel(net):
+    net_keys=net.params.keys()
+    le_last = 0
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le==1:
+            if 'PReLU' in net_keys[i]:
+                print("PReLU layer!")
+                n = net.params[net_keys[i]][0].data.shape
+                print("%s:value   ->  [%d]"%(net_keys[i], n[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                # BatchNorm layer后面的scale layer, 训练gamma和deta
+                w=net.params[net_keys[i]][0].data.shape
+                print("%s:gamma ->  [%d]"%(net_keys[i], w[0]))
+
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:beta  ->  [%d]"%(net_keys[i], w[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+            
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]"%(net_keys[i],w[0]))
+        elif le==3:
+            #BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]"%(net_keys[i],m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]"%(net_keys[i],v[0]))
+
+            #f = net.params[net_keys[i]][2].data.shape
+            #print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys=tensor.keys()
+    for i in range(0,len(tensor_keys)):
+        le=len(tensor[tensor_keys[i]].shape)
+        if le==1:
+            w=tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]"%(tensor_keys[i],w))
+        elif le==4:
+            weights=tensor[tensor_keys[i]]
+            #if "filter"  in tensor_keys[i]:
+                #weights=tensor[tensor_keys[i]].transpose((2,3,1,0)) 
+            #else:
+                #weights=tensor[tensor_keys[i]].transpose((3,2,1,0)) 
+            n,c,w,h=weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]"%(tensor_keys[i],n,c,w,h))
+
+#print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net('/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test10-t11-0115-6c.prototxt', caffe.TEST)
+    print_caffemodel(net)
+    print("########################################################################################################################")
+    tensor=np.load('inpainting/test10-t11-0121.npy').item()
+    print_tensorflow(tensor)
+    print(
+        "########################################################################################################################")
+    tensor2caffe(net,tensor)
+    net.save("inpainting/test10-t11-0121-6c.caffemodel")
Index: SaveProtxt_inpainting200113_t18.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting200113_t18.py	(date 1578912604000)
+++ SaveProtxt_inpainting200113_t18.py	(date 1578912604000)
@@ -0,0 +1,181 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1), depthwise不能改变通道数，所以需要再来一个1*1卷积
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name, input_channel = 0):
+    if input_channel == 0:
+        input_channel = num_filter
+    convs_depthwise = convs(data=data, num_filter=input_channel, kernel=kernel, stride=stride, pad=pad, group=input_channel, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='gen_gate_conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.Sigmoid(conv_f)
+    out_f = caffe.layers.Sigmoid(conv_f, in_place= True, name=name+"_conv_f_Sigmoid")
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1)
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel = ksize, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g_1")
+    # conv_g_1 = tf.nn.sigmoid(conv_g_1)
+    conv_g = caffe.layers.Sigmoid(conv_g_1, in_place= True, name = name + "_conv_g_1_Sigmoid")
+
+     # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation = 0, name = name + "_Eltwise")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_Sigmoid')
+    return data
+
+def random_offset(x, noise, filters = 32, width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1, _x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1, _x2 = caffe.layers.Slice(noise1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    noise2 = noise1
+    #需要复制与x相同的通道数
+    for i in range(1, filters) : #32通道
+        noise1 = caffe.layers.Concat(noise2, noise1, axis = 1, name = name + "_noise1_concat_%d"%i)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+    # noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)#需要复制与x相同的通道数
+
+    x1, _x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, _x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    _x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    _x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi")
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+    _res_r, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-r", slice_param=dict(slice_dim = 2, slice_point=[1]))
+    _res_c, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-c", slice_param=dict(slice_dim = 3, slice_point=[1]))
+    _res_r = caffe.layers.Reshape(_res_r, name = name + "_reshape_257_1")
+    _res_c = caffe.layers.Reshape(_res_c, name = name + "_reshape_1_256")
+    _x_256_1 = caffe.layers.Concat(_x3, _x5, axis = 1, name = name + "_others_256_1") # (256 * 1)
+    _x_1_255 = caffe.layers.Concat(_x4, _x6, axis = 1, name = name + "_others_1_255") # (1 * 255)
+    _x_256_1 = caffe.layers.Reshape(_x_256_1, name = name + "_reshape_256_1")
+    _x_1_255 = caffe.layers.Reshape(_x_1_255, name = name + "_reshape_1_255")
+    _x1 = caffe.layers.Reshape(_x1, name = name + "_reshape_512_257")
+    _x2 = caffe.layers.Reshape(_x2, name = name + "_reshape_257_255")
+    _x = caffe.layers.Concat(_res_r, _res_c, _x_256_1, _x_1_255, _x1, _x2, axis = 1, name = name + "_concat_others")
+    return res, _x
+
+# caffe (N*C*H*W)  tensorflow (默认 N*H*W*C， 也支持 N*C*H*W)
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    randomoff2 = 1 # for test11
+    #输入为512*512*5
+    # input_x, noise = caffe.layers.Slice(net.data, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    input_x = caffe.layers.Scale(net.data, bias_term = True, name = "input_x_Scale")
+    net.rgb, mask = caffe.layers.Slice(input_x, ntop = 2, name='slice_mask', slice_param=dict(slice_dim=1, slice_point=[3]))
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+    x_2 = gen_gate_conv(x_1, 2 * cnum, 5, 2, name='conv2')
+    x_3 = gen_gate_conv(x_2, 4 * cnum, 3, 2, name='conv3')
+    x_4 = gen_gate_conv(x_3,  4 * cnum, 3, 2, name='conv4')
+
+    x = gen_normal_deconv(x_4, 4*cnum, name='upsample2')
+    # x = tf.concat([x, x_3], axis=3, name='concat2')
+    x = caffe.layers.Concat(x, x_3, axis = 1, name = 'upsample2_concat2')
+    # x = tf.layers.conv2d(x, 4 * cnum, 3, 1, activation=tf.nn.leaky_relu, padding='same', name='up_conv1')
+    x = gen_gate_conv(x, 2*cnum, 3, 1, padding='same',name='up_conv6')
+
+    x = gen_normal_deconv(x, 2*cnum, name='upsample3')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'upsample3_concat3')
+    x = gen_gate_conv(x, cnum, 3, 1, padding='same',name='up_conv7')
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+    x = caffe.layers.Concat(x, x_1, axis = 1, name = 'upsample3_concat8')
+    # x = conv(data = x, num_filter = cnum, kernel = 3, stride = 1, pad = 1, dilation = 1, name = "conv8")
+    x = gen_gate_conv(x, cnum, 3, 1, padding='same', name='up_conv8')
+
+    x_f = gen_normal_deconv(x, cnum, name='upsample5')
+    for i in range(0, 5) : #32通道
+        mask = caffe.layers.Concat(mask, mask, axis = 1, name = "mask_concat_%d"%i)
+    x_f_msk = caffe.layers.Eltwise(x_f, mask, operation = 0, name = "Eltwise_mask")
+    x_f_msk = conv(data=x_f_msk, num_filter=3, kernel = 3, stride = 1, pad = 1, dilation = 1, name = 'conv9-2')
+    x_f_msk = caffe.layers.ReLU(x_f_msk, in_place = True, name = 'conv9-2-ReLU')
+    # x = tf.concat([x, input_x], axis=3, name='concat9')
+
+    x = caffe.layers.Concat(x_f, x_f_msk, input_x, axis = 1, name = 'concat9')
+    # x = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9-1')
+    x = caffe.layers.ReLU(x, in_place = True, name = 'conv9-1-ReLU')
+    net.output = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9')
+
+
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test10-t18.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: SaveProtxt_inpainting1028.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting1028.py	(date 1577777240000)
+++ SaveProtxt_inpainting1028.py	(date 1577777240000)
@@ -0,0 +1,390 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1)
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name):
+    convs_depthwise = convs(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, group=num_filter, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+# v7 2gateCon
+def gen_gate_conv_1(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=caffe.params.ReLU, training=True):
+    print('gen_gate_conv_1 (%s) start...'%name)
+    padding = ksize // 2 * rate
+    #conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+"_conv_f")
+    # out_f = leaky_relu(conv_f)
+    out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+
+    # # Soft 5*5 gating
+    # conv_g5 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g5 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g5")
+    # conv_g5 = leaky_relu(conv_g5)
+    conv_g5 = leakyReLU(conv_g5, name = name+"_conv_g5_ReLU")
+    # conv_g5 = tf.layers.separable_conv2d(inputs=conv_g5, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g5 = separable_convs(data = conv_g5, num_filter = cnum, kernel = ksize, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g5_separable")
+    # conv_g5 = tf.nn.sigmoid(conv_g5)
+    conv_g5 = caffe.layers.Sigmoid(conv_g5, in_place = True, name = name+"_conv_g5_separable_Sigmoid")
+
+    # # Soft 3*3 gating
+    padding = 3 // 2 * rate
+    # conv_g3 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g3 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g3")
+    # conv_g3 = leaky_relu(conv_g3)
+    conv_g3 = leakyReLU(conv_g3, name = name+"_conv_g3_ReLU")
+    # conv_g3 = tf.layers.separable_conv2d(inputs=conv_g3, filters=cnum, kernel_size=3, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g3 = separable_convs(data = conv_g3, num_filter = cnum, kernel = 3, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g3_separable")
+    # conv_g3 = tf.nn.sigmoid(conv_g3)
+    conv_g3 = caffe.layers.Sigmoid(conv_g3, in_place = True, name = "_conv_g3_separable_Sigmoid")
+
+    # # Soft 1*1 gating
+    # conv_g1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g1 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g1")
+    # conv_g1 = leaky_relu(conv_g1)
+    conv_g1 = leakyReLU(conv_g1, name = name + "_conv_g1_ReLU")
+    # conv_g1 = tf.layers.separable_conv2d(inputs=conv_g1, filters=cnum, kernel_size=1, padding='same',  strides=stride, dilation_rate=1)
+    conv_g1 = separable_convs(data = conv_g1, num_filter = cnum, kernel = 1, stride = stride, pad = 0, dilation = 1, name = name+"_conv_g1_separable")
+    # conv_g1 = tf.nn.tanh(conv_g1)
+    conv_g1 = caffe.layers.TanH(conv_g1, in_place = True, name = name + "_conv_g1_separable_TanH")
+
+    # # concat the gating
+    # conv_g = tf.concat([conv_g5, conv_g3], axis=3)
+    conv_g = caffe.layers.Concat(conv_g5, conv_g3, axis = 1)
+    # conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = conv(data = conv_g, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g")
+    # conv_g = tf.nn.sigmoid(conv_g)
+    conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+    # # Elementwise Multiply
+    # res_mul = tf.multiply(out_f, conv_g)
+    res_mul = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")#'PROD'-0,'SUM'-1,'MAX'-2
+    # x = tf.add(res_mul, conv_g1)
+    x = caffe.layers.Eltwise(res_mul, conv_g1, name = name+"_add")
+    print('gen_gate_conv_1 end!')
+    return x
+
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.Sigmoid(conv_f)
+    out_f = caffe.layers.Sigmoid(conv_f, in_place = True, name = name + '_out_f')
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_g_1')
+    # conv_g_1 = leaky_relu(conv_g_1)
+    # conv_g = tf.nn.sigmoid(conv_g_1)
+    conv_g = caffe.layers.Sigmoid(conv_g_1, in_place = True, name = name + '_out_g')
+    # conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=3, strides=1, padding='same')
+    # conv_g = tf.nn.sigmoid(conv_g_2)
+    # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")
+    return x
+
+def gate_res_block(x, cnum, name):
+    print("gate_res_block (%s) start ..."%name)
+    x_1 = gen_gate_conv(x, cnum = cnum, ksize=3, name=name+'_conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_BatchNorm")#测试时true，训练时false
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+    #x_1 = leaky_relu(x_1)
+    x_1 = leakyReLU(x_1, name = "_conv1_BatchNorm_Scale_ReLU")
+
+    x_1 = gen_gate_conv(x_1, cnum = cnum, ksize=3, name=name+'_conv2')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_BatchNorm")
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_BatchNorm_Scale")
+
+    #out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    #out = leaky_relu(out)
+    out = leakyReLU(out, name = name+"_out")
+    print("gate_res_block end!")
+    return out
+    
+def resnetblock(x, cnum, name = 'res'):
+    x_1 = gen_gate_conv(x, cnum, 3, name = name + '_conv1')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn1')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_bn1")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_bn1_Scale")#BatchNorm+Scale实现
+    # x_1 = tf.nn.relu(x_1)
+    x_1 = caffe.layers.ReLU(x_1, in_place = True, name = name + '_conv1_bn1_Scale_ReLU')
+    x_1 = gen_gate_conv(x_1, cnum, 3, name= name + '_conv2')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn2')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_bn2")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_bn2_Scale")#BatchNorm+Scale实现
+    # out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    return out
+
+def gen_layer_deconv(x, cnum, kernel_size = 3, stride = 2, name='upsample', pad='same', training=True):
+    # x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose')
+    k = kernel_size + 1
+    padding = k // 2 - 1
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=k,stride=2,pad=padding,bias_term=True), name=name+"_deconv")
+    # x = tf.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = name+"_deconv_Sigmoid")
+    return x
+
+def gen_layer_gate_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', pad='same', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data =caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.relu(data)
+    data = caffe.layers.ReLU(data, in_place = True, name = name+"_deconv_ReLU")
+
+    # data_gate = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv_gate")
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place = True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name+'_Eltwise')
+    return x
+
+def gen_deconv2(x, cnum, stride=1, name='upsample', padding='SAME', training=True):
+    data = deconv2(data=x, num_filter=cnum, kernel=3, stride=2, pad=1, name=name+'_dconv')
+    return data
+
+def gen_deconv_1(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    #x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding='same')
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    #x = tf.nn.relu(x)
+    x = caffe.layers.ReLU(x, in_place = True, name = name + '_deconv_ReLU')
+    return x
+
+def gen_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    # filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+    # data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], padding)
+    # data = tf.nn.bias_add(data, bias)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+'_deconv')
+    # data = leaky_relu(data, alpha=0.1)
+    data = leakyReLU(data, name = name + '_deconv_ReLU', alpha=0.1)
+    # data = caffe.layers.ReLU(data, in_place=True, name=name+"_deconv_ReLU")
+
+    # filt_gate = tf.get_variable('conv2d_transpose_gate/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias_gate = tf.get_variable('conv2d_transpose_gate/bias', [cnum], tf.float32)
+    # data_gate = tf.nn.conv2d_transpose(x, filt_gate, [b, scale_size * w, scale_size * h, c], [1, stride, stride, 1], padding)
+    # data_gate = tf.nn.bias_add(data_gate, bias_gate)
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum,kernel_size=4,stride=2,pad=1,bias_term=True),name=name+'_deconv_gate')
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place=True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name + '_Eltwise')
+    return x
+
+
+def resize_conv(x, cnum, kernel_size=3,stride=1, name='upsample', padding='SAME', training=True):
+    # x = resize(x, func=tf.image.resize_bilinear)
+    factor = 2
+    k = 2 * factor - factor % 2
+    p = int(math.ceil((factor - 1) / 2.))
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, group=cnum,kernel_size=k,stride=2,pad=p,weight_filler={"type": "bilinear"},bias_term=False), name=name+"_deconv")
+    # x = tf.layers.conv2d(
+    #     x, cnum, kernel_size=kernel_size, strides=(stride, stride),
+    #     padding=padding, trainable=training,
+    #     name=name + '_dconv'
+    # )
+    p = kernel_size // 2
+    x = conv(data = x, num_filter = cnum, kernel=kernel_size, stride=stride, pad=p, dilation=1, name=name+"_deconv_conv")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_Sigmoid')
+    return data
+
+def getNoise(offset_h, offset_w, width, height):
+    noiseMask = np.random.rand(height, width)
+    if offset_h and offset_w:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    elif offset_h:
+        noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+    elif offset_w:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    return noiseMask1, noiseMask2
+
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    input_x, noise_s1, noise_s2 = caffe.layers.Slice(net.data, ntop = 3, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4,5]))
+    input_x = caffe.layers.Scale(input_x, bias_term = True, name = "input_x_Scale")
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+    x_2 = gen_gate_conv(x_1, 2 * cnum, 5, 2, name='conv2')
+    x_3 = gen_gate_conv(x_2, 4 * cnum, 3, 2, name='conv3')
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 3, 2, name='conv4')
+
+    x = resnetblock(x_4, 4 * cnum, name='block1')
+    x = resnetblock(x, 4 * cnum, name='block2')
+    x = resnetblock(x, 4 * cnum, name='block3')
+    x = resnetblock(x, 4 * cnum, name='block4')
+
+    x = gen_normal_deconv(x, 4*cnum, name='upsample1')
+    x = caffe.layers.Concat(x, x_3, axis = 1, name = 'concat1')
+    x = gen_gate_conv(x, 3*cnum, 3, 1, padding='same',name='conv5')
+
+    x = gen_normal_deconv(x, 3*cnum, name='upsample2')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'concat2')
+    x = gen_gate_conv(x, 2*cnum, 3, 1, padding='same',name='conv6')
+
+    x = gen_normal_deconv(x, 2*cnum, name='upsample3')
+    x = caffe.layers.Concat(x, x_1, axis=1, name='concat3')
+    x = gen_gate_conv(x, cnum, 3, 1, padding='same', name='conv7')
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat8')
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv8')
+    x = caffe.layers.relu(x)
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9')
+
+    #post
+    height = 512
+    width = height
+    # noise = caffe.io.load_image(caffe_root + "noise_512.jpg",  False)
+    # net.noise = caffe.layers.ImageData(name="noise",source="noise.txt", batch_size=1, new_width=512, new_height=512, 
+    #     ntop=1,is_color=True,root_folder='/inpainting/')
+    # print str(net.to_proto())
+    # noise1_m0, noise1_m1 = caffe.layers.Slice(noise, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[1]))
+    _x1, noise1 = caffe.layers.Slice(noise_s1, ntop=2, name='slice_noise1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    _x2, noise1 = caffe.layers.Slice(noise1, ntop=2, name='slice_noise1-2', slice_param=dict(slice_dim=3, slice_point=[1]))
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = "_noise1_Scale")
+    # noise1 = caffe.layers.Tile(noise1, name='noise1_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise1_c1,noise1_c2, noise1_c3 = caffe.layers.Split(noise1, ntop=3)
+    noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    # noise1_1 = caffe.layers.Tile(noise1_1, name='noise1_1_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise1_1_c1,noise1_1_c2,noise1_1_c3 = caffe.layers.Split(noise1_1, ntop=3)
+    noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)
+    _x3, noise2 = caffe.layers.Slice(noise_s2, ntop=2, name='slice_noisem1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    _x4, noise2 = caffe.layers.Slice(noise2, ntop=2, name='slice_noisem1-2', slice_param=dict(slice_dim=3, slice_point=[2]))
+    noise2_1 = caffe.layers.Scale(noise2, bias_term = True, name = "_noise2_Scale")
+    # noise2 = caffe.layers.Tile(noise2, name='noise2_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise2_c1,noise2_c2,noise2_c3 = caffe.layers.Split(noise2, ntop=3)
+    noise2 = caffe.layers.Concat(noise2, noise2, noise2)
+    # noise2_1 = caffe.layers.Tile(noise2_1, name='noise2_1_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise2_1_c1,noise2_1_c2,noise2_1_c3 = caffe.layers.Split(noise2_1, ntop=3)
+    noise2_1 = caffe.layers.Concat(noise2_1, noise2_1, noise2_1)
+
+    _x5, x1 = caffe.layers.Slice(x, ntop=2, name='slice1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    x1, _x6 = caffe.layers.Slice(x1, ntop=2, name='slice1-2', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # x1_c1, x1_c2, x1_c3 = caffe.layers.Slice(x1, ntop=3, name='slice1-3', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    # res1_c1 = caffe.layers.Eltwise(x1_c1, noise1, operation=0, name = "x1Multi")
+    # res1_c2 = caffe.layers.Eltwise(x1_c2, noise1, operation=0, name = "x1Multi")
+    # res1_c3 = caffe.layers.Eltwise(x1_c3, noise1, operation=0, name = "x1Multi")
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = "x1Multi")
+
+    x2, _x7 = caffe.layers.Slice(x, ntop=2, name='slice2-1', slice_param=dict(slice_dim=2, slice_point=[height-2]))
+    _x8, x2 = caffe.layers.Slice(x2, ntop=2, name='slice2-2', slice_param=dict(slice_dim=3, slice_point=[1]))
+    # x2_c1, x2_c2, x2_c3 = caffe.layers.Slice(x2, ntop=3, name='slice1-3', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    # res2_c1 = caffe.layers.Eltwise(x2_c1, noise1_1, operation=0, name = "x1Multi")
+    # res2_c2 = caffe.layers.Eltwise(x2_c2, noise1_1, operation=0, name = "x1Multi")
+    # res2_c3 = caffe.layers.Eltwise(x2_c3, noise1_1, operation=0, name = "x1Multi")
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name="x2Multi")
+    res = caffe.layers.Eltwise(res1, res2, name="x1Multix2")
+
+    _x9, x1 = caffe.layers.Slice(res, ntop=2, name='slice3', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res1 = caffe.layers.Eltwise(x1, noise2, operation=0, name="x1Multi_1")
+
+    x2, _x10 = caffe.layers.Slice(res, ntop=2, name='slice4', slice_param=dict(slice_dim=3, slice_point=[width-2]))
+    res2 = caffe.layers.Eltwise(x2, noise2_1, operation=0, name="x2Multi_1")
+
+    res = caffe.layers.Eltwise(res1, res2, name="x1Multix2_1")
+
+    # Gauss blur
+    dt_b, dt_g, dt_r = caffe.layers.Slice(res, ntop=3, name='slice5', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    dt_b = convs(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convB_1')
+    dt_b = convs(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convB_2')
+    dt_b = convs(data=dt_b, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_b')
+
+    dt_g = convs(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convG_1')
+    dt_g = convs(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convG_2')
+    dt_g = convs(data=dt_g, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_g')
+
+    dt_r = convs(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convR_1')
+    dt_r = convs(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convR_2')
+    dt_r = convs(data=dt_r, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_r')
+
+    x = caffe.layers.Concat(dt_b, dt_g, dt_r, axis=1, name = 'concat_rgb')
+
+    net.output = x
+    _x_2_512 = caffe.layers.Concat(_x1, _x3, _x5, _x7, axis=1, name='concat_2_512')
+    _x_510_1 = caffe.layers.Concat(_x2, _x6, _x8, _x9, _x10, axis=1, name='concat_510_1')
+    _x_2_512 = caffe.layers.Reshape(_x_2_512,name='reshape_2_512')
+    _x_510_1 = caffe.layers.Reshape(_x_510_1,name='reshape_510_1')
+    _x_510_2 = caffe.layers.Reshape(_x4,name='reshape_510_2')
+    _x = caffe.layers.Concat(_x_2_512,_x_510_1,_x_510_2, axis=1, name='concat_others')
+    net.output1 = _x
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test2-1.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: weights_tensor2caffe_inpainting200110.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting200110.py	(date 1578911536000)
+++ weights_tensor2caffe_inpainting200110.py	(date 1578911536000)
@@ -0,0 +1,377 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+def tensor2caffe(net,tensor):
+
+    ct_map={"conv1_conv_f:weights"                              :   "inpaint_net/conv1/conv2d/kernel" ,
+            "conv1_conv_f:bias"                                 :   "inpaint_net/conv1/conv2d/bias" ,
+            "conv1_conv_g_1:weights"                            :   "inpaint_net/conv1/conv2d_1/kernel" ,
+            "conv1_conv_g_1:bias"                               :   "inpaint_net/conv1/conv2d_1/bias" ,
+            "conv1_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv1/separable_conv2d/depthwise_kernel" ,
+            "conv1_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv1/separable_conv2d/pointwise_kernel" ,
+            "conv1_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv1/separable_conv2d/bias" ,
+            "conv1_conv_g_3:weights"                            :   "inpaint_net/conv1/conv2d_2/kernel" ,
+            "conv1_conv_g_3:bias"                               :   "inpaint_net/conv1/conv2d_2/bias" ,
+            "conv1_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv1/separable_conv2d_1/depthwise_kernel" ,
+            "conv1_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv1/separable_conv2d_1/pointwise_kernel" ,
+            "conv1_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv1/separable_conv2d_1/bias" ,
+            "conv1_conv_g:weights"                              :   "inpaint_net/conv1/conv2d_3/kernel" ,
+            "conv1_conv_g:bias"                                 :   "inpaint_net/conv1/conv2d_3/bias" ,
+            "conv2_conv_f:weights"                              :   "inpaint_net/conv2/conv2d/kernel" ,
+            "conv2_conv_f:bias"                                 :   "inpaint_net/conv2/conv2d/bias" ,
+            "conv2_conv_g_1:weights"                            :   "inpaint_net/conv2/conv2d_1/kernel" ,
+            "conv2_conv_g_1:bias"                               :   "inpaint_net/conv2/conv2d_1/bias" ,
+            "conv2_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv2/separable_conv2d/depthwise_kernel" ,
+            "conv2_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv2/separable_conv2d/pointwise_kernel" ,
+            "conv2_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv2/separable_conv2d/bias" ,
+            "conv2_conv_g_3:weights"                            :   "inpaint_net/conv2/conv2d_2/kernel" ,
+            "conv2_conv_g_3:bias"                               :   "inpaint_net/conv2/conv2d_2/bias" ,
+            "conv2_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv2/separable_conv2d_1/depthwise_kernel" ,
+            "conv2_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv2/separable_conv2d_1/pointwise_kernel" ,
+            "conv2_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv2/separable_conv2d_1/bias" ,
+            "conv2_conv_g:weights"                              :   "inpaint_net/conv2/conv2d_3/kernel" ,
+            "conv2_conv_g:bias"                                 :   "inpaint_net/conv2/conv2d_3/bias" ,
+            "conv3_conv_f:weights"                              :   "inpaint_net/conv3/conv2d/kernel" ,
+            "conv3_conv_f:bias"                                 :   "inpaint_net/conv3/conv2d/bias" ,
+            "conv3_conv_g_1:weights"                            :   "inpaint_net/conv3/conv2d_1/kernel" ,
+            "conv3_conv_g_1:bias"                               :   "inpaint_net/conv3/conv2d_1/bias" ,
+            "conv3_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv3/separable_conv2d/depthwise_kernel" ,
+            "conv3_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv3/separable_conv2d/pointwise_kernel" ,
+            "conv3_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv3/separable_conv2d/bias" ,
+            "conv3_conv_g_3:weights"                            :   "inpaint_net/conv3/conv2d_2/kernel" ,
+            "conv3_conv_g_3:bias"                               :   "inpaint_net/conv3/conv2d_2/bias" ,
+            "conv3_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv3/separable_conv2d_1/depthwise_kernel" ,
+            "conv3_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv3/separable_conv2d_1/pointwise_kernel" ,
+            "conv3_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv3/separable_conv2d_1/bias" ,
+            "conv3_conv_g:weights"                              :   "inpaint_net/conv3/conv2d_3/kernel" ,
+            "conv3_conv_g:bias"                                 :   "inpaint_net/conv3/conv2d_3/bias" ,
+            "conv4_conv_f:weights"                              :   "inpaint_net/conv4/conv2d/kernel" ,
+            "conv4_conv_f:bias"                                 :   "inpaint_net/conv4/conv2d/bias" ,
+            "conv4_conv_g_1:weights"                            :   "inpaint_net/conv4/conv2d_1/kernel" ,
+            "conv4_conv_g_1:bias"                               :   "inpaint_net/conv4/conv2d_1/bias" ,
+            "conv4_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv4/separable_conv2d/depthwise_kernel" ,
+            "conv4_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv4/separable_conv2d/pointwise_kernel" ,
+            "conv4_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv4/separable_conv2d/bias" ,
+            "conv4_conv_g_3:weights"                            :   "inpaint_net/conv4/conv2d_2/kernel" ,
+            "conv4_conv_g_3:bias"                               :   "inpaint_net/conv4/conv2d_2/bias" ,
+            "conv4_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv4/separable_conv2d_1/depthwise_kernel" ,
+            "conv4_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv4/separable_conv2d_1/pointwise_kernel" ,
+            "conv4_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv4/separable_conv2d_1/bias" ,
+            "conv4_conv_g:weights"                              :   "inpaint_net/conv4/conv2d_3/kernel" ,
+            "conv4_conv_g:bias"                                 :   "inpaint_net/conv4/conv2d_3/bias" ,
+            "upsample1_deconv:weights"                          :   "inpaint_net/upsample1/conv2d_transpose/kernel" ,
+            "upsample1_deconv:bias"                             :   "inpaint_net/upsample1/conv2d_transpose/bias" ,
+            "upsample1_conv1:weights"                           :   "inpaint_net/up_conv1/kernel" ,
+            "upsample1_conv1:bias"                              :   "inpaint_net/up_conv1/bias" ,
+            "upsample2_deconv:weights"                          :   "inpaint_net/upsample2/conv2d_transpose/kernel" ,
+            "upsample2_deconv:bias"                             :   "inpaint_net/upsample2/conv2d_transpose/bias" ,
+            "up_conv2_conv_f:weights"                           :   "inpaint_net/up_conv2/conv2d/kernel" ,
+            "up_conv2_conv_f:bias"                              :   "inpaint_net/up_conv2/conv2d/bias" ,
+            "up_conv2_conv_g_1:weights"                         :   "inpaint_net/up_conv2/conv2d_1/kernel" ,
+            "up_conv2_conv_g_1:bias"                            :   "inpaint_net/up_conv2/conv2d_1/bias" ,
+            "up_conv2_separable_conv_g_2_depthwise:weights"     :   "inpaint_net/up_conv2/separable_conv2d/depthwise_kernel" ,
+            "up_conv2_separable_conv_g_2_pointwise:weights"     :   "inpaint_net/up_conv2/separable_conv2d/pointwise_kernel" ,
+            "up_conv2_separable_conv_g_2_pointwise:bias"        :   "inpaint_net/up_conv2/separable_conv2d/bias" ,
+            "up_conv2_conv_g_3:weights"                         :   "inpaint_net/up_conv2/conv2d_2/kernel" ,
+            "up_conv2_conv_g_3:bias"                            :   "inpaint_net/up_conv2/conv2d_2/bias" ,
+            "up_conv2_separable_conv_g1_1_depthwise:weights"    :   "inpaint_net/up_conv2/separable_conv2d_1/depthwise_kernel" ,
+            "up_conv2_separable_conv_g1_1_pointwise:weights"    :   "inpaint_net/up_conv2/separable_conv2d_1/pointwise_kernel" ,
+            "up_conv2_separable_conv_g1_1_pointwise:bias"       :   "inpaint_net/up_conv2/separable_conv2d_1/bias" ,
+            "up_conv2_conv_g:weights"                           :   "inpaint_net/up_conv2/conv2d_3/kernel" ,
+            "up_conv2_conv_g:bias"                              :   "inpaint_net/up_conv2/conv2d_3/bias" ,
+            "upsample3_deconv:weights"                          :   "inpaint_net/upsample3/conv2d_transpose/kernel" ,
+            "upsample3_deconv:bias"                             :   "inpaint_net/upsample3/conv2d_transpose/bias" ,
+            "upsample3_conv_conv_f:weights"                     :   "inpaint_net/conv8/conv2d/kernel" ,
+            "upsample3_conv_conv_f:bias"                        :   "inpaint_net/conv8/conv2d/bias" ,
+            "upsample3_conv_conv_g_1:weights"                       :   "inpaint_net/conv8/conv2d_1/kernel" ,
+            "upsample3_conv_conv_g_1:bias"                          :   "inpaint_net/conv8/conv2d_1/bias" ,
+            "upsample3_conv_separable_conv_g_2_depthwise:weights"   :   "inpaint_net/conv8/separable_conv2d/depthwise_kernel" ,
+            "upsample3_conv_separable_conv_g_2_pointwise:weights"   :   "inpaint_net/conv8/separable_conv2d/pointwise_kernel" ,
+            "upsample3_conv_separable_conv_g_2_pointwise:bias"      :   "inpaint_net/conv8/separable_conv2d/bias" ,
+            "upsample3_conv_conv_g_3:weights"                       :   "inpaint_net/conv8/conv2d_2/kernel" ,
+            "upsample3_conv_conv_g_3:bias"                          :   "inpaint_net/conv8/conv2d_2/bias" ,
+            "upsample3_conv_separable_conv_g1_1_depthwise:weights"  :   "inpaint_net/conv8/separable_conv2d_1/depthwise_kernel" ,
+            "upsample3_conv_separable_conv_g1_1_pointwise:weights"  :   "inpaint_net/conv8/separable_conv2d_1/pointwise_kernel" ,
+            "upsample3_conv_separable_conv_g1_1_pointwise:bias"     :   "inpaint_net/conv8/separable_conv2d_1/bias" ,
+            "upsample3_conv_conv_g:weights"                         :   "inpaint_net/conv8/conv2d_3/kernel" ,
+            "upsample3_conv_conv_g:bias"                            :   "inpaint_net/conv8/conv2d_3/bias" ,
+            "upsample4_deconv:weights"                              :   "inpaint_net/upsample4/conv2d_transpose/kernel" ,
+            "upsample4_deconv:bias"                                 :   "inpaint_net/upsample4/conv2d_transpose/bias" ,
+            "conv10:weights"                                        :   "inpaint_net/conv10/kernel" ,
+            "conv10:bias"                                           :   "inpaint_net/conv10/bias" ,
+      }
+    
+    num=0
+    net_keys=net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s'%(i, len(net_keys), le, net_keys[i]))
+        # if(net_keys[i] == 'upsample1_deconv'): continue
+        # if(net_keys[i] == 'upsample2_deconv'): continue
+        # if(net_keys[i] == 'upsample3_deconv'): continue
+        # if(net_keys[i] == 'upsample4_deconv'): continue
+        # if(net_keys[i] == 'upsample5_deconv'): continue
+        if le==1:
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value=weights_value.transpose((3,2,0,1)) 
+            else:
+                key0=net_keys[i]+':weights'
+                if "pad" in key0: # for padding, no operation
+                    # print(net.params[net_keys[i]][0].data.shape)
+                    print("pad layer")
+                    net.params[net_keys[i]][0].data[:,:,:,:] = np.array([1.])
+                    continue
+                value0=ct_map[key0]
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1)) 
+            try:
+                net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+            except:
+                net.params[net_keys[i]][0].data[:,:,:,:]=0
+                net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+            
+            
+            print(weights_value.shape,end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+            
+        elif le==2:
+            if "BatchNorm" in net_keys[i]:
+                print("le_last = %d"%le_last)
+                # for BN layers
+                key0=net_keys[i]+':gamma'
+                value0 = ct_map[key0]
+                gamma_value=tensor[value0] 
+
+                key1=net_keys[i]+':beta'
+                value1=ct_map[key1]
+                beta_value=tensor[value1]
+
+                value2 = value0.replace('gamma', 'moving_variance')
+                mv_value=tensor[value2] 
+                print(mv_value)
+
+                value3 = value1.replace('beta', 'moving_mean')
+                mm_value=tensor[value3] 
+                print(mm_value)
+
+                net.params[net_keys[i]][0].data[:] = gamma_value / np.sqrt(mv_value)
+                print(gamma_value.shape,end=' ')            
+                print(net.params[net_keys[i]][0].data.shape)
+                print(gamma_value)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+        
+                net.params[net_keys[i]][1].data[:] = beta_value - mm_value/np.sqrt(mv_value)*gamma_value
+                print(beta_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print(beta_value)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+
+                # key0=net_keys[i-1]+':moving_variance'
+                # value0 = ct_map[key0]
+                # mv_value=tensor[value0] 
+
+                # key1=net_keys[i-1]+':moving_mean'
+                # value1=ct_map[key1]
+                # mm_value=tensor[value1]
+
+                # # mm_value=np.full(mm_value.shape,0)
+                # net.params[net_keys[i-1]][0].data[:] = mm_value
+                # print(mm_value.shape,end=' ')
+                # print(mm_value)
+                # print(net.params[net_keys[i-1]][0].data.shape)
+                # print("%s  <-  %s"%(key0,value0))
+                # num=num+1
+
+                # # mv_value=np.full(mv_value.shape,1)
+                # net.params[net_keys[i-1]][1].data[:] = mv_value
+                # print(mv_value.shape,end=' ')
+                # print(mv_value)
+                # print(net.params[net_keys[i-1]][1].data.shape)
+                # print("%s  <-  %s"%(key1,value1))
+                # num=num+1
+
+                # net.params[net_keys[i-1]][2].data[:] = 1
+                # print(net.params[net_keys[i-1]][2].data.shape)
+                # print("bn  <-  1")
+                # num=num+1
+
+            elif "inverse" in net_keys[i]: # 1 - noise
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+                print("inverse layer")
+                print(net.params[net_keys[i]][0].data[:])
+                print(net.params[net_keys[i]][1].data[:])
+            else:   
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1))
+                    
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value[:,:,:,:]
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value[:,:,:,:]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+#                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+#                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':bias'
+                value1=ct_map[key1]
+                
+                bias_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=bias_value
+                print(bias_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+        # elif le==3:
+        #         key0=net_keys[i]+':moving_variance'
+        #         value0 = ct_map[key0]
+        #         mv_value=tensor[value0]  
+
+        #         key1=net_keys[i]+':moving_mean'
+        #         value1=ct_map[key1]
+        #         mm_value=tensor[value1]
+
+        #         mm_value=np.full(mm_value.shape,0)
+        #         net.params[net_keys[i]][0].data[:] = mm_value
+        #         print(mm_value.shape,end=' ')
+        #         print(mm_value)
+        #         print(net.params[net_keys[i]][0].data.shape)
+        #         print("%s  <-  %s"%(key1,value1))
+        #         num=num+1
+
+        #         mv_value=np.full(mv_value.shape,1)
+        #         net.params[net_keys[i]][1].data[:] = mv_value
+        #         print(mv_value.shape,end=' ')
+        #         print(mv_value)
+        #         print(net.params[net_keys[i]][1].data.shape)
+        #         print("%s  <-  %s"%(key0,value0))
+        #         num=num+1
+
+        #         net.params[net_keys[i]][2].data[:] = 1
+        #         print(net.params[net_keys[i]][2].data.shape)
+        #         num=num+1
+
+        else:
+            print("error: le=%d %s"%(le, net_keys[i]))
+        le_last = le
+    print(num)
+    print('net_keys[0]',net_keys[0])
+    net.params[net_keys[0]][0].data[:] = np.array([1./128, 1./128, 1./128, 1./255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0.])
+
+
+
+def print_caffemodel(net):
+    net_keys=net.params.keys()
+    le_last = 0
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le==1:
+            n,c,w,h=net.params[net_keys[i]][0].data.shape
+            print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+            	if 'inverse' in net_keys[i]:
+            	 	w=net.params[net_keys[i]][0].data.shape
+            	 	print("%s:weights ->  [%d]"%(net_keys[i], w[0]))
+            	 	w=net.params[net_keys[i]][1].data.shape
+            	 	print("%s:bias  ->  [%d]"%(net_keys[i], w[0]))
+            	else:
+            	 	# BatchNorm layer后面的scale layer, 训练gamma和deta
+            	 	w=net.params[net_keys[i]][0].data.shape
+            	 	print("%s:gamma ->  [%d]"%(net_keys[i], w[0]))
+            	 	w=net.params[net_keys[i]][1].data.shape
+            	 	print("%s:beta  ->  [%d]"%(net_keys[i], w[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+            
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]"%(net_keys[i],w[0]))
+        elif le==3:
+            #BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]"%(net_keys[i],m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]"%(net_keys[i],v[0]))
+
+            #f = net.params[net_keys[i]][2].data.shape
+            #print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys=tensor.keys()
+    for i in range(0,len(tensor_keys)):
+        le=len(tensor[tensor_keys[i]].shape)
+        if le==1:
+            w=tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]"%(tensor_keys[i],w))
+        elif le==4:
+            weights=tensor[tensor_keys[i]]
+            #if "filter"  in tensor_keys[i]:
+                #weights=tensor[tensor_keys[i]].transpose((2,3,1,0)) 
+            #else:
+                #weights=tensor[tensor_keys[i]].transpose((3,2,1,0)) 
+            n,c,w,h=weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]"%(tensor_keys[i],n,c,w,h))
+
+#print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net('/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test11_5c_test_LRE_woBN_1.prototxt', caffe.TEST)
+    print_caffemodel(net)
+    print("########################################################################################################################")
+    tensor=np.load('inpainting/test11-5c-LRE-0113.npy').item()
+    print_tensorflow(tensor)
+    print("########################################################################################################################")
+    tensor2caffe(net,tensor)
+    net.save("inpainting/test11-5c-LRE-0113.npy.caffemodel")
Index: weights_tensor2caffe_inpainting200113.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting200113.py	(date 1578882436000)
+++ weights_tensor2caffe_inpainting200113.py	(date 1578882436000)
@@ -0,0 +1,381 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+# caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/'
+# sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+
+def tensor2caffe(net, tensor):
+    ct_map = {"conv1_conv_f:weights": "inpaint_net/conv1/conv2d/kernel",
+              "conv1_conv_f:bias": "inpaint_net/conv1/conv2d/bias",
+              "conv1_conv_g_1:weights": "inpaint_net/conv1/conv2d_1/kernel",
+              "conv1_conv_g_1:bias": "inpaint_net/conv1/conv2d_1/bias",
+              "conv1_separable_conv_g_2_depthwise:weights": "inpaint_net/conv1/separable_conv2d/depthwise_kernel",
+              "conv1_separable_conv_g_2_pointwise:weights": "inpaint_net/conv1/separable_conv2d/pointwise_kernel",
+              "conv1_separable_conv_g_2_pointwise:bias": "inpaint_net/conv1/separable_conv2d/bias",
+              "conv1_conv_g_3:weights": "inpaint_net/conv1/conv2d_2/kernel",
+              "conv1_conv_g_3:bias": "inpaint_net/conv1/conv2d_2/bias",
+              "conv1_separable_conv_g1_1_depthwise:weights": "inpaint_net/conv1/separable_conv2d_1/depthwise_kernel",
+              "conv1_separable_conv_g1_1_pointwise:weights": "inpaint_net/conv1/separable_conv2d_1/pointwise_kernel",
+              "conv1_separable_conv_g1_1_pointwise:bias": "inpaint_net/conv1/separable_conv2d_1/bias",
+              "conv1_conv_g:weights": "inpaint_net/conv1/conv2d_3/kernel",
+              "conv1_conv_g:bias": "inpaint_net/conv1/conv2d_3/bias",
+              "conv2_conv_f:weights": "inpaint_net/conv2/conv2d/kernel",
+              "conv2_conv_f:bias": "inpaint_net/conv2/conv2d/bias",
+              "conv2_conv_g_1:weights": "inpaint_net/conv2/conv2d_1/kernel",
+              "conv2_conv_g_1:bias": "inpaint_net/conv2/conv2d_1/bias",
+              "conv2_separable_conv_g_2_depthwise:weights": "inpaint_net/conv2/separable_conv2d/depthwise_kernel",
+              "conv2_separable_conv_g_2_pointwise:weights": "inpaint_net/conv2/separable_conv2d/pointwise_kernel",
+              "conv2_separable_conv_g_2_pointwise:bias": "inpaint_net/conv2/separable_conv2d/bias",
+              "conv2_conv_g_3:weights": "inpaint_net/conv2/conv2d_2/kernel",
+              "conv2_conv_g_3:bias": "inpaint_net/conv2/conv2d_2/bias",
+              "conv2_separable_conv_g1_1_depthwise:weights": "inpaint_net/conv2/separable_conv2d_1/depthwise_kernel",
+              "conv2_separable_conv_g1_1_pointwise:weights": "inpaint_net/conv2/separable_conv2d_1/pointwise_kernel",
+              "conv2_separable_conv_g1_1_pointwise:bias": "inpaint_net/conv2/separable_conv2d_1/bias",
+              "conv2_conv_g:weights": "inpaint_net/conv2/conv2d_3/kernel",
+              "conv2_conv_g:bias": "inpaint_net/conv2/conv2d_3/bias",
+              "conv3_conv_f:weights": "inpaint_net/conv3/conv2d/kernel",
+              "conv3_conv_f:bias": "inpaint_net/conv3/conv2d/bias",
+              "conv3_conv_g_1:weights": "inpaint_net/conv3/conv2d_1/kernel",
+              "conv3_conv_g_1:bias": "inpaint_net/conv3/conv2d_1/bias",
+              "conv3_separable_conv_g_2_depthwise:weights": "inpaint_net/conv3/separable_conv2d/depthwise_kernel",
+              "conv3_separable_conv_g_2_pointwise:weights": "inpaint_net/conv3/separable_conv2d/pointwise_kernel",
+              "conv3_separable_conv_g_2_pointwise:bias": "inpaint_net/conv3/separable_conv2d/bias",
+              "conv3_conv_g_3:weights": "inpaint_net/conv3/conv2d_2/kernel",
+              "conv3_conv_g_3:bias": "inpaint_net/conv3/conv2d_2/bias",
+              "conv3_separable_conv_g1_1_depthwise:weights": "inpaint_net/conv3/separable_conv2d_1/depthwise_kernel",
+              "conv3_separable_conv_g1_1_pointwise:weights": "inpaint_net/conv3/separable_conv2d_1/pointwise_kernel",
+              "conv3_separable_conv_g1_1_pointwise:bias": "inpaint_net/conv3/separable_conv2d_1/bias",
+              "conv3_conv_g:weights": "inpaint_net/conv3/conv2d_3/kernel",
+              "conv3_conv_g:bias": "inpaint_net/conv3/conv2d_3/bias",
+              "conv4_conv_f:weights": "inpaint_net/conv4/conv2d/kernel",
+              "conv4_conv_f:bias": "inpaint_net/conv4/conv2d/bias",
+              "conv4_conv_g_1:weights": "inpaint_net/conv4/conv2d_1/kernel",
+              "conv4_conv_g_1:bias": "inpaint_net/conv4/conv2d_1/bias",
+              "conv4_separable_conv_g_2_depthwise:weights": "inpaint_net/conv4/separable_conv2d/depthwise_kernel",
+              "conv4_separable_conv_g_2_pointwise:weights": "inpaint_net/conv4/separable_conv2d/pointwise_kernel",
+              "conv4_separable_conv_g_2_pointwise:bias": "inpaint_net/conv4/separable_conv2d/bias",
+              "conv4_conv_g_3:weights": "inpaint_net/conv4/conv2d_2/kernel",
+              "conv4_conv_g_3:bias": "inpaint_net/conv4/conv2d_2/bias",
+              "conv4_separable_conv_g1_1_depthwise:weights": "inpaint_net/conv4/separable_conv2d_1/depthwise_kernel",
+              "conv4_separable_conv_g1_1_pointwise:weights": "inpaint_net/conv4/separable_conv2d_1/pointwise_kernel",
+              "conv4_separable_conv_g1_1_pointwise:bias": "inpaint_net/conv4/separable_conv2d_1/bias",
+              "conv4_conv_g:weights": "inpaint_net/conv4/conv2d_3/kernel",
+              "conv4_conv_g:bias": "inpaint_net/conv4/conv2d_3/bias",
+              "upsample1_deconv:weights": "inpaint_net/upsample1/conv2d_transpose/kernel",
+              "upsample1_deconv:bias": "inpaint_net/upsample1/conv2d_transpose/bias",
+              "upsample1_conv1:weights": "inpaint_net/up_conv1/kernel",
+              "upsample1_conv1:bias": "inpaint_net/up_conv1/bias",
+              "upsample2_deconv:weights": "inpaint_net/upsample2/conv2d_transpose/kernel",
+              "upsample2_deconv:bias": "inpaint_net/upsample2/conv2d_transpose/bias",
+              "up_conv2_conv_f:weights": "inpaint_net/up_conv2/conv2d/kernel",
+              "up_conv2_conv_f:bias": "inpaint_net/up_conv2/conv2d/bias",
+              "up_conv2_conv_g_1:weights": "inpaint_net/up_conv2/conv2d_1/kernel",
+              "up_conv2_conv_g_1:bias": "inpaint_net/up_conv2/conv2d_1/bias",
+              "up_conv2_separable_conv_g_2_depthwise:weights": "inpaint_net/up_conv2/separable_conv2d/depthwise_kernel",
+              "up_conv2_separable_conv_g_2_pointwise:weights": "inpaint_net/up_conv2/separable_conv2d/pointwise_kernel",
+              "up_conv2_separable_conv_g_2_pointwise:bias": "inpaint_net/up_conv2/separable_conv2d/bias",
+              "up_conv2_conv_g_3:weights": "inpaint_net/up_conv2/conv2d_2/kernel",
+              "up_conv2_conv_g_3:bias": "inpaint_net/up_conv2/conv2d_2/bias",
+              "up_conv2_separable_conv_g1_1_depthwise:weights": "inpaint_net/up_conv2/separable_conv2d_1/depthwise_kernel",
+              "up_conv2_separable_conv_g1_1_pointwise:weights": "inpaint_net/up_conv2/separable_conv2d_1/pointwise_kernel",
+              "up_conv2_separable_conv_g1_1_pointwise:bias": "inpaint_net/up_conv2/separable_conv2d_1/bias",
+              "up_conv2_conv_g:weights": "inpaint_net/up_conv2/conv2d_3/kernel",
+              "up_conv2_conv_g:bias": "inpaint_net/up_conv2/conv2d_3/bias",
+              "upsample3_deconv:weights": "inpaint_net/upsample3/conv2d_transpose/kernel",
+              "upsample3_deconv:bias": "inpaint_net/upsample3/conv2d_transpose/bias",
+              "upsample3_conv_conv_f:weights": "inpaint_net/conv8/conv2d/kernel",
+              "upsample3_conv_conv_f:bias": "inpaint_net/conv8/conv2d/bias",
+              "upsample3_conv_conv_g_1:weights": "inpaint_net/conv8/conv2d_1/kernel",
+              "upsample3_conv_conv_g_1:bias": "inpaint_net/conv8/conv2d_1/bias",
+              "upsample3_conv_separable_conv_g_2_depthwise:weights": "inpaint_net/conv8/separable_conv2d/depthwise_kernel",
+              "upsample3_conv_separable_conv_g_2_pointwise:weights": "inpaint_net/conv8/separable_conv2d/pointwise_kernel",
+              "upsample3_conv_separable_conv_g_2_pointwise:bias": "inpaint_net/conv8/separable_conv2d/bias",
+              "upsample3_conv_conv_g_3:weights": "inpaint_net/conv8/conv2d_2/kernel",
+              "upsample3_conv_conv_g_3:bias": "inpaint_net/conv8/conv2d_2/bias",
+              "upsample3_conv_separable_conv_g1_1_depthwise:weights": "inpaint_net/conv8/separable_conv2d_1/depthwise_kernel",
+              "upsample3_conv_separable_conv_g1_1_pointwise:weights": "inpaint_net/conv8/separable_conv2d_1/pointwise_kernel",
+              "upsample3_conv_separable_conv_g1_1_pointwise:bias": "inpaint_net/conv8/separable_conv2d_1/bias",
+              "upsample3_conv_conv_g:weights": "inpaint_net/conv8/conv2d_3/kernel",
+              "upsample3_conv_conv_g:bias": "inpaint_net/conv8/conv2d_3/bias",
+              "upsample4_deconv:weights": "inpaint_net/upsample4/conv2d_transpose/kernel",
+              "upsample4_deconv:bias": "inpaint_net/upsample4/conv2d_transpose/bias",
+              "conv10:weights": "inpaint_net/conv10/kernel",
+              "conv10:bias": "inpaint_net/conv10/bias",
+              }
+
+    num = 0
+    net_keys = net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1, len(net_keys)):
+        le = len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s' % (i, len(net_keys), le, net_keys[i]))
+        # if(net_keys[i] == 'upsample1_deconv'): continue
+        # if(net_keys[i] == 'upsample2_deconv'): continue
+        # if(net_keys[i] == 'upsample3_deconv'): continue
+        # if(net_keys[i] == 'upsample4_deconv'): continue
+        # if(net_keys[i] == 'upsample5_deconv'): continue
+        if le == 1:
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value = weights_value.transpose((3, 2, 0, 1))
+            else:
+                key0 = net_keys[i] + ':weights'
+                if "pad" in key0:  # for padding, no operation
+                    # print(net.params[net_keys[i]][0].data.shape)
+                    print("pad layer")
+                    net.params[net_keys[i]][0].data[:, :, :, :] = np.array([1.])
+                    continue
+                value0 = ct_map[key0]
+                weights_value = tensor[value0]
+                if "depthwise" in value0:
+                    weights_value = tensor[value0].transpose((2, 3, 0, 1))
+                else:
+                    weights_value = tensor[value0].transpose((3, 2, 0, 1))
+            try:
+                net.params[net_keys[i]][0].data[:, :, :, :] = weights_value
+            except:
+                net.params[net_keys[i]][0].data[:, :, :, :] = 0
+                net.params[net_keys[i]][0].data[:, :, 1:, 1:] = weights_value
+
+            print(weights_value.shape, end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s" % (key0, value0))
+            num = num + 1
+
+        elif le == 2:
+            if "BatchNorm" in net_keys[i]:
+                print("le_last = %d" % le_last)
+                # for BN layers
+                key0 = net_keys[i] + ':gamma'
+                value0 = ct_map[key0]
+                gamma_value = tensor[value0]
+
+                key1 = net_keys[i] + ':beta'
+                value1 = ct_map[key1]
+                beta_value = tensor[value1]
+
+                value2 = value0.replace('gamma', 'moving_variance')
+                mv_value = tensor[value2]
+                print(mv_value)
+
+                value3 = value1.replace('beta', 'moving_mean')
+                mm_value = tensor[value3]
+                print(mm_value)
+
+                net.params[net_keys[i]][0].data[:] = gamma_value / np.sqrt(mv_value)
+                print(gamma_value.shape, end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print(gamma_value)
+                print("%s  <-  %s" % (key0, value0))
+                num = num + 1
+
+                net.params[net_keys[i]][1].data[:] = beta_value - mm_value / np.sqrt(mv_value) * gamma_value
+                print(beta_value.shape, end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print(beta_value)
+                print("%s  <-  %s" % (key1, value1))
+                num = num + 1
+
+                # key0=net_keys[i-1]+':moving_variance'
+                # value0 = ct_map[key0]
+                # mv_value=tensor[value0]
+
+                # key1=net_keys[i-1]+':moving_mean'
+                # value1=ct_map[key1]
+                # mm_value=tensor[value1]
+
+                # # mm_value=np.full(mm_value.shape,0)
+                # net.params[net_keys[i-1]][0].data[:] = mm_value
+                # print(mm_value.shape,end=' ')
+                # print(mm_value)
+                # print(net.params[net_keys[i-1]][0].data.shape)
+                # print("%s  <-  %s"%(key0,value0))
+                # num=num+1
+
+                # # mv_value=np.full(mv_value.shape,1)
+                # net.params[net_keys[i-1]][1].data[:] = mv_value
+                # print(mv_value.shape,end=' ')
+                # print(mv_value)
+                # print(net.params[net_keys[i-1]][1].data.shape)
+                # print("%s  <-  %s"%(key1,value1))
+                # num=num+1
+
+                # net.params[net_keys[i-1]][2].data[:] = 1
+                # print(net.params[net_keys[i-1]][2].data.shape)
+                # print("bn  <-  1")
+                # num=num+1
+
+            elif "inverse" in net_keys[i]:  # 1 - noise
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+                print("inverse layer")
+                print(net.params[net_keys[i]][0].data[:])
+                print(net.params[net_keys[i]][1].data[:])
+            else:
+                key0 = net_keys[i] + ':weights'
+                value0 = ct_map[key0]
+
+                weights_value = tensor[value0]
+                if "depthwise" in value0:
+                    weights_value = tensor[value0].transpose((2, 3, 0, 1))
+                else:
+                    weights_value = tensor[value0].transpose((3, 2, 0, 1))
+
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:, :, :, :] = weights_value[:, :, :, :]
+                    except:
+                        net.params[net_keys[i]][0].data[:, :, :, :] = 0
+                        net.params[net_keys[i]][0].data[:, :, 1:, 1:] = weights_value[:, :, :, :]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:, :, :, :] = weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:, :, :, :] = 0
+                        net.params[net_keys[i]][0].data[:, :, 1:, 1:] = weights_value
+                #                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+                #                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape, end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s" % (key0, value0))
+                num = num + 1
+
+                key1 = net_keys[i] + ':bias'
+                value1 = ct_map[key1]
+
+                bias_value = tensor[value1]
+                net.params[net_keys[i]][1].data[:] = bias_value
+                print(bias_value.shape, end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s" % (key1, value1))
+                num = num + 1
+        # elif le==3:
+        #         key0=net_keys[i]+':moving_variance'
+        #         value0 = ct_map[key0]
+        #         mv_value=tensor[value0]
+
+        #         key1=net_keys[i]+':moving_mean'
+        #         value1=ct_map[key1]
+        #         mm_value=tensor[value1]
+
+        #         mm_value=np.full(mm_value.shape,0)
+        #         net.params[net_keys[i]][0].data[:] = mm_value
+        #         print(mm_value.shape,end=' ')
+        #         print(mm_value)
+        #         print(net.params[net_keys[i]][0].data.shape)
+        #         print("%s  <-  %s"%(key1,value1))
+        #         num=num+1
+
+        #         mv_value=np.full(mv_value.shape,1)
+        #         net.params[net_keys[i]][1].data[:] = mv_value
+        #         print(mv_value.shape,end=' ')
+        #         print(mv_value)
+        #         print(net.params[net_keys[i]][1].data.shape)
+        #         print("%s  <-  %s"%(key0,value0))
+        #         num=num+1
+
+        #         net.params[net_keys[i]][2].data[:] = 1
+        #         print(net.params[net_keys[i]][2].data.shape)
+        #         num=num+1
+
+        else:
+            print("error: le=%d %s" % (le, net_keys[i]))
+        le_last = le
+    print(num)
+    print('net_keys[0]', net_keys[0])
+    net.params[net_keys[0]][0].data[:] = np.array([1. / 128, 1. / 128, 1. / 128, 1. / 255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0.])
+
+
+def print_caffemodel(net):
+    net_keys = net.params.keys()
+    le_last = 0
+    for i in range(1, len(net_keys)):
+        le = len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le == 1:
+            n, c, w, h = net.params[net_keys[i]][0].data.shape
+            print("%s:weights  ->  [%d,%d,%d,%d]" % (net_keys[i], n, c, w, h))
+        elif le == 2:
+            if '_Sc' in net_keys[i]:
+                if 'inverse' in net_keys[i]:
+                    w = net.params[net_keys[i]][0].data.shape
+                    print("%s:weights ->  [%d]" % (net_keys[i], w[0]))
+                    w = net.params[net_keys[i]][1].data.shape
+                    print("%s:bias  ->  [%d]" % (net_keys[i], w[0]))
+                else:
+                    # BatchNorm layer后面的scale layer, 训练gamma和deta
+                    w = net.params[net_keys[i]][0].data.shape
+                    print("%s:gamma ->  [%d]" % (net_keys[i], w[0]))
+                    w = net.params[net_keys[i]][1].data.shape
+                    print("%s:beta  ->  [%d]" % (net_keys[i], w[0]))
+            else:
+                n, c, w, h = net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]" % (net_keys[i], n, c, w, h))
+
+                w = net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]" % (net_keys[i], w[0]))
+        elif le == 3:
+            # BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]" % (net_keys[i], m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]" % (net_keys[i], v[0]))
+
+            # f = net.params[net_keys[i]][2].data.shape
+            # print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s" % (net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys = tensor.keys()
+    for i in range(0, len(tensor_keys)):
+        le = len(tensor[tensor_keys[i]].shape)
+        if le == 1:
+            w = tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]" % (tensor_keys[i], w))
+        elif le == 4:
+            weights = tensor[tensor_keys[i]]
+            # if "filter"  in tensor_keys[i]:
+            # weights=tensor[tensor_keys[i]].transpose((2,3,1,0))
+            # else:
+            # weights=tensor[tensor_keys[i]].transpose((3,2,1,0))
+            n, c, w, h = weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]" % (tensor_keys[i], n, c, w, h))
+
+
+# print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net(
+        '/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test11_4c_without_BN_LER.prototxt',
+        caffe.TEST)
+    print_caffemodel(net)
+    print(
+        "########################################################################################################################")
+    tensor = np.load('inpainting/test11-0113.npy').item()
+    print_tensorflow(tensor)
+    print(
+        "########################################################################################################################")
+    tensor2caffe(net, tensor)
+    net.save("inpainting/test11_4c_without_BN_LER.caffemodel")
Index: SaveProtxt_inpainting200113.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting200113.py	(date 1578877298000)
+++ SaveProtxt_inpainting200113.py	(date 1578877298000)
@@ -0,0 +1,267 @@
+# -*- coding: UTF-8 -*-
+import caffe
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1), depthwise不能改变通道数，所以需要再来一个1*1卷积
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name, input_channel = 0):
+    if input_channel == 0:
+        input_channel = num_filter
+    convs_depthwise = convs(data=data, num_filter=input_channel, kernel=kernel, stride=stride, pad=pad, group=input_channel, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+
+def gen_gate_conv(x, inputChannel, cnum, ksize, stride=1, rate=1, name='gen_gate_conv'):
+    if stride == 1:
+        padding = ksize // 2
+        input1 = x
+        ######## feature #########
+        conv_f = conv(data=x, num_filter=cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate,
+                      name=name + '_conv_f')
+        out_f = leakyReLU(conv_f, name=name + "_conv_f_ReLU")
+
+        ######## Soft mask gating  #############
+        conv_g_1 = conv(data=x, num_filter=cnum, kernel=1, stride=1, pad=0, dilation=1, name=name + "_conv_g_1")
+        conv_g_1 = leakyReLU(conv_g_1, name=name + "_conv_g_1_ReLU")
+
+        conv_g_2 = separable_convs(data=conv_g_1, num_filter=cnum, kernel=ksize, stride=stride, pad=padding, dilation=1,
+                                   name=name + "_separable_conv_g_2", input_channel=cnum)
+        conv_g_2 = leakyReLU(conv_g_2, name=name + "_conv_g_2_ReLU")
+
+        conv_g_3 = conv(data=conv_g_2, num_filter=cnum, kernel=1, stride=1, pad=0, dilation=1, name=name + '_conv_g_3')
+        conv_g_3 = caffe.layers.Sigmoid(conv_g_3, in_place=True, name=name + "_conv_g_3_Sigmoid")
+
+        ######## Soft guide gating #############
+        conv_g1_1 = separable_convs(data=input1, num_filter=cnum, kernel=3, stride=stride, pad=padding, dilation=1,
+                                    name=name + "_separable_conv_g1_1", input_channel=inputChannel)
+        conv_g1_1 = caffe.layers.Sigmoid(conv_g1_1, in_place=True, name=name + "_conv_g1_1_Sigmoid")
+
+        # concat the gating
+        conv_g = caffe.layers.Concat(conv_g_3, conv_g1_1, axis=1, name=name + "_concat")
+        conv_g = conv(data=conv_g, num_filter=cnum, kernel=1, stride=1, pad=0, dilation=1, name=name + "_conv_g")
+        conv_g = caffe.layers.Sigmoid(conv_g, in_place=True, name=name + "_conv_g_Sigmoid")
+
+        # x = caffe.layers.Concat(out_f, conv_g, axis=1, name=name + "_concat_test")
+        # out_f, conv_g = caffe.layers.Slice(x, ntop=2, name=name + '_slice_test',
+        #                                    slice_param=dict(slice_dim=2, slice_point=[cnum]))
+
+        # Elementwise Multiply
+        x = caffe.layers.Eltwise(out_f, conv_g, operation=0, name=name + "_Eltwise")
+    else:
+        padding = ksize // 2 * rate
+        input1 = x
+        ######## feature ##########
+        conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+        out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+        ######## Soft mask gating #########
+        conv_g_1 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad=padding, dilation = 1, name = name+"_conv_g_1")
+        conv_g_1 = leakyReLU(conv_g_1, name = name + "_conv_g_1_ReLU")
+
+        conv_g_2 = separable_convs(data=conv_g_1, num_filter=cnum, kernel=ksize, stride=stride, pad=0, dilation=1, name=name+"_separable_conv_g_2", input_channel=cnum)
+        conv_g_2 = leakyReLU(conv_g_2, name=name+"_conv_g_2_ReLU")
+
+        conv_g_3 = conv(data = conv_g_2, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+'_conv_g_3')
+        conv_g_3 = caffe.layers.Sigmoid(conv_g_3, in_place = True, name = name + "_conv_g_3_Sigmoid")
+
+
+        # Soft guide gating
+        conv_g1_1 = separable_convs(data=input1, num_filter=cnum, kernel=3, stride=stride, pad=1, dilation=1, name=name+"_separable_conv_g1_1", input_channel=inputChannel)
+        conv_g1_1 = caffe.layers.Sigmoid(conv_g1_1, in_place = True, name = name + "_conv_g1_1_Sigmoid")
+
+        # concat the gating
+        conv_g = caffe.layers.Concat(conv_g_3, conv_g1_1, axis=1, name = name + "_concat")
+        conv_g = conv(data = conv_g, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g")
+        conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+        # x = caffe.layers.Concat(out_f, conv_g, axis = 1, name = name + "_concat_test")
+        # out_f, conv_g = caffe.layers.Slice(x, ntop=2, name=name+'_slice_test', slice_param=dict(slice_dim=2, slice_point=[cnum]) )
+
+        # Elementwise Multiply
+        # x = out_f * conv_g
+        x = caffe.layers.Eltwise(out_f, conv_g, operation = 0, name = name + "_Eltwise")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.leaky_relu(data)
+    data = leakyReLU(data, name = name + '_deconv_ReLU')
+    return data
+
+def _random_offset(x, noise, knum = 1, width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1, _x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1, _x2 = caffe.layers.Slice(noise1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    #需要复制与x相同的通道数
+    for i in range(5) : #32通道
+        noise1 = caffe.layers.Concat(noise1, noise1, axis = 1, name = name + "_noise1_concat_%d"%i)
+    for j in range(1, knum): # knum*cnum(=32)
+        noise1 = caffe.layers.Concat(noise1, noise1, axis = 1, name = name + "_noise1_concat_32_%d"%j)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+    # noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)#需要复制与x相同的通道数
+
+    x1, _x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, _x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    _x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    _x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi")
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    filters = knum * cnum
+    res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+    _res_r, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-r", slice_param=dict(slice_dim = 2, slice_point=[1]))
+    _res_c, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-c", slice_param=dict(slice_dim = 3, slice_point=[1]))
+    _res_r = caffe.layers.Reshape(_res_r, name = name + "_reshape_257_1")
+    _res_c = caffe.layers.Reshape(_res_c, name = name + "_reshape_1_256")
+    _x_256_1 = caffe.layers.Concat(_x3, _x5, axis = 1, name = name + "_others_256_1") # (256 * 1)
+    _x_1_255 = caffe.layers.Concat(_x4, _x6, axis = 1, name = name + "_others_1_255") # (1 * 255)
+    _x_256_1 = caffe.layers.Reshape(_x_256_1, name = name + "_reshape_256_1")
+    _x_1_255 = caffe.layers.Reshape(_x_1_255, name = name + "_reshape_1_255")
+    _x1 = caffe.layers.Reshape(_x1, name = name + "_reshape_512_257")
+    _x2 = caffe.layers.Reshape(_x2, name = name + "_reshape_257_255")
+    _x = caffe.layers.Concat(_res_r, _res_c, _x_256_1, _x_1_255, _x1, _x2, axis = 1, name = name + "_concat_others")
+    return res, _x
+
+
+def random_offset(x, noise, net, knum = 1,width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1_1, net.output_x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1_2, net.output_x2 = caffe.layers.Slice(noise1_1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+
+    noise1 = caffe.layers.Concat(noise1_2, noise1_2, axis=1, name=name + "_noise1_concat_32_1")
+    for j in range(2, knum*cnum): # knum*cnum(=32)
+        noise1 = caffe.layers.Concat(noise1, noise1_2, axis = 1, name = name + "_noise1_concat_32_%d"%j)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+
+    x1, net.output_x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, net.output_x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    net.output_x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    net.output_x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi") # x2 * (1.0 - noise1)
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    # filters = knum * cnum
+    # res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+
+    return res
+
+# caffe (N*C*H*W)  tensorflow (默认 N*H*W*C， 也支持 N*C*H*W)
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+
+    #输入为512*512*5
+    input_x = caffe.layers.Scale(net.data, bias_term = True, name = "input_x_Scale")
+    input_x, noise = caffe.layers.Slice(input_x, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, 4, cnum, 5, 2, name='conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = "conv1_BatchNorm")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = "conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_2 = gen_gate_conv(x_1, cnum, 2 * cnum, 5, 2, name='conv2')
+    #x_2 = tf.layers.batch_normalization(x_2)
+    # x_2 = caffe.layers.BatchNorm(x_2, use_global_stats = True, name = "conv2_BatchNorm")#测试时true，训练时false
+    # x_2 = caffe.layers.Scale(x_2, bias_term = True, name = "conv2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_3 = gen_gate_conv(x_2, 2 * cnum, 4 * cnum, 3, 2, name='conv3')
+    #x_3 = tf.layers.batch_normalization(x_3)
+    # x_3 = caffe.layers.BatchNorm(x_3, use_global_stats = True, name = "conv3_BatchNorm")#测试时true，训练时false
+    # x_3 = caffe.layers.Scale(x_3, bias_term = True, name = "conv3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 4 * cnum, 3, 2, name='conv4')
+    #x_4 = tf.layers.batch_normalization(x_4)
+    # x_4 = caffe.layers.BatchNorm(x_4, use_global_stats = True, name = "conv4_BatchNorm")#测试时true，训练时false
+    # x_4 = caffe.layers.Scale(x_4, bias_term = True, name = "conv4_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x_4, 4*cnum, name='upsample1')
+    x = conv(data = x, num_filter = 4*cnum, kernel=3, stride=1, pad=1, dilation=1, name='upsample1_conv1')
+    x = leakyReLU(x, name="up_conv1")
+    # x = tf.layers.batch_normalization(x)
+    # x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample1_BatchNorm")#测试时true，训练时false
+    # x = caffe.layers.Scale(x, bias_term = True, name = "upsample1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 4*cnum, name='upsample2')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'upsample2_concat')
+    x = gen_gate_conv(x, 6*cnum, 4*cnum, 3, 1, name='up_conv2')
+    # x = tf.layers.batch_normalization(x)
+    # x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample2_BatchNorm")#测试时true，训练时false
+    # x = caffe.layers.Scale(x, bias_term = True, name = "upsample2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 2 * cnum, name='upsample3')
+    x = caffe.layers.Concat(x, x_1, axis=1, name='upsample3_concat')
+    x = gen_gate_conv(x, 3*cnum, cnum, 3, 1, name='upsample3_conv')
+    # x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample3_BatchNorm")#测试时true，训练时false
+    # x = caffe.layers.Scale(x, bias_term = True, name = "upsample3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    net.output = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv10')
+
+
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test11_5c_without_BN_LER.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: SaveProtxt_inpainting1205.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting1205.py	(date 1576580145000)
+++ SaveProtxt_inpainting1205.py	(date 1576580145000)
@@ -0,0 +1,224 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1), depthwise不能改变通道数，所以需要再来一个1*1卷积
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name, input_channel = 0):
+    if input_channel == 0:
+        input_channel = num_filter
+    convs_depthwise = convs(data=data, num_filter=input_channel, kernel=kernel, stride=stride, pad=pad, group=input_channel, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+
+def gen_gate_conv(x, inputChannel, cnum, ksize, stride=1, rate=1, name='gen_gate_conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    input1 = x
+    input2 = x
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1)
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g_1")
+    # conv_g_1 = tf.nn.leaky_relu(conv_g_1)
+    conv_g_1 = leakyReLU(conv_g_1, name = name + "_conv_g_1_ReLU")
+    # conv_g_2 = tf.layers.separable_conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, padding=padding)
+    conv_g_2 = separable_convs(data=conv_g_1, num_filter=cnum, kernel=ksize, stride=stride, pad=padding, dilation=1, name=name+"_separable_conv_g_2", input_channel=cnum)
+    # conv_g_2 = tf.nn.leaky_relu(conv_g_2)
+    conv_g_2 = leakyReLU(conv_g_2, name=name+"_conv_g_2_ReLU")
+    # conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1)
+    conv_g_3 = conv(data = conv_g_2, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+'_conv_g_3')
+    # conv_g_3 = tf.nn.sigmoid(conv_g_3)
+    conv_g_3 = caffe.layers.Sigmoid(conv_g_3, in_place = True, name = name + "_conv_g_3_Sigmoid")
+
+
+    # Soft guide gating
+    # conv_g1_1 = tf.layers.separable_conv2d(inputs=input1, filters=cnum, kernel_size=3, strides=stride, padding=padding)#注意此处的padding不是same了
+    conv_g1_1 = separable_convs(data=input1, num_filter=cnum, kernel=3, stride=stride, pad=1, dilation=1, name=name+"_separable_conv_g1_1", input_channel=inputChannel)
+    # conv_g1_1 = tf.nn.sigmoid(conv_g1_1)
+    conv_g1_1 = caffe.layers.Sigmoid(conv_g1_1, in_place = True, name = name + "_conv_g1_1_Sigmoid")
+
+    # concat the gating
+    # conv_g = tf.concat([conv_g_3, conv_g1_1], axis=3)
+    conv_g = caffe.layers.Concat(conv_g_3, conv_g1_1, axis=1, name = name + "_concat")
+    # conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = conv(data = conv_g, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g")
+    # conv_g = tf.nn.sigmoid(conv_g)
+    conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = input2, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.leaky_relu(conv_f)
+    out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+    # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(conv_g, out_f, operation = 0, name = name + "_Eltwise")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.leaky_relu(data)
+    data = leakyReLU(data, name = name + '_deconv_ReLU')
+    return data
+
+def random_offset(x, noise, knum = 1, width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1, _x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1, _x2 = caffe.layers.Slice(noise1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    #需要复制与x相同的通道数
+    for i in range(5) : #32通道
+        noise1 = caffe.layers.Concat(noise1, noise1, axis = 1, name = name + "_noise1_concat_%d"%i)
+    for j in range(1, knum): # knum*cnum(=32)
+        noise1 = caffe.layers.Concat(noise1, noise1, axis = 1, name = name + "_noise1_concat_32_%d"%j)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+    # noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)#需要复制与x相同的通道数
+
+    x1, _x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, _x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    _x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    _x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi")
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    filters = knum * cnum
+    res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+    _res_r, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-r", slice_param=dict(slice_dim = 2, slice_point=[1]))
+    _res_c, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-c", slice_param=dict(slice_dim = 3, slice_point=[1]))
+    _res_r = caffe.layers.Reshape(_res_r, name = name + "_reshape_257_1")
+    _res_c = caffe.layers.Reshape(_res_c, name = name + "_reshape_1_256")
+    _x_256_1 = caffe.layers.Concat(_x3, _x5, axis = 1, name = name + "_others_256_1") # (256 * 1)
+    _x_1_255 = caffe.layers.Concat(_x4, _x6, axis = 1, name = name + "_others_1_255") # (1 * 255)
+    _x_256_1 = caffe.layers.Reshape(_x_256_1, name = name + "_reshape_256_1")
+    _x_1_255 = caffe.layers.Reshape(_x_1_255, name = name + "_reshape_1_255")
+    _x1 = caffe.layers.Reshape(_x1, name = name + "_reshape_512_257")
+    _x2 = caffe.layers.Reshape(_x2, name = name + "_reshape_257_255")
+    _x = caffe.layers.Concat(_res_r, _res_c, _x_256_1, _x_1_255, _x1, _x2, axis = 1, name = name + "_concat_others")
+    return res, _x
+
+# caffe (N*C*H*W)  tensorflow (默认 N*H*W*C， 也支持 N*C*H*W)
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    randomoff2 = 1 # for test11
+    #输入为512*512*5
+    input_x, noise = caffe.layers.Slice(net.data, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    input_x = caffe.layers.Scale(input_x, bias_term = True, name = "input_x_Scale")
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, 4, cnum, 5, 2, name='conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = "conv1_BatchNorm")#测试时true，训练时false
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = "conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_2 = gen_gate_conv(x_1, cnum, 2 * cnum, 5, 2, name='conv2')
+    #x_2 = tf.layers.batch_normalization(x_2)
+    x_2 = caffe.layers.BatchNorm(x_2, use_global_stats = True, name = "conv2_BatchNorm")#测试时true，训练时false
+    x_2 = caffe.layers.Scale(x_2, bias_term = True, name = "conv2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_3 = gen_gate_conv(x_2, 2 * cnum, 4 * cnum, 3, 2, name='conv3')
+    #x_3 = tf.layers.batch_normalization(x_3)
+    x_3 = caffe.layers.BatchNorm(x_3, use_global_stats = True, name = "conv3_BatchNorm")#测试时true，训练时false
+    x_3 = caffe.layers.Scale(x_3, bias_term = True, name = "conv3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 4 * cnum, 3, 2, name='conv4')
+    #x_4 = tf.layers.batch_normalization(x_4)
+    x_4 = caffe.layers.BatchNorm(x_4, use_global_stats = True, name = "conv4_BatchNorm")#测试时true，训练时false
+    x_4 = caffe.layers.Scale(x_4, bias_term = True, name = "conv4_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x_4, 4*cnum, name='upsample1')
+    # x = tf.layers.conv2d(x, 4 * cnum, 3, 1, activation=tf.nn.leaky_relu, padding='same', name='up_conv1')
+    x = conv(data = x, num_filter = 4*cnum, kernel=3, stride=1, pad=1, dilation=1, name='upsample1_conv1')
+    x = leakyReLU(x, name='up_conv1', alpha = 0.2)
+    # x = tf.layers.batch_normalization(x)
+    x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample1_BatchNorm")#测试时true，训练时false
+    x = caffe.layers.Scale(x, bias_term = True, name = "upsample1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 4*cnum, name='upsample2')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'upsample2_concat')
+    x = gen_gate_conv(x, 6*cnum, 4*cnum, 3, 1, padding='same',name='up_conv2')
+    # x = tf.layers.batch_normalization(x)
+    x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample2_BatchNorm")#测试时true，训练时false
+    x = caffe.layers.Scale(x, bias_term = True, name = "upsample2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 2 * cnum, name='upsample3')
+    # if randomoff2:
+    #     x, net.output1 = random_offset(x, noise, 2, name = "upsample3_randomoff")
+    
+    x = caffe.layers.Concat(x, x_1, axis=1, name='upsample3_concat')
+    x = gen_gate_conv(x, 3 * cnum, cnum, 3, 1, padding='same', name='upsample3_conv')
+    # x = tf.layers.batch_normalization(x)
+    x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample3_BatchNorm")#测试时true，训练时false
+    x = caffe.layers.Scale(x, bias_term = True, name = "upsample3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+    # x = tf.concat([x, input_x], axis=3, name='concat9')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    # x = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    net.output = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv10')
+
+
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test11_5c_1.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: SaveProtxt_inpainting1209.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting1209.py	(date 1575880772000)
+++ SaveProtxt_inpainting1209.py	(date 1575880772000)
@@ -0,0 +1,219 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1), depthwise不能改变通道数，所以需要再来一个1*1卷积
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name, input_channel = 0):
+    if input_channel == 0:
+        input_channel = num_filter
+    convs_depthwise = convs(data=data, num_filter=input_channel, kernel=kernel, stride=stride, pad=pad, group=input_channel, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+
+def gen_gate_conv(x, inputChannel, cnum, ksize, stride=1, rate=1, name='gen_gate_conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.sigmoid(conv_f)
+    out_f = caffe.layers.Sigmoid(conv_f, in_place= True, name=name+"_conv_f_Sigmoid")
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1)
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g_1")
+    # conv_g_1 = tf.nn.sigmoid(conv_g_1)
+    conv_g_1 = caffe.layers.Sigmoid(conv_g_1, in_place= True, name = name + "_conv_g_1_Sigmoid")
+    # conv_g_2 = tf.layers.separable_conv2d(inputs=conv_g_1, filters=cnum, kernel_size=ksize, strides=stride, padding=padding)
+    conv_g_2 = separable_convs(data=conv_g_1, num_filter=cnum, kernel=ksize, stride=stride, pad=padding, dilation=1, name=name+"_separable_conv_g_2", input_channel=cnum)
+    # conv_g_2 = tf.nn.sigmoid(conv_g_2)
+    conv_g_2 = caffe.layers.Sigmoid(conv_g_2, in_place= True, name=name+"_conv_g_2_Sigmoid")
+    # conv_g_3 = tf.layers.conv2d(inputs=conv_g_2, filters=cnum, kernel_size=1, strides=1)
+    conv_g_3 = conv(data = conv_g_2, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+'_conv_g_3')
+    # conv_g_3 = tf.nn.sigmoid(conv_g_3)
+    conv_g_3 = caffe.layers.Sigmoid(conv_g_3, in_place = True, name = name + "_conv_g_3_Sigmoid")
+
+
+    # Soft guide gating
+    # conv_g1_1 = tf.layers.separable_conv2d(inputs=input1, filters=cnum, kernel_size=3, strides=stride, padding=padding)#注意此处的padding不是same了
+    conv_g1_1 = separable_convs(data=x, num_filter=cnum, kernel=3, stride=stride, pad=1, dilation=1, name=name+"_separable_conv_g1_1", input_channel=inputChannel)
+    # conv_g1_1 = tf.nn.sigmoid(conv_g1_1)
+    conv_g1_1 = caffe.layers.Sigmoid(conv_g1_1, in_place = True, name = name + "_conv_g1_1_Sigmoid")
+
+    # concat the gating
+    # conv_g = tf.concat([conv_g_3, conv_g1_1], axis=3)
+    conv_g = caffe.layers.Concat(conv_g_3, conv_g1_1, axis=1, name = name + "_concat")
+    # conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = conv(data = conv_g, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g")
+    # conv_g = tf.nn.sigmoid(conv_g)
+    conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+    # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation = 0, name = name + "_Eltwise")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_ReLU_Sigmoid')
+    return data
+
+def random_offset(x, noise, filters = 32, width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1, _x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1, _x2 = caffe.layers.Slice(noise1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    noise2 = noise1
+    #需要复制与x相同的通道数
+    for i in range(1, filters) : #32通道
+        noise1 = caffe.layers.Concat(noise2, noise1, axis = 1, name = name + "_noise1_concat_%d"%i)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+    # noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)#需要复制与x相同的通道数
+
+    x1, _x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, _x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    _x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    _x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi")
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+    _res_r, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-r", slice_param=dict(slice_dim = 2, slice_point=[1]))
+    _res_c, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-c", slice_param=dict(slice_dim = 3, slice_point=[1]))
+    _res_r = caffe.layers.Reshape(_res_r, name = name + "_reshape_257_1")
+    _res_c = caffe.layers.Reshape(_res_c, name = name + "_reshape_1_256")
+    _x_256_1 = caffe.layers.Concat(_x3, _x5, axis = 1, name = name + "_others_256_1") # (256 * 1)
+    _x_1_255 = caffe.layers.Concat(_x4, _x6, axis = 1, name = name + "_others_1_255") # (1 * 255)
+    _x_256_1 = caffe.layers.Reshape(_x_256_1, name = name + "_reshape_256_1")
+    _x_1_255 = caffe.layers.Reshape(_x_1_255, name = name + "_reshape_1_255")
+    _x1 = caffe.layers.Reshape(_x1, name = name + "_reshape_512_257")
+    _x2 = caffe.layers.Reshape(_x2, name = name + "_reshape_257_255")
+    _x = caffe.layers.Concat(_res_r, _res_c, _x_256_1, _x_1_255, _x1, _x2, axis = 1, name = name + "_concat_others")
+    return res, _x
+
+# caffe (N*C*H*W)  tensorflow (默认 N*H*W*C， 也支持 N*C*H*W)
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    randomoff2 = 1 # for test11
+    #输入为512*512*5
+    input_x, noise = caffe.layers.Slice(net.data, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    input_x = caffe.layers.Scale(input_x, bias_term = True, name = "input_x_Scale")
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, 4, cnum, 5, 2, name='conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = "conv1_BatchNorm")#测试时true，训练时false
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = "conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_2 = gen_gate_conv(x_1, cnum, 2 * cnum, 5, 2, name='conv2')
+    #x_2 = tf.layers.batch_normalization(x_2)
+    x_2 = caffe.layers.BatchNorm(x_2, use_global_stats = True, name = "conv2_BatchNorm")#测试时true，训练时false
+    x_2 = caffe.layers.Scale(x_2, bias_term = True, name = "conv2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_3 = gen_gate_conv(x_2, 2 * cnum, 4 * cnum, 3, 2, name='conv3')
+    #x_3 = tf.layers.batch_normalization(x_3)
+    x_3 = caffe.layers.BatchNorm(x_3, use_global_stats = True, name = "conv3_BatchNorm")#测试时true，训练时false
+    x_3 = caffe.layers.Scale(x_3, bias_term = True, name = "conv3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 4 * cnum, 3, 2, name='conv4')
+    #x_4 = tf.layers.batch_normalization(x_4)
+    x_4 = caffe.layers.BatchNorm(x_4, use_global_stats = True, name = "conv4_BatchNorm")#测试时true，训练时false
+    x_4 = caffe.layers.Scale(x_4, bias_term = True, name = "conv4_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x_4, 4*cnum, name='upsample1')
+    # x = tf.layers.conv2d(x, 4 * cnum, 3, 1, activation=tf.nn.leaky_relu, padding='same', name='up_conv1')
+    x = conv(data = x, num_filter = 4*cnum, kernel=3, stride=1, pad=1, dilation=1, name='upsample1_conv1')
+    # x = leakyReLU(x, name='up_conv1', alpha = 0.2)
+    # x = tf.layers.batch_normalization(x)
+    x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample1_BatchNorm")#测试时true，训练时false
+    x = caffe.layers.Scale(x, bias_term = True, name = "upsample1_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 3*cnum, name='upsample2')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'upsample2_concat')
+    x = gen_gate_conv(x, 5*cnum, 3*cnum, 3, 1, padding='same',name='up_conv2')
+    # x = tf.layers.batch_normalization(x)
+    x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample2_BatchNorm")#测试时true，训练时false
+    x = caffe.layers.Scale(x, bias_term = True, name = "upsample2_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, 48, name='upsample3')
+    # if randomoff2:
+    #     x, net.output1 = random_offset(x, noise, 48, name = "upsample3_randomoff")
+    
+    x = caffe.layers.Concat(x, x_1, axis=1, name='upsample3_concat')
+    x = gen_gate_conv(x, cnum + 48, cnum, 3, 1, padding='same', name='upsample3_conv')
+    # x = tf.layers.batch_normalization(x)
+    x = caffe.layers.BatchNorm(x, use_global_stats = True, name = "upsample3_BatchNorm")#测试时true，训练时false
+    x = caffe.layers.Scale(x, bias_term = True, name = "upsample3_BatchNorm_Scale")#BatchNorm+Scale实现
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+    # x = tf.concat([x, input_x], axis=3, name='concat9')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    # x = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    net.output = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv10')
+
+
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test11_5c_s_no.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: loginfo.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- loginfo.txt	(date 1569408065000)
+++ loginfo.txt	(date 1569408065000)
@@ -0,0 +1,451 @@
+conv1-1_conv_f:weights  ->  [32,4,5,5]
+conv1-1_conv_f:bias  ->  [32]
+conv1-1_conv_g_1:weights  ->  [32,4,5,5]
+conv1-1_conv_g_1:bias  ->  [32]
+conv2_conv_f:weights  ->  [64,32,5,5]
+conv2_conv_f:bias  ->  [64]
+conv2_conv_g_1:weights  ->  [64,32,5,5]
+conv2_conv_g_1:bias  ->  [64]
+conv3_conv_f:weights  ->  [128,64,3,3]
+conv3_conv_f:bias  ->  [128]
+conv3_conv_g_1:weights  ->  [128,64,3,3]
+conv3_conv_g_1:bias  ->  [128]
+conv4_conv_f:weights  ->  [128,128,3,3]
+conv4_conv_f:bias  ->  [128]
+conv4_conv_g_1:weights  ->  [128,128,3,3]
+conv4_conv_g_1:bias  ->  [128]
+conv4-1_conv_f:weights  ->  [128,128,3,3]
+conv4-1_conv_f:bias  ->  [128]
+conv4-1_conv_g_1:weights  ->  [128,128,3,3]
+conv4-1_conv_g_1:bias  ->  [128]
+block1_conv1_conv_f:weights  ->  [128,128,3,3]
+block1_conv1_conv_f:bias  ->  [128]
+block1_conv1_conv_g_1:weights  ->  [128,128,3,3]
+block1_conv1_conv_g_1:bias  ->  [128]
+block1_conv2_conv_f:weights  ->  [128,128,3,3]
+block1_conv2_conv_f:bias  ->  [128]
+block1_conv2_conv_g_1:weights  ->  [128,128,3,3]
+block1_conv2_conv_g_1:bias  ->  [128]
+block2_conv1_conv_f:weights  ->  [128,128,3,3]
+block2_conv1_conv_f:bias  ->  [128]
+block2_conv1_conv_g_1:weights  ->  [128,128,3,3]
+block2_conv1_conv_g_1:bias  ->  [128]
+block2_conv2_conv_f:weights  ->  [128,128,3,3]
+block2_conv2_conv_f:bias  ->  [128]
+block2_conv2_conv_g_1:weights  ->  [128,128,3,3]
+block2_conv2_conv_g_1:bias  ->  [128]
+block3_conv1_conv_f:weights  ->  [128,128,3,3]
+block3_conv1_conv_f:bias  ->  [128]
+block3_conv1_conv_g_1:weights  ->  [128,128,3,3]
+block3_conv1_conv_g_1:bias  ->  [128]
+block3_conv2_conv_f:weights  ->  [128,128,3,3]
+block3_conv2_conv_f:bias  ->  [128]
+block3_conv2_conv_g_1:weights  ->  [128,128,3,3]
+block3_conv2_conv_g_1:bias  ->  [128]
+block4_conv1_conv_f:weights  ->  [128,128,3,3]
+block4_conv1_conv_f:bias  ->  [128]
+block4_conv1_conv_g_1:weights  ->  [128,128,3,3]
+block4_conv1_conv_g_1:bias  ->  [128]
+block4_conv2_conv_f:weights  ->  [128,128,3,3]
+block4_conv2_conv_f:bias  ->  [128]
+block4_conv2_conv_g_1:weights  ->  [128,128,3,3]
+block4_conv2_conv_g_1:bias  ->  [128]
+block5_conv1_conv_f:weights  ->  [128,128,3,3]
+block5_conv1_conv_f:bias  ->  [128]
+block5_conv1_conv_g_1:weights  ->  [128,128,3,3]
+block5_conv1_conv_g_1:bias  ->  [128]
+block5_conv2_conv_f:weights  ->  [128,128,3,3]
+block5_conv2_conv_f:bias  ->  [128]
+block5_conv2_conv_g_1:weights  ->  [128,128,3,3]
+block5_conv2_conv_g_1:bias  ->  [128]
+upsample1_deconv:weights  ->  [128,128,4,4]
+upsample1_deconv:bias  ->  [128]
+conv5:weights  ->  [128,256,3,3]
+conv5:bias  ->  [128]
+upsample2_deconv:weights  ->  [128,128,4,4]
+upsample2_deconv:bias  ->  [128]
+conv6:weights  ->  [64,256,3,3]
+conv6:bias  ->  [64]
+upsample3_deconv:weights  ->  [64,64,4,4]
+upsample3_deconv:bias  ->  [64]
+conv7:weights  ->  [32,128,3,3]
+conv7:bias  ->  [32]
+upsample4_deconv:weights  ->  [32,32,4,4]
+upsample4_deconv:bias  ->  [32]
+conv8:weights  ->  [32,64,3,3]
+conv8:bias  ->  [32]
+upsample5_deconv:weights  ->  [32,32,4,4]
+upsample5_deconv:bias  ->  [32]
+conv9-1:weights  ->  [3,36,3,3]
+conv9-1:bias  ->  [3]
+conv9-2:weights  ->  [3,3,5,5]
+conv9-2:bias  ->  [3]
+post_conv9-1:weights  ->  [3,3,3,3]
+post_conv9-1:bias  ->  [3]
+post_conv10-1:weights  ->  [3,3,1,1]
+post_conv10-1:bias  ->  [3]
+post_conv10-2:weights  ->  [3,3,3,3]
+post_conv10-2:bias  ->  [3]
+post_conv10-3:weights  ->  [3,3,1,1]
+post_conv10-3:bias  ->  [3]
+_noise2_Scale:gamma ->  [1]
+_noise2_Scale:beta  ->  [1]
+_noise2_1_Scale:gamma ->  [1]
+_noise2_1_Scale:beta  ->  [1]
+Gaussian_convB_1:weights  ->  [1,1,3,3]
+Gaussian_convB_2:weights  ->  [1,1,3,3]
+Gaussian_fcn_pad_b:weights  ->  [1,1,1,1]
+Gaussian_convG_1:weights  ->  [1,1,3,3]
+Gaussian_convG_2:weights  ->  [1,1,3,3]
+Gaussian_fcn_pad_g:weights  ->  [1,1,1,1]
+Gaussian_convR_1:weights  ->  [1,1,3,3]
+Gaussian_convR_2:weights  ->  [1,1,3,3]
+Gaussian_fcn_pad_r:weights  ->  [1,1,1,1]
+########################################################################################################################
+inpaint_net/block2/conv2d_3/bias  ->  [128]
+inpaint_net/conv2d_7/bias  ->  [128]
+inpaint_net/conv2d_4/kernel  ->  [3,3,64,128]
+inpaint_net/block5/conv2d_3/bias  ->  [128]
+inpaint_net/upsample5/conv2d_transpose/kernel  ->  [4,4,32,32]
+inpaint_net/upsample5/conv2d_transpose/bias  ->  [32]
+inpaint_net/block4/conv2d_2/bias  ->  [128]
+inpaint_net/upsample2/conv2d_transpose/bias  ->  [128]
+inpaint_net/conv2d_2/bias  ->  [64]
+inpaint_net/post/conv9-1/kernel  ->  [3,3,3,3]
+inpaint_net/conv6/bias  ->  [64]
+inpaint_net/block5/conv2d_1/kernel  ->  [3,3,128,128]
+inpaint_net/block1/conv2d_2/bias  ->  [128]
+inpaint_net/block1/conv2d/kernel  ->  [3,3,128,128]
+inpaint_net/block2/conv2d_2/bias  ->  [128]
+inpaint_net/post/conv10-2/bias  ->  [3]
+inpaint_net/block5/conv2d_3/kernel  ->  [3,3,128,128]
+inpaint_net/block4/conv2d/kernel  ->  [3,3,128,128]
+inpaint_net/upsample1/conv2d_transpose/kernel  ->  [4,4,128,128]
+inpaint_net/conv2d_3/bias  ->  [64]
+inpaint_net/upsample3/conv2d_transpose/bias  ->  [64]
+inpaint_net/block2/conv2d_1/bias  ->  [128]
+inpaint_net/block3/conv2d_1/bias  ->  [128]
+inpaint_net/post/conv10-3/kernel  ->  [1,1,3,3]
+inpaint_net/block2/conv2d_2/kernel  ->  [3,3,128,128]
+inpaint_net/block4/conv2d_1/bias  ->  [128]
+inpaint_net/conv2d/kernel  ->  [5,5,4,32]
+inpaint_net/conv2d_5/kernel  ->  [3,3,64,128]
+inpaint_net/block5/conv2d/kernel  ->  [3,3,128,128]
+inpaint_net/post/conv10-3/bias  ->  [3]
+inpaint_net/conv2d_1/bias  ->  [32]
+inpaint_net/conv2d_9/bias  ->  [128]
+inpaint_net/post/conv10-1/kernel  ->  [1,1,3,3]
+inpaint_net/conv6/kernel  ->  [3,3,256,64]
+inpaint_net/block3/conv2d_3/bias  ->  [128]
+inpaint_net/block1/conv2d_1/kernel  ->  [3,3,128,128]
+inpaint_net/block4/conv2d/bias  ->  [128]
+inpaint_net/block2/conv2d_1/kernel  ->  [3,3,128,128]
+inpaint_net/conv7/kernel  ->  [3,3,128,32]
+inpaint_net/conv2d_9/kernel  ->  [3,3,128,128]
+inpaint_net/block1/conv2d_2/kernel  ->  [3,3,128,128]
+inpaint_net/block5/conv2d_2/kernel  ->  [3,3,128,128]
+inpaint_net/post/conv10-2/kernel  ->  [3,3,3,3]
+inpaint_net/block4/conv2d_1/kernel  ->  [3,3,128,128]
+inpaint_net/conv8/kernel  ->  [3,3,64,32]
+inpaint_net/conv5/bias  ->  [128]
+inpaint_net/block5/conv2d/bias  ->  [128]
+inpaint_net/block3/conv2d/bias  ->  [128]
+inpaint_net/block4/conv2d_3/kernel  ->  [3,3,128,128]
+inpaint_net/conv9-1/bias  ->  [3]
+inpaint_net/upsample4/conv2d_transpose/kernel  ->  [4,4,32,32]
+inpaint_net/conv9-2/kernel  ->  [5,5,3,3]
+inpaint_net/conv2d/bias  ->  [32]
+inpaint_net/conv2d_8/kernel  ->  [3,3,128,128]
+inpaint_net/conv2d_3/kernel  ->  [5,5,32,64]
+inpaint_net/conv2d_8/bias  ->  [128]
+inpaint_net/block3/conv2d_2/kernel  ->  [3,3,128,128]
+inpaint_net/upsample3/conv2d_transpose/kernel  ->  [4,4,64,64]
+inpaint_net/conv2d_5/bias  ->  [128]
+inpaint_net/conv2d_1/kernel  ->  [5,5,4,32]
+inpaint_net/block4/conv2d_3/bias  ->  [128]
+inpaint_net/block3/conv2d/kernel  ->  [3,3,128,128]
+inpaint_net/conv8/bias  ->  [32]
+inpaint_net/block2/conv2d_3/kernel  ->  [3,3,128,128]
+inpaint_net/conv9-1/kernel  ->  [3,3,36,3]
+inpaint_net/conv2d_6/bias  ->  [128]
+inpaint_net/post/conv10-1/bias  ->  [3]
+inpaint_net/conv2d_7/kernel  ->  [3,3,128,128]
+inpaint_net/conv9-2/bias  ->  [3]
+inpaint_net/post/conv9-1/bias  ->  [3]
+inpaint_net/block1/conv2d_1/bias  ->  [128]
+inpaint_net/upsample2/conv2d_transpose/kernel  ->  [4,4,128,128]
+inpaint_net/conv5/kernel  ->  [3,3,256,128]
+inpaint_net/conv2d_2/kernel  ->  [5,5,32,64]
+inpaint_net/block4/conv2d_2/kernel  ->  [3,3,128,128]
+inpaint_net/conv2d_4/bias  ->  [128]
+inpaint_net/block1/conv2d_3/kernel  ->  [3,3,128,128]
+inpaint_net/block3/conv2d_3/kernel  ->  [3,3,128,128]
+inpaint_net/conv7/bias  ->  [32]
+inpaint_net/block3/conv2d_2/bias  ->  [128]
+inpaint_net/block2/conv2d/bias  ->  [128]
+inpaint_net/block5/conv2d_1/bias  ->  [128]
+inpaint_net/block1/conv2d/bias  ->  [128]
+inpaint_net/upsample1/conv2d_transpose/bias  ->  [128]
+inpaint_net/upsample4/conv2d_transpose/bias  ->  [32]
+inpaint_net/block3/conv2d_1/kernel  ->  [3,3,128,128]
+inpaint_net/block2/conv2d/kernel  ->  [3,3,128,128]
+inpaint_net/block5/conv2d_2/bias  ->  [128]
+inpaint_net/conv2d_6/kernel  ->  [3,3,128,128]
+inpaint_net/block1/conv2d_3/bias  ->  [128]
+i = 1, len = 57, le=2, key = conv1-1_conv_f
+(32, 4, 5, 5) (32, 4, 5, 5)
+conv1-1_conv_f:weights  <-  inpaint_net/conv2d/kernel
+(32,) (32,)
+conv1-1_conv_f:bias  <-  inpaint_net/conv2d/bias
+i = 2, len = 57, le=2, key = conv1-1_conv_g_1
+(32, 4, 5, 5) (32, 4, 5, 5)
+conv1-1_conv_g_1:weights  <-  inpaint_net/conv2d_1/kernel
+(32,) (32,)
+conv1-1_conv_g_1:bias  <-  inpaint_net/conv2d_1/bias
+i = 3, len = 57, le=2, key = conv2_conv_f
+(64, 32, 5, 5) (64, 32, 5, 5)
+conv2_conv_f:weights  <-  inpaint_net/conv2d_2/kernel
+(64,) (64,)
+conv2_conv_f:bias  <-  inpaint_net/conv2d_2/bias
+i = 4, len = 57, le=2, key = conv2_conv_g_1
+(64, 32, 5, 5) (64, 32, 5, 5)
+conv2_conv_g_1:weights  <-  inpaint_net/conv2d_3/kernel
+(64,) (64,)
+conv2_conv_g_1:bias  <-  inpaint_net/conv2d_3/bias
+i = 5, len = 57, le=2, key = conv3_conv_f
+(128, 64, 3, 3) (128, 64, 3, 3)
+conv3_conv_f:weights  <-  inpaint_net/conv2d_4/kernel
+(128,) (128,)
+conv3_conv_f:bias  <-  inpaint_net/conv2d_4/bias
+i = 6, len = 57, le=2, key = conv3_conv_g_1
+(128, 64, 3, 3) (128, 64, 3, 3)
+conv3_conv_g_1:weights  <-  inpaint_net/conv2d_5/kernel
+(128,) (128,)
+conv3_conv_g_1:bias  <-  inpaint_net/conv2d_5/bias
+i = 7, len = 57, le=2, key = conv4_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+conv4_conv_f:weights  <-  inpaint_net/conv2d_6/kernel
+(128,) (128,)
+conv4_conv_f:bias  <-  inpaint_net/conv2d_6/bias
+i = 8, len = 57, le=2, key = conv4_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+conv4_conv_g_1:weights  <-  inpaint_net/conv2d_7/kernel
+(128,) (128,)
+conv4_conv_g_1:bias  <-  inpaint_net/conv2d_7/bias
+i = 9, len = 57, le=2, key = conv4-1_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+conv4-1_conv_f:weights  <-  inpaint_net/conv2d_8/kernel
+(128,) (128,)
+conv4-1_conv_f:bias  <-  inpaint_net/conv2d_8/bias
+i = 10, len = 57, le=2, key = conv4-1_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+conv4-1_conv_g_1:weights  <-  inpaint_net/conv2d_9/kernel
+(128,) (128,)
+conv4-1_conv_g_1:bias  <-  inpaint_net/conv2d_9/bias
+i = 11, len = 57, le=2, key = block1_conv1_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block1_conv1_conv_f:weights  <-  inpaint_net/block1/conv2d/kernel
+(128,) (128,)
+block1_conv1_conv_f:bias  <-  inpaint_net/block1/conv2d/bias
+i = 12, len = 57, le=2, key = block1_conv1_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block1_conv1_conv_g_1:weights  <-  inpaint_net/block1/conv2d_1/kernel
+(128,) (128,)
+block1_conv1_conv_g_1:bias  <-  inpaint_net/block1/conv2d_1/bias
+i = 13, len = 57, le=2, key = block1_conv2_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block1_conv2_conv_f:weights  <-  inpaint_net/block1/conv2d_2/kernel
+(128,) (128,)
+block1_conv2_conv_f:bias  <-  inpaint_net/block1/conv2d_2/bias
+i = 14, len = 57, le=2, key = block1_conv2_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block1_conv2_conv_g_1:weights  <-  inpaint_net/block1/conv2d_3/kernel
+(128,) (128,)
+block1_conv2_conv_g_1:bias  <-  inpaint_net/block1/conv2d_3/bias
+i = 15, len = 57, le=2, key = block2_conv1_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block2_conv1_conv_f:weights  <-  inpaint_net/block2/conv2d/kernel
+(128,) (128,)
+block2_conv1_conv_f:bias  <-  inpaint_net/block2/conv2d/bias
+i = 16, len = 57, le=2, key = block2_conv1_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block2_conv1_conv_g_1:weights  <-  inpaint_net/block2/conv2d_1/kernel
+(128,) (128,)
+block2_conv1_conv_g_1:bias  <-  inpaint_net/block2/conv2d_1/bias
+i = 17, len = 57, le=2, key = block2_conv2_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block2_conv2_conv_f:weights  <-  inpaint_net/block2/conv2d_2/kernel
+(128,) (128,)
+block2_conv2_conv_f:bias  <-  inpaint_net/block2/conv2d_2/bias
+i = 18, len = 57, le=2, key = block2_conv2_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block2_conv2_conv_g_1:weights  <-  inpaint_net/block2/conv2d_3/kernel
+(128,) (128,)
+block2_conv2_conv_g_1:bias  <-  inpaint_net/block2/conv2d_3/bias
+i = 19, len = 57, le=2, key = block3_conv1_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block3_conv1_conv_f:weights  <-  inpaint_net/block3/conv2d/kernel
+(128,) (128,)
+block3_conv1_conv_f:bias  <-  inpaint_net/block3/conv2d/bias
+i = 20, len = 57, le=2, key = block3_conv1_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block3_conv1_conv_g_1:weights  <-  inpaint_net/block3/conv2d_1/kernel
+(128,) (128,)
+block3_conv1_conv_g_1:bias  <-  inpaint_net/block3/conv2d_1/bias
+i = 21, len = 57, le=2, key = block3_conv2_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block3_conv2_conv_f:weights  <-  inpaint_net/block3/conv2d_2/kernel
+(128,) (128,)
+block3_conv2_conv_f:bias  <-  inpaint_net/block3/conv2d_2/bias
+i = 22, len = 57, le=2, key = block3_conv2_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block3_conv2_conv_g_1:weights  <-  inpaint_net/block3/conv2d_3/kernel
+(128,) (128,)
+block3_conv2_conv_g_1:bias  <-  inpaint_net/block3/conv2d_3/bias
+i = 23, len = 57, le=2, key = block4_conv1_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block4_conv1_conv_f:weights  <-  inpaint_net/block4/conv2d/kernel
+(128,) (128,)
+block4_conv1_conv_f:bias  <-  inpaint_net/block4/conv2d/bias
+i = 24, len = 57, le=2, key = block4_conv1_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block4_conv1_conv_g_1:weights  <-  inpaint_net/block4/conv2d_1/kernel
+(128,) (128,)
+block4_conv1_conv_g_1:bias  <-  inpaint_net/block4/conv2d_1/bias
+i = 25, len = 57, le=2, key = block4_conv2_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block4_conv2_conv_f:weights  <-  inpaint_net/block4/conv2d_2/kernel
+(128,) (128,)
+block4_conv2_conv_f:bias  <-  inpaint_net/block4/conv2d_2/bias
+i = 26, len = 57, le=2, key = block4_conv2_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block4_conv2_conv_g_1:weights  <-  inpaint_net/block4/conv2d_3/kernel
+(128,) (128,)
+block4_conv2_conv_g_1:bias  <-  inpaint_net/block4/conv2d_3/bias
+i = 27, len = 57, le=2, key = block5_conv1_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block5_conv1_conv_f:weights  <-  inpaint_net/block5/conv2d/kernel
+(128,) (128,)
+block5_conv1_conv_f:bias  <-  inpaint_net/block5/conv2d/bias
+i = 28, len = 57, le=2, key = block5_conv1_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block5_conv1_conv_g_1:weights  <-  inpaint_net/block5/conv2d_1/kernel
+(128,) (128,)
+block5_conv1_conv_g_1:bias  <-  inpaint_net/block5/conv2d_1/bias
+i = 29, len = 57, le=2, key = block5_conv2_conv_f
+(128, 128, 3, 3) (128, 128, 3, 3)
+block5_conv2_conv_f:weights  <-  inpaint_net/block5/conv2d_2/kernel
+(128,) (128,)
+block5_conv2_conv_f:bias  <-  inpaint_net/block5/conv2d_2/bias
+i = 30, len = 57, le=2, key = block5_conv2_conv_g_1
+(128, 128, 3, 3) (128, 128, 3, 3)
+block5_conv2_conv_g_1:weights  <-  inpaint_net/block5/conv2d_3/kernel
+(128,) (128,)
+block5_conv2_conv_g_1:bias  <-  inpaint_net/block5/conv2d_3/bias
+i = 31, len = 57, le=2, key = upsample1_deconv
+(128, 128, 4, 4) (128, 128, 4, 4)
+upsample1_deconv:weights  <-  inpaint_net/upsample1/conv2d_transpose/kernel
+(128,) (128,)
+upsample1_deconv:bias  <-  inpaint_net/upsample1/conv2d_transpose/bias
+i = 32, len = 57, le=2, key = conv5
+(128, 256, 3, 3) (128, 256, 3, 3)
+conv5:weights  <-  inpaint_net/conv5/kernel
+(128,) (128,)
+conv5:bias  <-  inpaint_net/conv5/bias
+i = 33, len = 57, le=2, key = upsample2_deconv
+(128, 128, 4, 4) (128, 128, 4, 4)
+upsample2_deconv:weights  <-  inpaint_net/upsample2/conv2d_transpose/kernel
+(128,) (128,)
+upsample2_deconv:bias  <-  inpaint_net/upsample2/conv2d_transpose/bias
+i = 34, len = 57, le=2, key = conv6
+(64, 256, 3, 3) (64, 256, 3, 3)
+conv6:weights  <-  inpaint_net/conv6/kernel
+(64,) (64,)
+conv6:bias  <-  inpaint_net/conv6/bias
+i = 35, len = 57, le=2, key = upsample3_deconv
+(64, 64, 4, 4) (64, 64, 4, 4)
+upsample3_deconv:weights  <-  inpaint_net/upsample3/conv2d_transpose/kernel
+(64,) (64,)
+upsample3_deconv:bias  <-  inpaint_net/upsample3/conv2d_transpose/bias
+i = 36, len = 57, le=2, key = conv7
+(32, 128, 3, 3) (32, 128, 3, 3)
+conv7:weights  <-  inpaint_net/conv7/kernel
+(32,) (32,)
+conv7:bias  <-  inpaint_net/conv7/bias
+i = 37, len = 57, le=2, key = upsample4_deconv
+(32, 32, 4, 4) (32, 32, 4, 4)
+upsample4_deconv:weights  <-  inpaint_net/upsample4/conv2d_transpose/kernel
+(32,) (32,)
+upsample4_deconv:bias  <-  inpaint_net/upsample4/conv2d_transpose/bias
+i = 38, len = 57, le=2, key = conv8
+(32, 64, 3, 3) (32, 64, 3, 3)
+conv8:weights  <-  inpaint_net/conv8/kernel
+(32,) (32,)
+conv8:bias  <-  inpaint_net/conv8/bias
+i = 39, len = 57, le=2, key = upsample5_deconv
+(32, 32, 4, 4) (32, 32, 4, 4)
+upsample5_deconv:weights  <-  inpaint_net/upsample5/conv2d_transpose/kernel
+(32,) (32,)
+upsample5_deconv:bias  <-  inpaint_net/upsample5/conv2d_transpose/bias
+i = 40, len = 57, le=2, key = conv9-1
+(3, 36, 3, 3) (3, 36, 3, 3)
+conv9-1:weights  <-  inpaint_net/conv9-1/kernel
+(3,) (3,)
+conv9-1:bias  <-  inpaint_net/conv9-1/bias
+i = 41, len = 57, le=2, key = conv9-2
+(3, 3, 5, 5) (3, 3, 5, 5)
+conv9-2:weights  <-  inpaint_net/conv9-2/kernel
+(3,) (3,)
+conv9-2:bias  <-  inpaint_net/conv9-2/bias
+i = 42, len = 57, le=2, key = post_conv9-1
+(3, 3, 3, 3) (3, 3, 3, 3)
+post_conv9-1:weights  <-  inpaint_net/post/conv9-1/kernel
+(3,) (3,)
+post_conv9-1:bias  <-  inpaint_net/post/conv9-1/bias
+i = 43, len = 57, le=2, key = post_conv10-1
+(3, 3, 1, 1) (3, 3, 1, 1)
+post_conv10-1:weights  <-  inpaint_net/post/conv10-1/kernel
+(3,) (3,)
+post_conv10-1:bias  <-  inpaint_net/post/conv10-1/bias
+i = 44, len = 57, le=2, key = post_conv10-2
+(3, 3, 3, 3) (3, 3, 3, 3)
+post_conv10-2:weights  <-  inpaint_net/post/conv10-2/kernel
+(3,) (3,)
+post_conv10-2:bias  <-  inpaint_net/post/conv10-2/bias
+i = 45, len = 57, le=2, key = post_conv10-3
+(3, 3, 1, 1) (3, 3, 1, 1)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+(3,) (3,)
+post_conv10-3:bias  <-  inpaint_net/post/conv10-3/bias
+i = 46, len = 57, le=2, key = _noise2_Scale
+scale layer!
+i = 47, len = 57, le=2, key = _noise2_1_Scale
+scale layer!
+i = 48, len = 57, le=1, key = Gaussian_convB_1
+(1, 1, 3, 3) (1, 1, 3, 3)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 49, len = 57, le=1, key = Gaussian_convB_2
+(1, 1, 3, 3) (1, 1, 3, 3)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 50, len = 57, le=1, key = Gaussian_fcn_pad_b
+(1, 1, 1, 1) (1, 1, 1, 1)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 51, len = 57, le=1, key = Gaussian_convG_1
+(1, 1, 3, 3) (1, 1, 3, 3)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 52, len = 57, le=1, key = Gaussian_convG_2
+(1, 1, 3, 3) (1, 1, 3, 3)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 53, len = 57, le=1, key = Gaussian_fcn_pad_g
+(1, 1, 1, 1) (1, 1, 1, 1)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 54, len = 57, le=1, key = Gaussian_convR_1
+(1, 1, 3, 3) (1, 1, 3, 3)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 55, len = 57, le=1, key = Gaussian_convR_2
+(1, 1, 3, 3) (1, 1, 3, 3)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+i = 56, len = 57, le=1, key = Gaussian_fcn_pad_r
+(1, 1, 1, 1) (1, 1, 1, 1)
+post_conv10-3:weights  <-  inpaint_net/post/conv10-3/kernel
+99
Index: weights_tensor2caffe_inpainting1206.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting1206.py	(date 1578281147000)
+++ weights_tensor2caffe_inpainting1206.py	(date 1578281147000)
@@ -0,0 +1,351 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+def tensor2caffe(net,tensor):
+
+    ct_map={"conv1_conv_f:weights"                              :   "inpaint_net/conv1/conv2d/kernel" ,
+            "conv1_conv_f:bias"                                 :   "inpaint_net/conv1/conv2d/bias" ,
+            "conv1_conv_g_1:weights"                            :   "inpaint_net/conv1/conv2d_1/kernel" ,
+            "conv1_conv_g_1:bias"                               :   "inpaint_net/conv1/conv2d_1/bias" ,
+            "conv1_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv1/separable_conv2d/depthwise_kernel" ,
+            "conv1_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv1/separable_conv2d/pointwise_kernel" ,
+            "conv1_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv1/separable_conv2d/bias" ,
+            "conv1_conv_g_3:weights"                            :   "inpaint_net/conv1/conv2d_2/kernel" ,
+            "conv1_conv_g_3:bias"                               :   "inpaint_net/conv1/conv2d_2/bias" ,
+            "conv1_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv1/separable_conv2d_1/depthwise_kernel" ,
+            "conv1_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv1/separable_conv2d_1/pointwise_kernel" ,
+            "conv1_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv1/separable_conv2d_1/bias" ,
+            "conv1_conv_g:weights"                              :   "inpaint_net/conv1/conv2d_3/kernel" ,
+            "conv1_conv_g:bias"                                 :   "inpaint_net/conv1/conv2d_3/bias" ,
+            "conv1_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization/moving_mean"  ,
+            "conv1_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization/moving_variance"  ,
+            "conv1_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization/gamma" ,
+            "conv1_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization/beta" ,
+            "conv2_conv_f:weights"                              :   "inpaint_net/conv2/conv2d/kernel" ,
+            "conv2_conv_f:bias"                                 :   "inpaint_net/conv2/conv2d/bias" ,
+            "conv2_conv_g_1:weights"                            :   "inpaint_net/conv2/conv2d_1/kernel" ,
+            "conv2_conv_g_1:bias"                               :   "inpaint_net/conv2/conv2d_1/bias" ,
+            "conv2_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv2/separable_conv2d/depthwise_kernel" ,
+            "conv2_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv2/separable_conv2d/pointwise_kernel" ,
+            "conv2_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv2/separable_conv2d/bias" ,
+            "conv2_conv_g_3:weights"                            :   "inpaint_net/conv2/conv2d_2/kernel" ,
+            "conv2_conv_g_3:bias"                               :   "inpaint_net/conv2/conv2d_2/bias" ,
+            "conv2_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv2/separable_conv2d_1/depthwise_kernel" ,
+            "conv2_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv2/separable_conv2d_1/pointwise_kernel" ,
+            "conv2_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv2/separable_conv2d_1/bias" ,
+            "conv2_conv_g:weights"                              :   "inpaint_net/conv2/conv2d_3/kernel" ,
+            "conv2_conv_g:bias"                                 :   "inpaint_net/conv2/conv2d_3/bias" ,
+            "conv2_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_1/moving_mean"  ,
+            "conv2_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_1/moving_variance"  ,
+            "conv2_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_1/gamma" ,
+            "conv2_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_1/beta" ,
+            "conv3_conv_f:weights"                              :   "inpaint_net/conv3/conv2d/kernel" ,
+            "conv3_conv_f:bias"                                 :   "inpaint_net/conv3/conv2d/bias" ,
+            "conv3_conv_g_1:weights"                            :   "inpaint_net/conv3/conv2d_1/kernel" ,
+            "conv3_conv_g_1:bias"                               :   "inpaint_net/conv3/conv2d_1/bias" ,
+            "conv3_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv3/separable_conv2d/depthwise_kernel" ,
+            "conv3_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv3/separable_conv2d/pointwise_kernel" ,
+            "conv3_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv3/separable_conv2d/bias" ,
+            "conv3_conv_g_3:weights"                            :   "inpaint_net/conv3/conv2d_2/kernel" ,
+            "conv3_conv_g_3:bias"                               :   "inpaint_net/conv3/conv2d_2/bias" ,
+            "conv3_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv3/separable_conv2d_1/depthwise_kernel" ,
+            "conv3_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv3/separable_conv2d_1/pointwise_kernel" ,
+            "conv3_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv3/separable_conv2d_1/bias" ,
+            "conv3_conv_g:weights"                              :   "inpaint_net/conv3/conv2d_3/kernel" ,
+            "conv3_conv_g:bias"                                 :   "inpaint_net/conv3/conv2d_3/bias" ,
+            "conv3_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_2/moving_mean"  ,
+            "conv3_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_2/moving_variance"  ,
+            "conv3_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_2/gamma" ,
+            "conv3_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_2/beta" ,
+            "conv4_conv_f:weights"                              :   "inpaint_net/conv4/conv2d/kernel" ,
+            "conv4_conv_f:bias"                                 :   "inpaint_net/conv4/conv2d/bias" ,
+            "conv4_conv_g_1:weights"                            :   "inpaint_net/conv4/conv2d_1/kernel" ,
+            "conv4_conv_g_1:bias"                               :   "inpaint_net/conv4/conv2d_1/bias" ,
+            "conv4_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv4/separable_conv2d/depthwise_kernel" ,
+            "conv4_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv4/separable_conv2d/pointwise_kernel" ,
+            "conv4_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv4/separable_conv2d/bias" ,
+            "conv4_conv_g_3:weights"                            :   "inpaint_net/conv4/conv2d_2/kernel" ,
+            "conv4_conv_g_3:bias"                               :   "inpaint_net/conv4/conv2d_2/bias" ,
+            "conv4_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv4/separable_conv2d_1/depthwise_kernel" ,
+            "conv4_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv4/separable_conv2d_1/pointwise_kernel" ,
+            "conv4_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv4/separable_conv2d_1/bias" ,
+            "conv4_conv_g:weights"                              :   "inpaint_net/conv4/conv2d_3/kernel" ,
+            "conv4_conv_g:bias"                                 :   "inpaint_net/conv4/conv2d_3/bias" ,
+            "conv4_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_3/moving_mean"  ,
+            "conv4_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_3/moving_variance"  ,
+            "conv4_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_3/gamma" ,
+            "conv4_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_3/beta" ,
+            "upsample1_deconv:weights"                          :   "inpaint_net/upsample1/conv2d_transpose/kernel" ,
+            "upsample1_deconv:bias"                             :   "inpaint_net/upsample1/conv2d_transpose/bias" ,
+            "upsample1_conv1:weights"                           :   "inpaint_net/up_conv1/kernel" ,
+            "upsample1_conv1:bias"                              :   "inpaint_net/up_conv1/bias" ,
+            "upsample1_BatchNorm:moving_mean"                   :   "inpaint_net/batch_normalization_4/moving_mean"  ,
+            "upsample1_BatchNorm:moving_variance"               :   "inpaint_net/batch_normalization_4/moving_variance"  ,
+            "upsample1_BatchNorm_Scale:gamma"                   :   "inpaint_net/batch_normalization_4/gamma" ,
+            "upsample1_BatchNorm_Scale:beta"                    :   "inpaint_net/batch_normalization_4/beta" ,
+            "upsample2_deconv:weights"                          :   "inpaint_net/upsample2/conv2d_transpose/kernel" ,
+            "upsample2_deconv:bias"                             :   "inpaint_net/upsample2/conv2d_transpose/bias" ,
+            "up_conv2_conv_f:weights"                           :   "inpaint_net/up_conv2/conv2d/kernel" ,
+            "up_conv2_conv_f:bias"                              :   "inpaint_net/up_conv2/conv2d/bias" ,
+            "up_conv2_conv_g_1:weights"                         :   "inpaint_net/up_conv2/conv2d_1/kernel" ,
+            "up_conv2_conv_g_1:bias"                            :   "inpaint_net/up_conv2/conv2d_1/bias" ,
+            "up_conv2_separable_conv_g_2_depthwise:weights"     :   "inpaint_net/up_conv2/separable_conv2d/depthwise_kernel" ,
+            "up_conv2_separable_conv_g_2_pointwise:weights"     :   "inpaint_net/up_conv2/separable_conv2d/pointwise_kernel" ,
+            "up_conv2_separable_conv_g_2_pointwise:bias"        :   "inpaint_net/up_conv2/separable_conv2d/bias" ,
+            "up_conv2_conv_g_3:weights"                         :   "inpaint_net/up_conv2/conv2d_2/kernel" ,
+            "up_conv2_conv_g_3:bias"                            :   "inpaint_net/up_conv2/conv2d_2/bias" ,
+            "up_conv2_separable_conv_g1_1_depthwise:weights"    :   "inpaint_net/up_conv2/separable_conv2d_1/depthwise_kernel" ,
+            "up_conv2_separable_conv_g1_1_pointwise:weights"    :   "inpaint_net/up_conv2/separable_conv2d_1/pointwise_kernel" ,
+            "up_conv2_separable_conv_g1_1_pointwise:bias"       :   "inpaint_net/up_conv2/separable_conv2d_1/bias" ,
+            "up_conv2_conv_g:weights"                           :   "inpaint_net/up_conv2/conv2d_3/kernel" ,
+            "up_conv2_conv_g:bias"                              :   "inpaint_net/up_conv2/conv2d_3/bias" ,
+            "upsample2_BatchNorm:moving_mean"                   :   "inpaint_net/batch_normalization_5/moving_mean"  ,
+            "upsample2_BatchNorm:moving_variance"               :   "inpaint_net/batch_normalization_5/moving_variance"  ,
+            "upsample2_BatchNorm_Scale:gamma"                   :   "inpaint_net/batch_normalization_5/gamma" ,
+            "upsample2_BatchNorm_Scale:beta"                    :   "inpaint_net/batch_normalization_5/beta" ,
+            "upsample3_deconv:weights"                          :   "inpaint_net/upsample3/conv2d_transpose/kernel" ,
+            "upsample3_deconv:bias"                             :   "inpaint_net/upsample3/conv2d_transpose/bias" ,
+            "upsample3_conv_conv_f:weights"                     :   "inpaint_net/conv8/conv2d/kernel" ,
+            "upsample3_conv_conv_f:bias"                        :   "inpaint_net/conv8/conv2d/bias" ,
+            "upsample3_conv_conv_g_1:weights"                       :   "inpaint_net/conv8/conv2d_1/kernel" ,
+            "upsample3_conv_conv_g_1:bias"                          :   "inpaint_net/conv8/conv2d_1/bias" ,
+            "upsample3_conv_separable_conv_g_2_depthwise:weights"   :   "inpaint_net/conv8/separable_conv2d/depthwise_kernel" ,
+            "upsample3_conv_separable_conv_g_2_pointwise:weights"   :   "inpaint_net/conv8/separable_conv2d/pointwise_kernel" ,
+            "upsample3_conv_separable_conv_g_2_pointwise:bias"      :   "inpaint_net/conv8/separable_conv2d/bias" ,
+            "upsample3_conv_conv_g_3:weights"                       :   "inpaint_net/conv8/conv2d_2/kernel" ,
+            "upsample3_conv_conv_g_3:bias"                          :   "inpaint_net/conv8/conv2d_2/bias" ,
+            "upsample3_conv_separable_conv_g1_1_depthwise:weights"  :   "inpaint_net/conv8/separable_conv2d_1/depthwise_kernel" ,
+            "upsample3_conv_separable_conv_g1_1_pointwise:weights"  :   "inpaint_net/conv8/separable_conv2d_1/pointwise_kernel" ,
+            "upsample3_conv_separable_conv_g1_1_pointwise:bias"     :   "inpaint_net/conv8/separable_conv2d_1/bias" ,
+            "upsample3_conv_conv_g:weights"                         :   "inpaint_net/conv8/conv2d_3/kernel" ,
+            "upsample3_conv_conv_g:bias"                            :   "inpaint_net/conv8/conv2d_3/bias" ,
+            "upsample3_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_6/moving_mean"  ,
+            "upsample3_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_6/moving_variance"  ,
+            "upsample3_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_6/gamma" ,
+            "upsample3_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_6/beta" ,
+            "upsample4_deconv:weights"                              :   "inpaint_net/upsample4/conv2d_transpose/kernel" ,
+            "upsample4_deconv:bias"                                 :   "inpaint_net/upsample4/conv2d_transpose/bias" ,
+            "conv10:weights"                                        :   "inpaint_net/conv10/kernel" ,
+            "conv10:bias"                                           :   "inpaint_net/conv10/bias" ,
+      }
+    
+    num=0
+    net_keys=net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s'%(i, len(net_keys), le, net_keys[i]))
+        # if(net_keys[i] == 'upsample1_deconv'): continue
+        # if(net_keys[i] == 'upsample2_deconv'): continue
+        # if(net_keys[i] == 'upsample3_deconv'): continue
+        # if(net_keys[i] == 'upsample4_deconv'): continue
+        # if(net_keys[i] == 'upsample5_deconv'): continue
+        if le==1:
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value=weights_value.transpose((3,2,0,1)) 
+            else:
+                key0=net_keys[i]+':weights'
+                if "pad" in key0: # for padding, no operation
+                    # print(net.params[net_keys[i]][0].data.shape)
+                    print("pad layer")
+                    net.params[net_keys[i]][0].data[:,:,:,:] = np.array([1.])
+                    continue
+                value0=ct_map[key0]
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1)) 
+            try:
+                net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+            except:
+                net.params[net_keys[i]][0].data[:,:,:,:]=0
+                net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+            
+            
+            print(weights_value.shape,end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+            
+        elif le==2:
+            if le_last==3:
+                print("le_last = %d"%le_last)
+                key0=net_keys[i]+':gamma'
+                value0=ct_map[key0]
+                
+                gamma_value=tensor[value0] 
+                net.params[net_keys[i]][0].data[:]=gamma_value
+                # print(gamma_value.shape,end=' ')
+                # print(net.params[net_keys[i]][0].data.shape)
+                # print(gamma_value)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':beta'
+                value1=ct_map[key1]
+                
+                beta_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=beta_value
+                # print(beta_value.shape,end=' ')
+                # print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+            elif "inverse" in net_keys[i]: # 1 - noise
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+            else:   
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1))
+                    
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value[:,:,:,:]
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value[:,:,:,:]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+#                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+#                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':bias'
+                value1=ct_map[key1]
+                
+                bias_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=bias_value
+                print(bias_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+        elif le==3:
+            print("le = %d"%le)
+            key0=net_keys[i]+':moving_mean'
+            value0=ct_map[key0]
+            
+            mean_value=tensor[value0]
+            net.params[net_keys[i]][0].data[:]=mean_value
+            # print(mean_value.shape,end=' ')
+            # print(net.params[net_keys[i]][0].data.shape)
+            # print(mean_value)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+        
+            key1=net_keys[i]+':moving_variance'
+            value1=ct_map[key1]
+            
+            variance_value=tensor[value1]
+            net.params[net_keys[i]][1].data[:]=variance_value
+            # print(variance_value.shape,end=' ')
+            # print(net.params[net_keys[i]][1].data.shape)
+            # print(variance_value)
+            print("%s  <-  %s"%(key1,value1))
+            num=num+1
+        else:
+            print("error: le=%d %s"%(le, net_keys[i]))
+        le_last = le
+    print(num)
+    print('net_keys[0]',net_keys[0])
+    net.params[net_keys[0]][0].data[:] = np.array([1./128, 1./128, 1./128, 1./255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0.])
+
+
+
+def print_caffemodel(net):
+    net_keys=net.params.keys()
+    le_last = 0
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le==1:
+            n,c,w,h=net.params[net_keys[i]][0].data.shape
+            print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                # BatchNorm layer后面的scale layer, 训练gamma和deta
+                w=net.params[net_keys[i]][0].data.shape
+                print("%s:gamma ->  [%d]"%(net_keys[i], w[0]))
+
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:beta  ->  [%d]"%(net_keys[i], w[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+            
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]"%(net_keys[i],w[0]))
+        elif le==3:
+            #BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]"%(net_keys[i],m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]"%(net_keys[i],v[0]))
+
+            #f = net.params[net_keys[i]][2].data.shape
+            #print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys=tensor.keys()
+    for i in range(0,len(tensor_keys)):
+        le=len(tensor[tensor_keys[i]].shape)
+        if le==1:
+            w=tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]"%(tensor_keys[i],w))
+        elif le==4:
+            weights=tensor[tensor_keys[i]]
+            #if "filter"  in tensor_keys[i]:
+                #weights=tensor[tensor_keys[i]].transpose((2,3,1,0)) 
+            #else:
+                #weights=tensor[tensor_keys[i]].transpose((3,2,1,0)) 
+            n,c,w,h=weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]"%(tensor_keys[i],n,c,w,h))
+
+#print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net('/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test11_5c_test.prototxt', caffe.TEST)
+    print_caffemodel(net)
+    print("########################################################################################################################")
+    tensor=np.load('inpainting/test11.npy').item()
+    print_tensorflow(tensor)
+    print("########################################################################################################################")
+    tensor2caffe(net,tensor)
+    net.save("inpainting/test11_5c_test.caffemodel")
Index: weights_tensor2caffe_inpainting1209.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting1209.py	(date 1578911802000)
+++ weights_tensor2caffe_inpainting1209.py	(date 1578911802000)
@@ -0,0 +1,405 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+def tensor2caffe(net,tensor):
+
+    ct_map={"conv1_conv_f:weights"                              :   "inpaint_net/conv1/conv2d/kernel" ,
+            "conv1_conv_f:bias"                                 :   "inpaint_net/conv1/conv2d/bias" ,
+            "conv1_conv_g_1:weights"                            :   "inpaint_net/conv1/conv2d_1/kernel" ,
+            "conv1_conv_g_1:bias"                               :   "inpaint_net/conv1/conv2d_1/bias" ,
+            "conv1_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv1/separable_conv2d/depthwise_kernel" ,
+            "conv1_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv1/separable_conv2d/pointwise_kernel" ,
+            "conv1_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv1/separable_conv2d/bias" ,
+            "conv1_conv_g_3:weights"                            :   "inpaint_net/conv1/conv2d_2/kernel" ,
+            "conv1_conv_g_3:bias"                               :   "inpaint_net/conv1/conv2d_2/bias" ,
+            "conv1_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv1/separable_conv2d_1/depthwise_kernel" ,
+            "conv1_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv1/separable_conv2d_1/pointwise_kernel" ,
+            "conv1_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv1/separable_conv2d_1/bias" ,
+            "conv1_conv_g:weights"                              :   "inpaint_net/conv1/conv2d_3/kernel" ,
+            "conv1_conv_g:bias"                                 :   "inpaint_net/conv1/conv2d_3/bias" ,
+            "conv1_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization/moving_mean"  ,
+            "conv1_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization/moving_variance"  ,
+            "conv1_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization/gamma" ,
+            "conv1_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization/beta" ,
+            "conv2_conv_f:weights"                              :   "inpaint_net/conv2/conv2d/kernel" ,
+            "conv2_conv_f:bias"                                 :   "inpaint_net/conv2/conv2d/bias" ,
+            "conv2_conv_g_1:weights"                            :   "inpaint_net/conv2/conv2d_1/kernel" ,
+            "conv2_conv_g_1:bias"                               :   "inpaint_net/conv2/conv2d_1/bias" ,
+            "conv2_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv2/separable_conv2d/depthwise_kernel" ,
+            "conv2_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv2/separable_conv2d/pointwise_kernel" ,
+            "conv2_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv2/separable_conv2d/bias" ,
+            "conv2_conv_g_3:weights"                            :   "inpaint_net/conv2/conv2d_2/kernel" ,
+            "conv2_conv_g_3:bias"                               :   "inpaint_net/conv2/conv2d_2/bias" ,
+            "conv2_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv2/separable_conv2d_1/depthwise_kernel" ,
+            "conv2_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv2/separable_conv2d_1/pointwise_kernel" ,
+            "conv2_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv2/separable_conv2d_1/bias" ,
+            "conv2_conv_g:weights"                              :   "inpaint_net/conv2/conv2d_3/kernel" ,
+            "conv2_conv_g:bias"                                 :   "inpaint_net/conv2/conv2d_3/bias" ,
+            "conv2_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_1/moving_mean"  ,
+            "conv2_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_1/moving_variance"  ,
+            "conv2_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_1/gamma" ,
+            "conv2_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_1/beta" ,
+            "conv3_conv_f:weights"                              :   "inpaint_net/conv3/conv2d/kernel" ,
+            "conv3_conv_f:bias"                                 :   "inpaint_net/conv3/conv2d/bias" ,
+            "conv3_conv_g_1:weights"                            :   "inpaint_net/conv3/conv2d_1/kernel" ,
+            "conv3_conv_g_1:bias"                               :   "inpaint_net/conv3/conv2d_1/bias" ,
+            "conv3_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv3/separable_conv2d/depthwise_kernel" ,
+            "conv3_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv3/separable_conv2d/pointwise_kernel" ,
+            "conv3_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv3/separable_conv2d/bias" ,
+            "conv3_conv_g_3:weights"                            :   "inpaint_net/conv3/conv2d_2/kernel" ,
+            "conv3_conv_g_3:bias"                               :   "inpaint_net/conv3/conv2d_2/bias" ,
+            "conv3_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv3/separable_conv2d_1/depthwise_kernel" ,
+            "conv3_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv3/separable_conv2d_1/pointwise_kernel" ,
+            "conv3_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv3/separable_conv2d_1/bias" ,
+            "conv3_conv_g:weights"                              :   "inpaint_net/conv3/conv2d_3/kernel" ,
+            "conv3_conv_g:bias"                                 :   "inpaint_net/conv3/conv2d_3/bias" ,
+            "conv3_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_2/moving_mean"  ,
+            "conv3_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_2/moving_variance"  ,
+            "conv3_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_2/gamma" ,
+            "conv3_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_2/beta" ,
+            "conv4_conv_f:weights"                              :   "inpaint_net/conv4/conv2d/kernel" ,
+            "conv4_conv_f:bias"                                 :   "inpaint_net/conv4/conv2d/bias" ,
+            "conv4_conv_g_1:weights"                            :   "inpaint_net/conv4/conv2d_1/kernel" ,
+            "conv4_conv_g_1:bias"                               :   "inpaint_net/conv4/conv2d_1/bias" ,
+            "conv4_separable_conv_g_2_depthwise:weights"        :   "inpaint_net/conv4/separable_conv2d/depthwise_kernel" ,
+            "conv4_separable_conv_g_2_pointwise:weights"        :   "inpaint_net/conv4/separable_conv2d/pointwise_kernel" ,
+            "conv4_separable_conv_g_2_pointwise:bias"           :   "inpaint_net/conv4/separable_conv2d/bias" ,
+            "conv4_conv_g_3:weights"                            :   "inpaint_net/conv4/conv2d_2/kernel" ,
+            "conv4_conv_g_3:bias"                               :   "inpaint_net/conv4/conv2d_2/bias" ,
+            "conv4_separable_conv_g1_1_depthwise:weights"       :   "inpaint_net/conv4/separable_conv2d_1/depthwise_kernel" ,
+            "conv4_separable_conv_g1_1_pointwise:weights"       :   "inpaint_net/conv4/separable_conv2d_1/pointwise_kernel" ,
+            "conv4_separable_conv_g1_1_pointwise:bias"          :   "inpaint_net/conv4/separable_conv2d_1/bias" ,
+            "conv4_conv_g:weights"                              :   "inpaint_net/conv4/conv2d_3/kernel" ,
+            "conv4_conv_g:bias"                                 :   "inpaint_net/conv4/conv2d_3/bias" ,
+            "conv4_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_3/moving_mean"  ,
+            "conv4_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_3/moving_variance"  ,
+            "conv4_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_3/gamma" ,
+            "conv4_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_3/beta" ,
+            "upsample1_deconv:weights"                          :   "inpaint_net/upsample1/conv2d_transpose/kernel" ,
+            "upsample1_deconv:bias"                             :   "inpaint_net/upsample1/conv2d_transpose/bias" ,
+            "upsample1_conv1:weights"                           :   "inpaint_net/up_conv1/kernel" ,
+            "upsample1_conv1:bias"                              :   "inpaint_net/up_conv1/bias" ,
+            "upsample1_BatchNorm:moving_mean"                   :   "inpaint_net/batch_normalization_4/moving_mean"  ,
+            "upsample1_BatchNorm:moving_variance"               :   "inpaint_net/batch_normalization_4/moving_variance"  ,
+            "upsample1_BatchNorm_Scale:gamma"                   :   "inpaint_net/batch_normalization_4/gamma" ,
+            "upsample1_BatchNorm_Scale:beta"                    :   "inpaint_net/batch_normalization_4/beta" ,
+            "upsample2_deconv:weights"                          :   "inpaint_net/upsample2/conv2d_transpose/kernel" ,
+            "upsample2_deconv:bias"                             :   "inpaint_net/upsample2/conv2d_transpose/bias" ,
+            "up_conv2_conv_f:weights"                           :   "inpaint_net/up_conv2/conv2d/kernel" ,
+            "up_conv2_conv_f:bias"                              :   "inpaint_net/up_conv2/conv2d/bias" ,
+            "up_conv2_conv_g_1:weights"                         :   "inpaint_net/up_conv2/conv2d_1/kernel" ,
+            "up_conv2_conv_g_1:bias"                            :   "inpaint_net/up_conv2/conv2d_1/bias" ,
+            "up_conv2_separable_conv_g_2_depthwise:weights"     :   "inpaint_net/up_conv2/separable_conv2d/depthwise_kernel" ,
+            "up_conv2_separable_conv_g_2_pointwise:weights"     :   "inpaint_net/up_conv2/separable_conv2d/pointwise_kernel" ,
+            "up_conv2_separable_conv_g_2_pointwise:bias"        :   "inpaint_net/up_conv2/separable_conv2d/bias" ,
+            "up_conv2_conv_g_3:weights"                         :   "inpaint_net/up_conv2/conv2d_2/kernel" ,
+            "up_conv2_conv_g_3:bias"                            :   "inpaint_net/up_conv2/conv2d_2/bias" ,
+            "up_conv2_separable_conv_g1_1_depthwise:weights"    :   "inpaint_net/up_conv2/separable_conv2d_1/depthwise_kernel" ,
+            "up_conv2_separable_conv_g1_1_pointwise:weights"    :   "inpaint_net/up_conv2/separable_conv2d_1/pointwise_kernel" ,
+            "up_conv2_separable_conv_g1_1_pointwise:bias"       :   "inpaint_net/up_conv2/separable_conv2d_1/bias" ,
+            "up_conv2_conv_g:weights"                           :   "inpaint_net/up_conv2/conv2d_3/kernel" ,
+            "up_conv2_conv_g:bias"                              :   "inpaint_net/up_conv2/conv2d_3/bias" ,
+            "upsample2_BatchNorm:moving_mean"                   :   "inpaint_net/batch_normalization_5/moving_mean"  ,
+            "upsample2_BatchNorm:moving_variance"               :   "inpaint_net/batch_normalization_5/moving_variance"  ,
+            "upsample2_BatchNorm_Scale:gamma"                   :   "inpaint_net/batch_normalization_5/gamma" ,
+            "upsample2_BatchNorm_Scale:beta"                    :   "inpaint_net/batch_normalization_5/beta" ,
+            "upsample3_deconv:weights"                          :   "inpaint_net/upsample3/conv2d_transpose/kernel" ,
+            "upsample3_deconv:bias"                             :   "inpaint_net/upsample3/conv2d_transpose/bias" ,
+            "upsample3_conv_conv_f:weights"                     :   "inpaint_net/conv8/conv2d/kernel" ,
+            "upsample3_conv_conv_f:bias"                        :   "inpaint_net/conv8/conv2d/bias" ,
+            "upsample3_conv_conv_g_1:weights"                       :   "inpaint_net/conv8/conv2d_1/kernel" ,
+            "upsample3_conv_conv_g_1:bias"                          :   "inpaint_net/conv8/conv2d_1/bias" ,
+            "upsample3_conv_separable_conv_g_2_depthwise:weights"   :   "inpaint_net/conv8/separable_conv2d/depthwise_kernel" ,
+            "upsample3_conv_separable_conv_g_2_pointwise:weights"   :   "inpaint_net/conv8/separable_conv2d/pointwise_kernel" ,
+            "upsample3_conv_separable_conv_g_2_pointwise:bias"      :   "inpaint_net/conv8/separable_conv2d/bias" ,
+            "upsample3_conv_conv_g_3:weights"                       :   "inpaint_net/conv8/conv2d_2/kernel" ,
+            "upsample3_conv_conv_g_3:bias"                          :   "inpaint_net/conv8/conv2d_2/bias" ,
+            "upsample3_conv_separable_conv_g1_1_depthwise:weights"  :   "inpaint_net/conv8/separable_conv2d_1/depthwise_kernel" ,
+            "upsample3_conv_separable_conv_g1_1_pointwise:weights"  :   "inpaint_net/conv8/separable_conv2d_1/pointwise_kernel" ,
+            "upsample3_conv_separable_conv_g1_1_pointwise:bias"     :   "inpaint_net/conv8/separable_conv2d_1/bias" ,
+            "upsample3_conv_conv_g:weights"                         :   "inpaint_net/conv8/conv2d_3/kernel" ,
+            "upsample3_conv_conv_g:bias"                            :   "inpaint_net/conv8/conv2d_3/bias" ,
+            "upsample3_BatchNorm:moving_mean"                       :   "inpaint_net/batch_normalization_6/moving_mean"  ,
+            "upsample3_BatchNorm:moving_variance"                   :   "inpaint_net/batch_normalization_6/moving_variance"  ,
+            "upsample3_BatchNorm_Scale:gamma"                       :   "inpaint_net/batch_normalization_6/gamma" ,
+            "upsample3_BatchNorm_Scale:beta"                        :   "inpaint_net/batch_normalization_6/beta" ,
+            "upsample4_deconv:weights"                              :   "inpaint_net/upsample4/conv2d_transpose/kernel" ,
+            "upsample4_deconv:bias"                                 :   "inpaint_net/upsample4/conv2d_transpose/bias" ,
+            "conv10:weights"                                        :   "inpaint_net/conv10/kernel" ,
+            "conv10:bias"                                           :   "inpaint_net/conv10/bias" ,
+      }
+    
+    num=0
+    net_keys=net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s'%(i, len(net_keys), le, net_keys[i]))
+        # if(net_keys[i] == 'upsample1_deconv'): continue
+        # if(net_keys[i] == 'upsample2_deconv'): continue
+        # if(net_keys[i] == 'upsample3_deconv'): continue
+        # if(net_keys[i] == 'upsample4_deconv'): continue
+        # if(net_keys[i] == 'upsample5_deconv'): continue
+        if le==1:
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value=weights_value.transpose((3,2,0,1)) 
+            else:
+                key0=net_keys[i]+':weights'
+                if "pad" in key0: # for padding, no operation
+                    # print(net.params[net_keys[i]][0].data.shape)
+                    print("pad layer")
+                    net.params[net_keys[i]][0].data[:,:,:,:] = np.array([1.])
+                    continue
+                value0=ct_map[key0]
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1)) 
+            try:
+                net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+            except:
+                net.params[net_keys[i]][0].data[:,:,:,:]=0
+                net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+            
+            
+            print(weights_value.shape,end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+            
+        elif le==2:
+            if "BatchNorm" in net_keys[i]:
+                print("le_last = %d"%le_last)
+                # for BN layers
+                key0=net_keys[i]+':gamma'
+                value0 = ct_map[key0]
+                gamma_value=tensor[value0] 
+
+                key1=net_keys[i]+':beta'
+                value1=ct_map[key1]
+                beta_value=tensor[value1]
+
+                value2 = value0.replace('gamma', 'moving_variance')
+                mv_value=tensor[value2] 
+                print(mv_value)
+
+                value3 = value1.replace('beta', 'moving_mean')
+                mm_value=tensor[value3] 
+                print(mm_value)
+
+                net.params[net_keys[i]][0].data[:] = gamma_value / np.sqrt(mv_value)
+                print(gamma_value.shape,end=' ')            
+                print(net.params[net_keys[i]][0].data.shape)
+                print(gamma_value)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+        
+                net.params[net_keys[i]][1].data[:] = beta_value - mm_value/np.sqrt(mv_value)*gamma_value
+                print(beta_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print(beta_value)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+
+                # key0=net_keys[i-1]+':moving_variance'
+                # value0 = ct_map[key0]
+                # mv_value=tensor[value0] 
+
+                # key1=net_keys[i-1]+':moving_mean'
+                # value1=ct_map[key1]
+                # mm_value=tensor[value1]
+
+                # # mm_value=np.full(mm_value.shape,0)
+                # net.params[net_keys[i-1]][0].data[:] = mm_value
+                # print(mm_value.shape,end=' ')
+                # print(mm_value)
+                # print(net.params[net_keys[i-1]][0].data.shape)
+                # print("%s  <-  %s"%(key0,value0))
+                # num=num+1
+
+                # # mv_value=np.full(mv_value.shape,1)
+                # net.params[net_keys[i-1]][1].data[:] = mv_value
+                # print(mv_value.shape,end=' ')
+                # print(mv_value)
+                # print(net.params[net_keys[i-1]][1].data.shape)
+                # print("%s  <-  %s"%(key1,value1))
+                # num=num+1
+
+                # net.params[net_keys[i-1]][2].data[:] = 1
+                # print(net.params[net_keys[i-1]][2].data.shape)
+                # print("bn  <-  1")
+                # num=num+1
+
+            elif "inverse" in net_keys[i]: # 1 - noise
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+                print("inverse layer")
+                print(net.params[net_keys[i]][0].data[:])
+                print(net.params[net_keys[i]][1].data[:])
+            else:   
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1))
+                    
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value[:,:,:,:]
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value[:,:,:,:]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+#                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+#                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':bias'
+                value1=ct_map[key1]
+                
+                bias_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=bias_value
+                print(bias_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+        # elif le==3:
+        #         key0=net_keys[i]+':moving_variance'
+        #         value0 = ct_map[key0]
+        #         mv_value=tensor[value0]  
+
+        #         key1=net_keys[i]+':moving_mean'
+        #         value1=ct_map[key1]
+        #         mm_value=tensor[value1]
+
+        #         mm_value=np.full(mm_value.shape,0)
+        #         net.params[net_keys[i]][0].data[:] = mm_value
+        #         print(mm_value.shape,end=' ')
+        #         print(mm_value)
+        #         print(net.params[net_keys[i]][0].data.shape)
+        #         print("%s  <-  %s"%(key1,value1))
+        #         num=num+1
+
+        #         mv_value=np.full(mv_value.shape,1)
+        #         net.params[net_keys[i]][1].data[:] = mv_value
+        #         print(mv_value.shape,end=' ')
+        #         print(mv_value)
+        #         print(net.params[net_keys[i]][1].data.shape)
+        #         print("%s  <-  %s"%(key0,value0))
+        #         num=num+1
+
+        #         net.params[net_keys[i]][2].data[:] = 1
+        #         print(net.params[net_keys[i]][2].data.shape)
+        #         num=num+1
+
+        else:
+            print("error: le=%d %s"%(le, net_keys[i]))
+        le_last = le
+    print(num)
+    print('net_keys[0]',net_keys[0])
+    net.params[net_keys[0]][0].data[:] = np.array([1./128, 1./128, 1./128, 1./255, 1./255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0., 0.])
+
+
+
+def print_caffemodel(net):
+    net_keys=net.params.keys()
+    le_last = 0
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le==1:
+            n,c,w,h=net.params[net_keys[i]][0].data.shape
+            print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+            	if 'inverse' in net_keys[i]:
+            	 	w=net.params[net_keys[i]][0].data.shape
+            	 	print("%s:weights ->  [%d]"%(net_keys[i], w[0]))
+            	 	w=net.params[net_keys[i]][1].data.shape
+            	 	print("%s:bias  ->  [%d]"%(net_keys[i], w[0]))
+            	else:
+            	 	# BatchNorm layer后面的scale layer, 训练gamma和deta
+            	 	w=net.params[net_keys[i]][0].data.shape
+            	 	print("%s:gamma ->  [%d]"%(net_keys[i], w[0]))
+            	 	w=net.params[net_keys[i]][1].data.shape
+            	 	print("%s:beta  ->  [%d]"%(net_keys[i], w[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+            
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]"%(net_keys[i],w[0]))
+        elif le==3:
+            #BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]"%(net_keys[i],m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]"%(net_keys[i],v[0]))
+
+            #f = net.params[net_keys[i]][2].data.shape
+            #print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys=tensor.keys()
+    for i in range(0,len(tensor_keys)):
+        le=len(tensor[tensor_keys[i]].shape)
+        if le==1:
+            w=tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]"%(tensor_keys[i],w))
+        elif le==4:
+            weights=tensor[tensor_keys[i]]
+            #if "filter"  in tensor_keys[i]:
+                #weights=tensor[tensor_keys[i]].transpose((2,3,1,0)) 
+            #else:
+                #weights=tensor[tensor_keys[i]].transpose((3,2,1,0)) 
+            n,c,w,h=weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]"%(tensor_keys[i],n,c,w,h))
+
+#print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net('/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test11_5c_test_LRE_woBN_1.prototxt', caffe.TEST)
+    print_caffemodel(net)
+    print("########################################################################################################################")
+    tensor=np.load('inpainting/test11-5c-LRE-0113.npy').item()
+    print_tensorflow(tensor)
+    print("########################################################################################################################")
+    tensor2caffe(net,tensor)
+    net.save("inpainting/test11-5c-LRE-0113.caffemodel")
Index: weights_tensor2caffe_inpainting.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting.py	(date 1569487169000)
+++ weights_tensor2caffe_inpainting.py	(date 1569487169000)
@@ -0,0 +1,295 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+def tensor2caffe(net,tensor):
+
+    ct_map={"conv1-1_conv_f:weights"                              :   "inpaint_net/conv2d/kernel" ,
+            "conv1-1_conv_f:bias"                                 :   "inpaint_net/conv2d/bias" ,
+            "conv1-1_conv_g_1:weights"                            :   "inpaint_net/conv2d_1/kernel" ,
+            "conv1-1_conv_g_1:bias"                               :   "inpaint_net/conv2d_1/bias" ,
+            "conv2_conv_f:weights"                              :   "inpaint_net/conv2d_2/kernel" ,
+            "conv2_conv_f:bias"                                 :   "inpaint_net/conv2d_2/bias" ,
+            "conv2_conv_g_1:weights"                            :   "inpaint_net/conv2d_3/kernel" ,
+            "conv2_conv_g_1:bias"                               :   "inpaint_net/conv2d_3/bias" ,
+            "conv3_conv_f:weights"                              :   "inpaint_net/conv2d_4/kernel" ,
+            "conv3_conv_f:bias"                                 :   "inpaint_net/conv2d_4/bias" ,
+            "conv3_conv_g_1:weights"                            :   "inpaint_net/conv2d_5/kernel" ,
+            "conv3_conv_g_1:bias"                               :   "inpaint_net/conv2d_5/bias" ,
+            "conv4_conv_f:weights"                              :   "inpaint_net/conv2d_6/kernel" ,
+            "conv4_conv_f:bias"                                 :   "inpaint_net/conv2d_6/bias" ,
+            "conv4_conv_g_1:weights"                            :   "inpaint_net/conv2d_7/kernel" ,
+            "conv4_conv_g_1:bias"                               :   "inpaint_net/conv2d_7/bias" ,
+            "conv4-1_conv_f:weights"                            :   "inpaint_net/conv2d_8/kernel" ,
+            "conv4-1_conv_f:bias"                               :   "inpaint_net/conv2d_8/bias" ,
+            "conv4-1_conv_g_1:weights"                          :   "inpaint_net/conv2d_9/kernel" ,
+            "conv4-1_conv_g_1:bias"                             :   "inpaint_net/conv2d_9/bias" ,
+            "block1_conv1_conv_f:weights"                       :   "inpaint_net/block1/conv2d/kernel" ,
+            "block1_conv1_conv_f:bias"                          :   "inpaint_net/block1/conv2d/bias" ,
+            "block1_conv1_conv_g_1:weights"                     :   "inpaint_net/block1/conv2d_1/kernel" ,
+            "block1_conv1_conv_g_1:bias"                        :   "inpaint_net/block1/conv2d_1/bias" ,
+            "block1_conv2_conv_f:weights"                       :   "inpaint_net/block1/conv2d_2/kernel" ,
+            "block1_conv2_conv_f:bias"                          :   "inpaint_net/block1/conv2d_2/bias" ,
+            "block1_conv2_conv_g_1:weights"                     :   "inpaint_net/block1/conv2d_3/kernel" ,
+            "block1_conv2_conv_g_1:bias"                        :   "inpaint_net/block1/conv2d_3/bias" ,
+            "block2_conv1_conv_f:weights"                       :   "inpaint_net/block2/conv2d/kernel" ,
+            "block2_conv1_conv_f:bias"                          :   "inpaint_net/block2/conv2d/bias" ,
+            "block2_conv1_conv_g_1:weights"                     :   "inpaint_net/block2/conv2d_1/kernel" ,
+            "block2_conv1_conv_g_1:bias"                        :   "inpaint_net/block2/conv2d_1/bias" ,
+            "block2_conv2_conv_f:weights"                       :   "inpaint_net/block2/conv2d_2/kernel" ,
+            "block2_conv2_conv_f:bias"                          :   "inpaint_net/block2/conv2d_2/bias" ,
+            "block2_conv2_conv_g_1:weights"                     :   "inpaint_net/block2/conv2d_3/kernel" ,
+            "block2_conv2_conv_g_1:bias"                        :   "inpaint_net/block2/conv2d_3/bias" ,
+            "block3_conv1_conv_f:weights"                       :   "inpaint_net/block3/conv2d/kernel" ,
+            "block3_conv1_conv_f:bias"                          :   "inpaint_net/block3/conv2d/bias" ,
+            "block3_conv1_conv_g_1:weights"                     :   "inpaint_net/block3/conv2d_1/kernel" ,
+            "block3_conv1_conv_g_1:bias"                        :   "inpaint_net/block3/conv2d_1/bias" ,
+            "block3_conv2_conv_f:weights"                       :   "inpaint_net/block3/conv2d_2/kernel" ,
+            "block3_conv2_conv_f:bias"                          :   "inpaint_net/block3/conv2d_2/bias" ,
+            "block3_conv2_conv_g_1:weights"                     :   "inpaint_net/block3/conv2d_3/kernel" ,
+            "block3_conv2_conv_g_1:bias"                        :   "inpaint_net/block3/conv2d_3/bias" ,
+            "block4_conv1_conv_f:weights"                       :   "inpaint_net/block4/conv2d/kernel" ,
+            "block4_conv1_conv_f:bias"                          :   "inpaint_net/block4/conv2d/bias" ,
+            "block4_conv1_conv_g_1:weights"                     :   "inpaint_net/block4/conv2d_1/kernel" ,
+            "block4_conv1_conv_g_1:bias"                        :   "inpaint_net/block4/conv2d_1/bias" ,
+            "block4_conv2_conv_f:weights"                       :   "inpaint_net/block4/conv2d_2/kernel" ,
+            "block4_conv2_conv_f:bias"                          :   "inpaint_net/block4/conv2d_2/bias" ,
+            "block4_conv2_conv_g_1:weights"                     :   "inpaint_net/block4/conv2d_3/kernel" ,
+            "block4_conv2_conv_g_1:bias"                        :   "inpaint_net/block4/conv2d_3/bias" ,
+            "block5_conv1_conv_f:weights"                       :   "inpaint_net/block5/conv2d/kernel" ,
+            "block5_conv1_conv_f:bias"                          :   "inpaint_net/block5/conv2d/bias" ,
+            "block5_conv1_conv_g_1:weights"                     :   "inpaint_net/block5/conv2d_1/kernel" ,
+            "block5_conv1_conv_g_1:bias"                        :   "inpaint_net/block5/conv2d_1/bias" ,
+            "block5_conv2_conv_f:weights"                       :   "inpaint_net/block5/conv2d_2/kernel" ,
+            "block5_conv2_conv_f:bias"                          :   "inpaint_net/block5/conv2d_2/bias" ,
+            "block5_conv2_conv_g_1:weights"                     :   "inpaint_net/block5/conv2d_3/kernel" ,
+            "block5_conv2_conv_g_1:bias"                        :   "inpaint_net/block5/conv2d_3/bias" ,
+            "upsample1_deconv:weights"                     :   "inpaint_net/upsample1/conv2d_transpose/kernel" ,
+            "upsample1_deconv:bias"                        :   "inpaint_net/upsample1/conv2d_transpose/bias" ,
+            "conv5:weights"                                     :   "inpaint_net/conv5/kernel" ,
+            "conv5:bias"                                        :   "inpaint_net/conv5/bias" ,
+            "upsample2_deconv:weights"                     :   "inpaint_net/upsample2/conv2d_transpose/kernel" ,
+            "upsample2_deconv:bias"                        :   "inpaint_net/upsample2/conv2d_transpose/bias" ,
+            "conv6:weights"                                     :   "inpaint_net/conv6/kernel" ,
+            "conv6:bias"                                        :   "inpaint_net/conv6/bias" ,
+            "upsample3_deconv:weights"                     :   "inpaint_net/upsample3/conv2d_transpose/kernel" ,
+            "upsample3_deconv:bias"                        :   "inpaint_net/upsample3/conv2d_transpose/bias" ,
+            "conv7:weights"                                     :   "inpaint_net/conv7/kernel" ,
+            "conv7:bias"                                        :   "inpaint_net/conv7/bias" ,
+            "upsample4_deconv:weights"                     :   "inpaint_net/upsample4/conv2d_transpose/kernel" ,
+            "upsample4_deconv:bias"                        :   "inpaint_net/upsample4/conv2d_transpose/bias" ,
+            "conv8:weights"                                     :   "inpaint_net/conv8/kernel" ,
+            "conv8:bias"                                        :   "inpaint_net/conv8/bias" ,
+            "upsample5_deconv:weights"                     :   "inpaint_net/upsample5/conv2d_transpose/kernel" ,
+            "upsample5_deconv:bias"                        :   "inpaint_net/upsample5/conv2d_transpose/bias" ,
+            "conv9-1:weights"                                   :   "inpaint_net/conv9-1/kernel" ,
+            "conv9-1:bias"                                      :   "inpaint_net/conv9-1/bias" ,
+            "conv9-2:weights"                                   :   "inpaint_net/conv9-2/kernel" ,
+            "conv9-2:bias"                                      :   "inpaint_net/conv9-2/bias" ,
+            "post_conv9-1:weights"                              :   "inpaint_net/post/conv9-1/kernel" ,
+            "post_conv9-1:bias"                                 :   "inpaint_net/post/conv9-1/bias" ,
+            "post_conv10-1:weights"                             :   "inpaint_net/post/conv10-1/kernel" ,
+            "post_conv10-1:bias"                                :   "inpaint_net/post/conv10-1/bias" ,
+            "post_conv10-2:weights"                             :   "inpaint_net/post/conv10-2/kernel" ,
+            "post_conv10-2:bias"                                :   "inpaint_net/post/conv10-2/bias" ,
+            "post_conv10-3:weights"                             :   "inpaint_net/post/conv10-3/kernel" ,
+            "post_conv10-3:bias"                                :   "inpaint_net/post/conv10-3/bias" ,
+      }
+    
+    num=0
+    net_keys=net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s'%(i, len(net_keys), le, net_keys[i]))
+        if le==1:
+            if(net_keys[i] == 'upsample1_deconv'): continue
+            if(net_keys[i] == 'upsample2_deconv'): continue
+            if(net_keys[i] == 'upsample3_deconv'): continue
+            if(net_keys[i] == 'upsample4_deconv'): continue
+            if(net_keys[i] == 'upsample5_deconv'): continue
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value=weights_value.transpose((3,2,0,1)) 
+            else:
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1)) 
+            try:
+                net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+            except:
+                net.params[net_keys[i]][0].data[:,:,:,:]=0
+                net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+            
+            
+            print(weights_value.shape,end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+            
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                print("scale layer!")
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+                continue
+                print("le_last = %d"%le_last)
+                # for BN layers
+                key0=net_keys[i]+':gamma'
+                value0 = ct_map[key0]
+                gamma_value=tensor[value0] 
+
+                key1=net_keys[i]+':beta'
+                value1=ct_map[key1]
+                beta_value=tensor[value1]
+
+                value2 = value0.replace('gamma', 'moving_variance')
+                mv_value=tensor[value2] 
+
+                value3 = value1.replace('beta', 'moving_mean')
+                mm_value=tensor[value3] 
+
+
+                net.params[net_keys[i]][0].data[:] = gamma_value / np.sqrt(mv_value)
+                print(gamma_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                net.params[net_keys[i]][1].data[:] = beta_value - mm_value/np.sqrt(mv_value)*gamma_value
+                print(beta_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+            else:   
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                
+                weights_value=tensor[value0]
+                if "filter"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1))
+                    
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value[:,:,:,:]
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value[:,:,:,:]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+#                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+#                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':bias'
+                value1=ct_map[key1]
+                
+                bias_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=bias_value
+                print(bias_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+    print(num)
+    net.params[net_keys[0]][0].data[:] = np.array([1./128, 1./128, 1./128, 1./255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0.])
+
+
+
+def print_caffemodel(net):
+    net_keys=net.params.keys()
+    le_last = 0
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le==1:
+            n,c,w,h=net.params[net_keys[i]][0].data.shape
+            print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                # BatchNorm layer后面的scale layer, 训练gamma和deta
+                w=net.params[net_keys[i]][0].data.shape
+                print("%s:gamma ->  [%d]"%(net_keys[i], w[0]))
+
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:beta  ->  [%d]"%(net_keys[i], w[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+            
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]"%(net_keys[i],w[0]))
+        elif le==3:
+            #BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]"%(net_keys[i],m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]"%(net_keys[i],v[0]))
+
+            #f = net.params[net_keys[i]][2].data.shape
+            #print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys=tensor.keys()
+    for i in range(0,len(tensor_keys)):
+        le=len(tensor[tensor_keys[i]].shape)
+        if le==1:
+            w=tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]"%(tensor_keys[i],w))
+        elif le==4:
+            weights=tensor[tensor_keys[i]]
+            #if "filter"  in tensor_keys[i]:
+                #weights=tensor[tensor_keys[i]].transpose((2,3,1,0)) 
+            #else:
+                #weights=tensor[tensor_keys[i]].transpose((3,2,1,0)) 
+            n,c,w,h=weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]"%(tensor_keys[i],n,c,w,h))
+
+#print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net('/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test_eden_post_6c_1.prototxt', caffe.TEST)
+    print_caffemodel(net)
+    print("########################################################################################################################")
+    tensor=np.load('inpainting/test_eden.npy').item()
+    print_tensorflow(tensor)
+    tensor2caffe(net,tensor)
+    net.save("inpainting/test_eden_post_6c_1.caffemodel")
Index: SaveProtxt_inpainting0923.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting0923.py	(date 1569484793000)
+++ SaveProtxt_inpainting0923.py	(date 1569484793000)
@@ -0,0 +1,435 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1)
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name):
+    convs_depthwise = convs(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, group=num_filter, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+# v7 2gateCon
+def gen_gate_conv_1(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=caffe.params.ReLU, training=True):
+    print('gen_gate_conv_1 (%s) start...'%name)
+    padding = ksize // 2 * rate
+    #conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+"_conv_f")
+    # out_f = leaky_relu(conv_f)
+    out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+
+    # # Soft 5*5 gating
+    # conv_g5 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g5 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g5")
+    # conv_g5 = leaky_relu(conv_g5)
+    conv_g5 = leakyReLU(conv_g5, name = name+"_conv_g5_ReLU")
+    # conv_g5 = tf.layers.separable_conv2d(inputs=conv_g5, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g5 = separable_convs(data = conv_g5, num_filter = cnum, kernel = ksize, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g5_separable")
+    # conv_g5 = tf.nn.sigmoid(conv_g5)
+    conv_g5 = caffe.layers.Sigmoid(conv_g5, in_place = True, name = name+"_conv_g5_separable_Sigmoid")
+
+    # # Soft 3*3 gating
+    padding = 3 // 2 * rate
+    # conv_g3 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g3 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g3")
+    # conv_g3 = leaky_relu(conv_g3)
+    conv_g3 = leakyReLU(conv_g3, name = name+"_conv_g3_ReLU")
+    # conv_g3 = tf.layers.separable_conv2d(inputs=conv_g3, filters=cnum, kernel_size=3, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g3 = separable_convs(data = conv_g3, num_filter = cnum, kernel = 3, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g3_separable")
+    # conv_g3 = tf.nn.sigmoid(conv_g3)
+    conv_g3 = caffe.layers.Sigmoid(conv_g3, in_place = True, name = "_conv_g3_separable_Sigmoid")
+
+    # # Soft 1*1 gating
+    # conv_g1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g1 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g1")
+    # conv_g1 = leaky_relu(conv_g1)
+    conv_g1 = leakyReLU(conv_g1, name = name + "_conv_g1_ReLU")
+    # conv_g1 = tf.layers.separable_conv2d(inputs=conv_g1, filters=cnum, kernel_size=1, padding='same',  strides=stride, dilation_rate=1)
+    conv_g1 = separable_convs(data = conv_g1, num_filter = cnum, kernel = 1, stride = stride, pad = 0, dilation = 1, name = name+"_conv_g1_separable")
+    # conv_g1 = tf.nn.tanh(conv_g1)
+    conv_g1 = caffe.layers.TanH(conv_g1, in_place = True, name = name + "_conv_g1_separable_TanH")
+
+    # # concat the gating
+    # conv_g = tf.concat([conv_g5, conv_g3], axis=3)
+    conv_g = caffe.layers.Concat(conv_g5, conv_g3, axis = 1)
+    # conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = conv(data = conv_g, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g")
+    # conv_g = tf.nn.sigmoid(conv_g)
+    conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+    # # Elementwise Multiply
+    # res_mul = tf.multiply(out_f, conv_g)
+    res_mul = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")#'PROD'-0,'SUM'-1,'MAX'-2
+    # x = tf.add(res_mul, conv_g1)
+    x = caffe.layers.Eltwise(res_mul, conv_g1, name = name+"_add")
+    print('gen_gate_conv_1 end!')
+    return x
+
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.sigmoid(conv_f)
+    out_f = caffe.layers.Sigmoid(conv_f, in_place = True, name = name + '_out_f')
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_g_1')
+    # conv_g_1 = leaky_relu(conv_g_1)
+    # conv_g = tf.nn.sigmoid(conv_g_1)
+    conv_g = caffe.layers.Sigmoid(conv_g_1, in_place = True, name = name + '_out_g')
+    # conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=3, strides=1, padding='same')
+    # conv_g = tf.nn.sigmoid(conv_g_2)
+    # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")
+    return x
+
+def gate_res_block(x, cnum, name):
+    print("gate_res_block (%s) start ..."%name)
+    x_1 = gen_gate_conv(x, cnum = cnum, ksize=3, name=name+'_conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_BatchNorm")#测试时true，训练时false
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+    #x_1 = leaky_relu(x_1)
+    x_1 = leakyReLU(x_1, name = "_conv1_BatchNorm_Scale_ReLU")
+
+    x_1 = gen_gate_conv(x_1, cnum = cnum, ksize=3, name=name+'_conv2')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_BatchNorm")
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_BatchNorm_Scale")
+
+    #out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    #out = leaky_relu(out)
+    out = leakyReLU(out, name = name+"_out")
+    print("gate_res_block end!")
+    return out
+    
+def resnetblock(x, cnum, name = 'res'):
+    x_1 = gen_gate_conv(x, cnum, 3, name = name + '_conv1')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn1')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_bn1")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_bn1_Scale")#BatchNorm+Scale实现
+    # x_1 = tf.nn.relu(x_1)
+    x_1 = caffe.layers.ReLU(x_1, in_place = True, name = name + '_conv1_bn1_Scale_ReLU')
+    x_1 = gen_gate_conv(x_1, cnum, 3, name= name + '_conv2')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn2')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_bn2")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_bn2_Scale")#BatchNorm+Scale实现
+    # out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    return out
+
+def gen_layer_deconv(x, cnum, kernel_size = 3, stride = 2, name='upsample', pad='same', training=True):
+    # x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose')
+    k = kernel_size + 1
+    padding = k // 2 - 1
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=k,stride=2,pad=padding,bias_term=True), name=name+"_deconv")
+    # x = tf.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = name+"_deconv_Sigmoid")
+    return x
+
+def gen_layer_gate_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', pad='same', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data =caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.relu(data)
+    data = caffe.layers.ReLU(data, in_place = True, name = name+"_deconv_ReLU")
+
+    # data_gate = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv_gate")
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place = True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name+'_Eltwise')
+    return x
+
+def gen_deconv2(x, cnum, stride=1, name='upsample', padding='SAME', training=True):
+    data = deconv2(data=x, num_filter=cnum, kernel=3, stride=2, pad=1, name=name+'_dconv')
+    return data
+
+def gen_deconv_1(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    #x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding='same')
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    #x = tf.nn.relu(x)
+    x = caffe.layers.ReLU(x, in_place = True, name = name + '_deconv_ReLU')
+    return x
+
+def gen_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    # filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+    # data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], padding)
+    # data = tf.nn.bias_add(data, bias)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+'_deconv')
+    # data = leaky_relu(data, alpha=0.1)
+    data = leakyReLU(data, name = name + '_deconv_ReLU', alpha=0.1)
+    # data = caffe.layers.ReLU(data, in_place=True, name=name+"_deconv_ReLU")
+
+    # filt_gate = tf.get_variable('conv2d_transpose_gate/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias_gate = tf.get_variable('conv2d_transpose_gate/bias', [cnum], tf.float32)
+    # data_gate = tf.nn.conv2d_transpose(x, filt_gate, [b, scale_size * w, scale_size * h, c], [1, stride, stride, 1], padding)
+    # data_gate = tf.nn.bias_add(data_gate, bias_gate)
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum,kernel_size=4,stride=2,pad=1,bias_term=True),name=name+'_deconv_gate')
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place=True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name + '_Eltwise')
+    return x
+
+
+def resize_conv(x, cnum, kernel_size=3,stride=1, name='upsample', padding='SAME', training=True):
+    # x = resize(x, func=tf.image.resize_bilinear)
+    factor = 2
+    k = 2 * factor - factor % 2
+    p = int(math.ceil((factor - 1) / 2.))
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, group=cnum,kernel_size=k,stride=2,pad=p,weight_filler={"type": "bilinear"},bias_term=False), name=name+"_deconv")
+    # x = tf.layers.conv2d(
+    #     x, cnum, kernel_size=kernel_size, strides=(stride, stride),
+    #     padding=padding, trainable=training,
+    #     name=name + '_dconv'
+    # )
+    p = kernel_size // 2
+    x = conv(data = x, num_filter = cnum, kernel=kernel_size, stride=stride, pad=p, dilation=1, name=name+"_deconv_conv")
+    return x
+
+def gen_normal_layers_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_Sigmoid')
+    return data
+
+def getNoise(offset_h, offset_w, width, height):
+    noiseMask = np.random.rand(height, width)
+    if offset_h and offset_w:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    elif offset_h:
+        noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+    elif offset_w:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    return noiseMask1, noiseMask2
+
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    input_x, noise = caffe.layers.Slice(net.data, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    input_x = caffe.layers.Scale(input_x, bias_term = True, name = "input_x_Scale")
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1-1')
+    x_2 = gen_gate_conv(x_1, 2 * cnum, 5, 2, name='conv2')
+    x_3 = gen_gate_conv(x_2, 4 * cnum, 3, 2, name='conv3')
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 3, 2, name='conv4')
+    x_5 = gen_gate_conv(x_4, 4 * cnum, 3, 2, name='conv4-1')
+
+    x = resnetblock(x_5, 4 * cnum, name='block1')
+    x = resnetblock(x, 4 * cnum, name='block2')
+    x = resnetblock(x, 4 * cnum, name='block3')
+    x = resnetblock(x, 4 * cnum, name='block4')
+    x = resnetblock(x, 4 * cnum, name='block5')
+    # x = gate_res_block(x, 8 * cnum, name='block6')
+    # x = gate_res_block(x, 8 * cnum, name='block7')
+    # x = gate_res_block(x, 8 * cnum, name='block8')
+
+    x = gen_normal_layers_deconv(x, 4 * cnum, kernel_size=4, pad=1 ,name='upsample1')
+    # x = tf.concat([x, x_4], axis=3, name='concat1')
+    x = caffe.layers.Concat(x, x_4, axis = 1, name='concat1')
+    # x = tf.layers.conv2d(x, 4 * cnum, 3, 1, padding='same', name='conv5')
+    x = conv(data = x, num_filter = 4*cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv5')
+
+    x = gen_normal_layers_deconv(x, 4*cnum, kernel_size=4, name='upsample2')
+    # x = tf.concat([x, x_3], axis=3, name='concat2')
+    x = caffe.layers.Concat(x, x_3, axis = 1, name = 'concat2')
+    # x = tf.layers.conv2d(x, 2*cnum, 3, 1, padding='same',name='conv6')
+    x = conv(data = x, num_filter = 2*cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv6')
+
+    x = gen_normal_layers_deconv(x, 2*cnum, kernel_size=4, name='upsample3')
+    # x = tf.concat([x, x_2], axis=3, name='concat3')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'concat3')
+    # x = tf.layers.conv2d(x, cnum, 3, 1, padding='same',name='conv7')
+    x = conv(data = x, num_filter = cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv7')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = "conv7_Sigmoid")
+
+    x = gen_normal_layers_deconv(x, cnum, kernel_size=4, name='upsample4')
+    # x = tf.concat([x, x_1], axis=3, name='concat8')
+    x = caffe.layers.Concat(x, x_1, axis=1, name='concat8')
+    # x = tf.layers.conv2d(x, cnum, 3, 1, padding='same', name='conv8')
+    x = conv(data = x, num_filter = cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv8')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = 'conv8_Sigmoid')
+
+    x = gen_normal_layers_deconv(x, cnum, kernel_size=4, name='upsample5')
+    # x = tf.concat([x, input_x], axis=3, name='concat9')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    # x = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9-1')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name='conv9-1_Sigmoid')
+
+    # x = tf.concat([x, noise], axis=3, name='noise')
+    # x = caffe.layers.Concat(x, noise, axis=1, name = 'noise')
+    # x = tf.layers.conv2d(x, 3, 5, 1, padding='same',name='conv9-2')
+    x = conv(data = x, num_filter = 3, kernel=5, stride=1, pad=2, dilation=1, name='conv9-2')
+    # x_1 = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x_1 = conv(data = x, num_filter = 3, kernel = 3, stride = 1, pad = 1, dilation = 1, name = 'post_conv9-1')
+    # x_1 = tf.nn.relu(x_1)
+    # x_2 = tf.layers.conv2d(x, 3, 1, 1, padding='same', name='conv10-1')
+    x_2 = conv(data = x, num_filter = 3, kernel = 1, stride = 1, pad = 0, dilation = 1, name = 'post_conv10-1')
+    # x_2 = tf.nn.relu(x_2)
+    x_2 = caffe.layers.ReLU(x_2, in_place = True, name = 'post_conv10-1_Relu')
+    # x_2 = tf.layers.conv2d(x_2, 3, 3, 1, padding='same', name='conv10-2')
+    x_2 = conv(data = x_2, num_filter = 3, kernel = 3, stride = 1, pad = 1, dilation = 1, name = 'post_conv10-2')
+    # x_2 = tf.nn.relu(x_2)
+    x_2 = caffe.layers.ReLU(x_2, in_place = True, name = 'post_conv10-2_Relu')
+    # x_2 = tf.layers.conv2d(x_2, 3, 1, 1, padding='same', name='conv10-3')
+    x_2 = conv(data = x_2, num_filter = 3, kernel = 1, stride = 1, pad = 0, dilation = 1, name = 'post_conv10-3')
+    # x_2 = tf.nn.sigmoid(x_2)
+    x_2 = caffe.layers.Sigmoid(x_2, in_place = True, name = 'post_conv10-3_Sigmoid')
+    # x = tf.multiply(x_1, x_2, name='conv9-2')
+    x = caffe.layers.Eltwise(x_1, x_2, operation=0, name = "post_conv9-2")
+
+    #post
+    height = 512
+    width = height
+    # noise = caffe.io.load_image(caffe_root + "noise_512.jpg",  False)
+    # net.noise = caffe.layers.ImageData(name="noise",source="noise.txt", batch_size=1, new_width=512, new_height=512, 
+    #     ntop=1,is_color=True,root_folder='/inpainting/')
+    print str(net.to_proto())
+    # noise1_m0, noise1_m1 = caffe.layers.Slice(noise, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[1]))
+    _x1, noise1 = caffe.layers.Slice(noise, ntop=2, name='slice_noise1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    _x2, noise1 = caffe.layers.Slice(noise1, ntop=2, name='slice_noise1-2', slice_param=dict(slice_dim=3, slice_point=[1]))
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = "_noise2_Scale")
+    # noise1 = caffe.layers.Tile(noise1, name='noise1_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise1_c1,noise1_c2, noise1_c3 = caffe.layers.Split(noise1, ntop=3)
+    noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    # noise1_1 = caffe.layers.Tile(noise1_1, name='noise1_1_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise1_1_c1,noise1_1_c2,noise1_1_c3 = caffe.layers.Split(noise1_1, ntop=3)
+    noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)
+    _x3, noise2 = caffe.layers.Slice(noise, ntop=2, name='slice_noisem1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    _x4, noise2 = caffe.layers.Slice(noise2, ntop=2, name='slice_noisem1-2', slice_param=dict(slice_dim=3, slice_point=[2]))
+    noise2_1 = caffe.layers.Scale(noise2, bias_term = True, name = "_noise2_1_Scale")
+    # noise2 = caffe.layers.Tile(noise2, name='noise2_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise2_c1,noise2_c2,noise2_c3 = caffe.layers.Split(noise2, ntop=3)
+    noise2 = caffe.layers.Concat(noise2, noise2, noise2)
+    # noise2_1 = caffe.layers.Tile(noise2_1, name='noise2_1_tile', tile_param=dict(axis=1, tiles = 3))
+    # noise2_1_c1,noise2_1_c2,noise2_1_c3 = caffe.layers.Split(noise2_1, ntop=3)
+    noise2_1 = caffe.layers.Concat(noise2_1, noise2_1, noise2_1)
+
+    _x5, x1 = caffe.layers.Slice(x, ntop=2, name='slice1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    x1, _x6 = caffe.layers.Slice(x1, ntop=2, name='slice1-2', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # x1_c1, x1_c2, x1_c3 = caffe.layers.Slice(x1, ntop=3, name='slice1-3', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    # res1_c1 = caffe.layers.Eltwise(x1_c1, noise1, operation=0, name = "x1Multi")
+    # res1_c2 = caffe.layers.Eltwise(x1_c2, noise1, operation=0, name = "x1Multi")
+    # res1_c3 = caffe.layers.Eltwise(x1_c3, noise1, operation=0, name = "x1Multi")
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = "x1Multi")
+
+    x2, _x7 = caffe.layers.Slice(x, ntop=2, name='slice2-1', slice_param=dict(slice_dim=2, slice_point=[height-2]))
+    _x8, x2 = caffe.layers.Slice(x2, ntop=2, name='slice2-2', slice_param=dict(slice_dim=3, slice_point=[1]))
+    # x2_c1, x2_c2, x2_c3 = caffe.layers.Slice(x2, ntop=3, name='slice1-3', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    # res2_c1 = caffe.layers.Eltwise(x2_c1, noise1_1, operation=0, name = "x1Multi")
+    # res2_c2 = caffe.layers.Eltwise(x2_c2, noise1_1, operation=0, name = "x1Multi")
+    # res2_c3 = caffe.layers.Eltwise(x2_c3, noise1_1, operation=0, name = "x1Multi")
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name="x2Multi")
+    res = caffe.layers.Eltwise(res1, res2, name="x1Multix2")
+
+    _x9, x1 = caffe.layers.Slice(res, ntop=2, name='slice3', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res1 = caffe.layers.Eltwise(x1, noise2, operation=0, name="x1Multi_1")
+
+    x2, _x10 = caffe.layers.Slice(res, ntop=2, name='slice4', slice_param=dict(slice_dim=3, slice_point=[width-2]))
+    res2 = caffe.layers.Eltwise(x2, noise2_1, operation=0, name="x2Multi_1")
+
+    res = caffe.layers.Eltwise(res1, res2, name="x1Multix2_1")
+
+    # Gauss blur
+    dt_b, dt_g, dt_r = caffe.layers.Slice(res, ntop=3, name='slice5', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    dt_b = convs(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convB_1')
+    dt_b = convs(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convB_2')
+    dt_b = convs(data=dt_b, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_b')
+
+    dt_g = convs(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convG_1')
+    dt_g = convs(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convG_2')
+    dt_g = convs(data=dt_g, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_g')
+
+    dt_r = convs(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convR_1')
+    dt_r = convs(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convR_2')
+    dt_r = convs(data=dt_r, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_r')
+
+    x = caffe.layers.Concat(dt_b, dt_g, dt_r, axis=1, name = 'concat_rgb')
+
+    net.output = x
+    _x_2_512 = caffe.layers.Concat(_x1, _x3, _x5, _x7, axis=1, name='concat_2_512')
+    _x_510_1 = caffe.layers.Concat(_x2, _x6, _x8, _x9, _x10, axis=1, name='concat_510_1')
+    _x_2_512 = caffe.layers.Reshape(_x_2_512,name='reshape_2_512')
+    _x_510_1 = caffe.layers.Reshape(_x_510_1,name='reshape_510_1')
+    _x_510_2 = caffe.layers.Reshape(_x4,name='reshape_510_2')
+    _x = caffe.layers.Concat(_x_2_512,_x_510_1,_x_510_2, axis=1, name='concat_others')
+    net.output1 = _x
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test_eden_post.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: run_hed_inpainting_6c.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- run_hed_inpainting_6c.py	(date 1579682210000)
+++ run_hed_inpainting_6c.py	(date 1579682210000)
@@ -0,0 +1,480 @@
+import numpy as np
+import matplotlib.pyplot as plt
+import matplotlib.pylab as pylab
+import matplotlib.cm as cm
+import scipy.misc
+from datetime import datetime
+from PIL import Image
+import scipy.io
+import os
+import sys
+# caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/'
+# caffe_root = '/home/bo718.wang/sl1015.liu/caffe_RCNN_Mobile_Shuffe_HED/caffe/'
+# sys.path.insert(0, caffe_root + 'python')
+import caffe
+from skimage import io, transform
+import random
+import cv2
+
+
+# remove the following two lines if testing with cpu
+# caffe.set_mode_gpu()
+# caffe.set_device(7)
+
+# tensor=np.load('weights.npy').item()
+def test1(im0, mask0, save_name):
+    print(im0.shape, mask0.shape)
+    s0 = im0.shape
+    sc = 512. / max(im0.shape)
+    im = transform.rescale(im0, sc)
+    mask = transform.rescale(mask0, sc)
+    mask[mask >= 0.1] = 1.0
+    mask[mask < 0.1] = 0.0
+    # guide = transform.rescale(guide0, sc)
+    # guide[guide >= 0.1] = 1.0
+    # guide[guide < 0.1] = 0.0
+    im = im * 2.0 - 1.0
+    im = im * (1. - mask)
+    # io.imsave(model_root + "mask_sc.jpg", mask)
+    # io.imsave(model_root + "guide_sc.jpg", guide)
+    # image_incomplete = transform.resize(im, s0)
+    # io.imsave(model_root + "test_incomplete_sc.jpg", im)
+    # print(image_incomplete.shape)
+    # im *= 255
+    im = im[..., ::-1]  # RGB->BGR
+    print(im.shape)
+    canvas = np.zeros([512, 512, 5], np.float32)
+    canvas[:im.shape[0], :im.shape[1], 0:3] = im[:, :, :]
+    canvas[:im.shape[0], :im.shape[1], 3] = 1
+    canvas[:im.shape[0], :im.shape[1], 4] = mask[:, :, 0]
+    # canvas[:im.shape[0], :im.shape[1], 5] = 0
+    # im[:] = 1
+    canvas = np.transpose(canvas, [2, 0, 1])
+    canvas = np.expand_dims(canvas, 0)
+    print(canvas.shape)
+    net.blobs['data'].data[...] = canvas
+
+    # run net and take argmax for prediction
+    d1 = datetime.now()
+    net.forward()
+    fuse = net.blobs['output'].data[0][:, :, :]
+    print(fuse.shape)
+    fuse = np.transpose(fuse, [1, 2, 0])
+    print(fuse.shape)
+    fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+    fuse += 1
+    fuse /= 2
+    fuse_im = transform.resize(fuse, s0)
+    new_im = np.zeros_like(im0)
+    new_im[:, :, 0] = (255 * fuse_im[:, :, 2]).astype(np.uint8)  # RGB->BGR
+    new_im[:, :, 1] = (255 * fuse_im[:, :, 1]).astype(np.uint8)
+    new_im[:, :, 2] = (255 * fuse_im[:, :, 0]).astype(np.uint8)
+    print(new_im.shape)
+    # new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+    # new_im = new_im.astype(np.uint8)
+    io.imsave(model_root + save_name, new_im)
+
+
+def test2(im0, mask0, noise0, save_name):
+    print(im0.shape, mask0.shape)
+    s0 = im0.shape
+    sc = 512. / max(im0.shape)
+    im = transform.rescale(im0, sc)
+    mask = transform.rescale(mask0, sc)
+    mask[mask >= 0.1] = 1.0
+    mask[mask < 0.1] = 0.0
+    noise = noise0 / 255.
+    # guide = transform.rescale(guide0, sc)
+    # guide[guide >= 0.1] = 1.0
+    # guide[guide < 0.1] = 0.0
+    im = im * 2.0 - 1.0
+    im = im * (1. - mask)
+    # io.imsave(model_root + "mask_sc.jpg", mask)
+    # io.imsave(model_root + "guide_sc.jpg", guide)
+    # image_incomplete = transform.resize(im, s0)
+    # io.imsave(model_root + "test_incomplete_sc.jpg", im)
+    # print(image_incomplete.shape)
+    # im *= 255
+    im = im[..., ::-1]  # RGB->BGR
+    print(im.shape)
+    canvas = np.zeros([512, 512, 6], np.float32)
+    canvas[:im.shape[0], :im.shape[1], 0:3] = im[:, :, :]
+    canvas[:im.shape[0], :im.shape[1], 3] = 1
+    canvas[:im.shape[0], :im.shape[1], 4] = mask[:, :, 0]
+    canvas[:im.shape[0], :im.shape[1], 5] = noise[:, :, 0]  # random.random()
+    # im[:] = 1
+    canvas = np.transpose(canvas, [2, 0, 1])
+    canvas = np.expand_dims(canvas, 0)
+    print(canvas.shape)
+    net.blobs['data'].data[...] = canvas
+
+    # run net and take argmax for prediction
+    d1 = datetime.now()
+    net.forward()
+    fuse = net.blobs['output'].data[0][:, :, :]
+    print(fuse.shape)
+    fuse = np.transpose(fuse, [1, 2, 0])
+    print(fuse.shape)
+    fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+    fuse += 1
+    fuse /= 2
+    fuse_im = transform.resize(fuse, s0)
+    new_im = np.zeros_like(im0)
+    new_im[:, :, 0] = (255 * fuse_im[:, :, 2])  # RGB->BGR
+    new_im[:, :, 1] = (255 * fuse_im[:, :, 1])
+    new_im[:, :, 2] = (255 * fuse_im[:, :, 0])
+    print(new_im.shape)
+    # new_im = im0 * (1. - mask) + new_im * mask
+    new_im = new_im.astype(np.uint8)
+    # new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+    # new_im = new_im.astype(np.uint8)
+    io.imsave(model_root + save_name, new_im)
+
+
+# processMask --- input data range is [-1,1]
+def test3(im0, mask0, noise0, save_name):
+    print(im0.shape, mask0.shape)
+    s0 = im0.shape
+    sc = 512. / max(im0.shape)
+    im = transform.rescale(im0, sc)
+    mask = transform.rescale(mask0, sc)
+    blendMask = mask
+    mask = cv2.GaussianBlur(mask, ksize=(25, 25), sigmaX=0, sigmaY=0)
+    # blendMask = cv2.
+    mask[mask >= 0.1] = 1.0
+    mask[mask < 0.1] = 0.0
+    noise = noise0 / 255.
+    # guide = transform.rescale(guide0, sc)
+    # guide[guide >= 0.1] = 1.0
+    # guide[guide < 0.1] = 0.0
+    im = im * 2.0 - 1.0
+    im = im * (1. - mask)
+    # io.imsave(model_root + "mask_sc.jpg", mask)
+    # io.imsave(model_root + "guide_sc.jpg", guide)
+    # image_incomplete = transform.resize(im, s0)
+    # io.imsave(model_root + "test_incomplete_sc.jpg", im)
+    # print(image_incomplete.shape)
+    # im *= 255
+    im = im[..., ::-1]  # RGB->BGR
+    print(im.shape)
+    canvas = np.zeros([512, 512, 6], np.float32)
+    canvas[:im.shape[0], :im.shape[1], 0:3] = im[:, :, :]
+    canvas[:im.shape[0], :im.shape[1], 3] = 1
+    canvas[:im.shape[0], :im.shape[1], 4] = mask[:, :, 0]
+    canvas[:im.shape[0], :im.shape[1], 5] = noise[:, :, 0]  # random.random()
+    # im[:] = 1
+    canvas = np.transpose(canvas, [2, 0, 1])
+    canvas = np.expand_dims(canvas, 0)
+    print(canvas.shape)
+    net.blobs['data'].data[...] = canvas
+
+    # run net and take argmax for prediction
+    d1 = datetime.now()
+    net.forward()
+    fuse = net.blobs['output'].data[0][:, :, :]
+    print(fuse.shape)
+    fuse = np.transpose(fuse, [1, 2, 0])
+    print(fuse.shape)
+    fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+    fuse += 1
+    fuse /= 2
+    fuse_im = transform.resize(fuse, s0)
+    new_im = np.zeros_like(im0)
+    new_im[:, :, 0] = (255 * fuse_im[:, :, 2])  # RGB->BGR
+    new_im[:, :, 1] = (255 * fuse_im[:, :, 1])
+    new_im[:, :, 2] = (255 * fuse_im[:, :, 0])
+    print(new_im.shape)
+    # io.imsave(model_root + "mask_blurred.jpg", blendMask)
+    # new_im = cv2.warpAffine(new_im,)
+    blendMask[blendMask <= 0.2] = 0.0
+    # blendMask -= 0.2
+    # blendMask /= 0.8
+    new_im = im0 * (1. - blendMask) + new_im * blendMask
+    new_im = new_im.astype(np.uint8)
+    # new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+    # new_im = new_im.astype(np.uint8)
+    io.imsave(model_root + save_name, new_im)
+
+
+# input data range is [-1,1]
+def test_4c(im0, mask0, save_name):
+    print(im0.shape, mask0.shape)
+    s0 = im0.shape
+    sc = 512. / max(im0.shape)
+    im = transform.rescale(im0, sc)
+    mask = transform.rescale(mask0, sc)
+    mask[mask >= 0.1] = 1.0
+    mask[mask < 0.1] = 0.0
+    im = im * 2.0 - 1.0
+    im = im * (1. - mask)
+    im = im[..., ::-1]  # RGB->BGR
+    print(im.shape)
+    canvas = np.zeros([512, 512, 4], np.float32)
+    canvas[:im.shape[0], :im.shape[1], 0:3] = im[:, :, :]
+    canvas[:im.shape[0], :im.shape[1], 3] = mask[:, :, 0]
+    # im[:] = 1
+    canvas = np.transpose(canvas, [2, 0, 1])
+    canvas = np.expand_dims(canvas, 0)
+    print(canvas.shape)
+    net.blobs['data'].data[...] = canvas
+
+    # run net and take argmax for prediction
+    d1 = datetime.now()
+    net.forward()
+    fuse = net.blobs['output'].data[0][:, :, :]
+    print(fuse.shape)
+    fuse = np.transpose(fuse, [1, 2, 0])
+    print(fuse.shape)
+    fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+    fuse += 1
+    fuse /= 2
+    fuse_im = transform.resize(fuse, s0)
+    new_im = np.zeros_like(im0)
+    new_im[:, :, 0] = (255 * fuse_im[:, :, 2])  # RGB->BGR
+    new_im[:, :, 1] = (255 * fuse_im[:, :, 1])
+    new_im[:, :, 2] = (255 * fuse_im[:, :, 0])
+    print(new_im.shape)
+    # new_im = im0 * (1. - mask) + new_im * mask
+    new_im = new_im.astype(np.uint8)
+    # new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+    # new_im = new_im.astype(np.uint8)
+    io.imsave(model_root + save_name, new_im)
+
+
+# input data range is [0,255]
+def test_4c_uc(im0, mask0, save_name):
+    print(im0.shape, mask0.shape)
+    s0 = im0.shape
+    sc = 512. / max(im0.shape)
+    im = transform.rescale(im0, sc)
+    mask = transform.rescale(mask0, sc)
+    mask[mask >= 0.1] = 1.0
+    mask[mask < 0.1] = 0.0
+    im = im[..., ::-1]  # RGB->BGR
+    im *= 2.0
+    im -= 1.0
+    im = im * (1. - mask)
+    im += 1.0
+    im /= 2.0
+    im *= 255
+    mask *= 255
+    print(im.shape)
+    canvas = np.zeros([512, 512, 4], np.float32)
+    canvas[:im.shape[0], :im.shape[1], 0:3] = im[:, :, :]
+    canvas[:im.shape[0], :im.shape[1], 3] = mask[:, :, 0]
+    # im[:] = 1
+    canvas = np.transpose(canvas, [2, 0, 1])
+    canvas = np.expand_dims(canvas, 0)
+    print(canvas.shape)
+    net.blobs['input'].data[...] = canvas
+
+    # run net and take argmax for prediction
+    d1 = datetime.now()
+    net.forward()
+    fuse = net.blobs['output'].data[0][:, :, :]
+    print(fuse.shape)
+    fuse = np.transpose(fuse, [1, 2, 0])
+    print(fuse.shape)
+    fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+    fuse += 1
+    fuse /= 2
+    fuse_im = transform.resize(fuse, s0)
+    new_im = np.zeros_like(im0)
+    new_im[:, :, 0] = (255 * fuse_im[:, :, 2])  # RGB->BGR
+    new_im[:, :, 1] = (255 * fuse_im[:, :, 1])
+    new_im[:, :, 2] = (255 * fuse_im[:, :, 0])
+    print(new_im.shape)
+    # new_im = im0 * (1. - mask) + new_im * mask
+    new_im = new_im.astype(np.uint8)
+    # new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+    # new_im = new_im.astype(np.uint8)
+    io.imsave(model_root + save_name, new_im)
+
+
+def show_data(data, padsize=1, padval=0):
+    data -= data.min()
+    data /= data.max()
+
+    # force the number of filters to be square
+    n = int(np.ceil(np.sqrt(data.shape[0])))
+    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)
+    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))
+
+    # tile the filters into an image
+    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))
+    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])
+    plt.figure()
+    plt.imshow(data, cmap='gray')
+    plt.axis('off')
+
+
+# input data range is [0,255]
+def test_5c_uc(im0, mask0, save_name):
+    print(im0.shape, mask0.shape)
+    s0 = im0.shape
+    sc = 512. / max(im0.shape)
+    im = transform.rescale(im0, sc)
+    mask = transform.rescale(mask0, sc)
+    mask[mask >= 0.1] = 1.0
+    mask[mask < 0.1] = 0.0
+    im = im[..., ::-1]  # RGB->BGR
+    im *= 2.0
+    im -= 1.0
+    im = im * (1. - mask)
+    im += 1.0
+    im /= 2.0
+    im *= 255
+    mask *= 255
+    noise = np.random.rand(512, 512)
+    noise *= 255
+    # noise = (noise[:, :] > 0.5).astype(np.float32)
+    print(im.shape)
+    canvas = np.zeros([512, 512, 5], np.float32)
+    canvas[:im.shape[0], :im.shape[1], 0:3] = im[:, :, :]
+    canvas[:im.shape[0], :im.shape[1], 3] = mask[:, :, 0]
+    canvas[:im.shape[0], :im.shape[1], 4] = noise
+    # im[:] = 1
+    canvas = np.transpose(canvas, [2, 0, 1])
+    canvas = np.expand_dims(canvas, 0)
+    print(canvas.shape)
+    net.blobs['input'].data[...] = canvas
+
+    # run net and take argmax for prediction
+    d1 = datetime.now()
+    net.forward()
+
+    # print(net.params[''][1].data)
+    # print(net.params['conv1_BatchNorm'][0].data)
+    # print(net.params['conv1_BatchNorm'][1].data)
+    # print(net.params['conv1_BatchNorm'][2].data)
+    # print(net.params['conv1_BatchNorm_Scale'][0].data)
+    # print(net.params['conv1_BatchNorm_Scale'][1].data)
+    #
+    # print(net.params['upsample1_BatchNorm'][0].data)
+    # print(net.params['upsample1_BatchNorm'][1].data)
+    # print(net.params['upsample1_BatchNorm'][2].data)
+    # print(net.params['upsample1_BatchNorm_Scale'][0].data)
+    # print(net.params['upsample1_BatchNorm_Scale'][1].data)
+
+    # LER layer display
+    # inputx = net.blobs['Deconvolution3'].data[...]
+    # x_offx = net.blobs['Slice6'].data[...]
+    # x_offy = net.blobs['Slice8'].data[...]
+    # noise = net.blobs['Concat69'].data[...]
+    # noise_1 = net.blobs['Scale8'].data[...]
+    # inputnoise = net.blobs['Eltwise6'].data[...]
+    # inputnoise_1 = net.blobs['Eltwise7'].data[...]
+    # output = net.blobs['Eltwise8'].data[...]
+
+    # Conv layer display
+    # conv4bn = net.blobs['Scale5'].data[...]
+
+    # # Deconv layer display
+    # deconv1 = net.blobs['Deconvolution1'].data[...]
+    # print (deconv1)
+    # deconv1bn = net.blobs['Scale6'].data[...]
+    # print (deconv1bn)
+    # deconv2 = net.blobs['Deconvolution2'].data[...]
+    # deconv2bn = net.blobs['Scale7'].data[...]
+    # deconv3 = net.blobs['Deconvolution3'].data[...]
+    # deconv3bn = net.blobs['Scale9'].data[...]
+    # deconv4 = net.blobs['Deconvolution4'].data[...]
+    # deconv4bn = net.blobs['output'].data[...]
+
+    # for k, v in net.blobs.items():
+    #     v.data[0]
+    #     k
+    #     if 'upsample3_randomoff_noise1_concat_32_63' in k:
+    #         print(k)
+    #         print(v.data.shape)
+
+    #         show_data(v.data[0])
+
+    fuse = net.blobs['output'].data[0][:, :, :]
+
+    print(fuse.shape)
+    fuse = np.transpose(fuse, [1, 2, 0])
+    print(fuse.shape)
+    fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+    fuse += 1
+    fuse /= 2
+    fuse_im = transform.resize(fuse, s0)
+    new_im = np.zeros_like(im0)
+    new_im[:, :, 0] = (255 * fuse_im[:, :, 2])  # RGB->BGR
+    new_im[:, :, 1] = (255 * fuse_im[:, :, 1])
+    new_im[:, :, 2] = (255 * fuse_im[:, :, 0])
+    print(new_im.shape)
+    # new_im = im0 * (1. - mask) + new_im * mask
+    new_im = new_im.astype(np.uint8)
+    # new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+    # new_im = new_im.astype(np.uint8)
+    io.imsave(model_root + save_name, new_im)
+
+
+def test_6c_uc(im0, mask0, save_name):
+    print(im0.shape, mask0.shape)
+    s0 = im0.shape
+    sc = 512. / max(im0.shape)
+    im = transform.rescale(im0, sc)
+    mask = transform.rescale(mask0, sc)
+    mask[mask >= 0.1] = 1.0
+    mask[mask < 0.1] = 0.0
+    im = im[..., ::-1]  # RGB->BGR
+    im *= 2.0
+    im -= 1.0
+    im = im * (1. - mask)
+    im += 1.0
+    im /= 2.0
+    im *= 255
+    mask *= 255
+    noise = np.random.rand(512, 512)
+    noise = (noise[:, :] > 0.5).astype(np.float32)
+    print(im.shape)
+
+    noise1 = np.random.rand(512, 512)
+    noise1 = (noise1[:, :] > 0.5).astype(np.float32)
+    canvas = np.zeros([512, 512, 6], np.float32)
+    canvas[:im.shape[0], :im.shape[1], 0:3] = im[:, :, :]
+    canvas[:im.shape[0], :im.shape[1], 3] = mask[:, :, 0]
+    canvas[:im.shape[0], :im.shape[1], 4] = noise
+    canvas[:im.shape[0], :im.shape[1], 5] = noise1
+    # im[:] = 1
+    canvas = np.transpose(canvas, [2, 0, 1])
+    canvas = np.expand_dims(canvas, 0)
+    print(canvas.shape)
+    net.blobs['input'].data[...] = canvas
+
+    # run net and take argmax for prediction
+    d1 = datetime.now()
+    net.forward()
+    fuse = net.blobs['output'].data[0][:, :, :]
+    print(fuse.shape)
+    fuse = np.transpose(fuse, [1, 2, 0])
+    print(fuse.shape)
+    fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+    fuse += 1
+    fuse /= 2
+    fuse_im = transform.resize(fuse, s0)
+    new_im = np.zeros_like(im0)
+    new_im[:, :, 0] = (255 * fuse_im[:, :, 2])  # RGB->BGR
+    new_im[:, :, 1] = (255 * fuse_im[:, :, 1])
+    new_im[:, :, 2] = (255 * fuse_im[:, :, 0])
+    print(new_im.shape)
+    # new_im = im0 * (1. - mask) + new_im * mask
+    new_im = new_im.astype(np.uint8)
+    # new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+    # new_im = new_im.astype(np.uint8)
+    io.imsave(model_root + save_name, new_im)
+
+
+# load net
+model_root = './inpainting/'
+net = caffe.Net(model_root + 'test10-t11-0115-6c.prototxt', model_root + 'test10-t11-0121-6c.caffemodel', caffe.TEST)
+
+im0 = io.imread(model_root + "test0.jpg")
+# guide0 = io.imread(model_root + "sketch0.jpg", True)
+mask0 = io.imread(model_root + "mask_test1.jpg")
+# noise0 = io.imread(model_root + "noise_512.jpg")
+# test3(im0, mask0, noise0, "test_eden_resizeonly_0903_2.jpg")
+test_6c_uc(im0, mask0, "test10-t11-0121-6c.jpg")
+# test_4c_uc(im0, mask0, "test9-t2.jpg")
Index: SaveProtxt_inpainting200103.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting200103.py	(date 1579052610000)
+++ SaveProtxt_inpainting200103.py	(date 1579052610000)
@@ -0,0 +1,174 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1), depthwise不能改变通道数，所以需要再来一个1*1卷积
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name, input_channel = 0):
+    if input_channel == 0:
+        input_channel = num_filter
+    convs_depthwise = convs(data=data, num_filter=input_channel, kernel=kernel, stride=stride, pad=pad, group=input_channel, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='gen_gate_conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.Sigmoid(conv_f)
+    out_f = caffe.layers.Sigmoid(conv_f, in_place= True, name=name+"_conv_f_Sigmoid")
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=input, filters=cnum, kernel_size=1, strides=1)
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel = ksize, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g_1")
+    # conv_g_1 = tf.nn.sigmoid(conv_g_1)
+    conv_g = caffe.layers.Sigmoid(conv_g_1, in_place= True, name = name + "_conv_g_1_Sigmoid")
+
+     # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation = 0, name = name + "_Eltwise")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_Sigmoid')
+    return data
+
+def random_offset(x, noise, filters = 32, width = 256, height = 256, cnum = 32, name = "random_offset"):
+    noise1, _x1 = caffe.layers.Slice(noise, ntop=2, name=name+'_slice_noise1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    noise1, _x2 = caffe.layers.Slice(noise1, ntop=2, name=name+'_slice_noise1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    noise2 = noise1
+    #需要复制与x相同的通道数
+    for i in range(1, filters) : #32通道
+        noise1 = caffe.layers.Concat(noise2, noise1, axis = 1, name = name + "_noise1_concat_%d"%i)
+
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = name+"_noise1_Scale_inverse") #1-noise1
+    # noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)#需要复制与x相同的通道数
+
+    x1, _x3 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x1-r', slice_param=dict(slice_dim=2, slice_point=[height-1]))
+    x1, _x4 = caffe.layers.Slice(x1, ntop=2, name=name+'_slice_x1-c', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = name+"_x1Multi") # x1 * noise1
+
+    _x5, x2 = caffe.layers.Slice(x, ntop=2, name=name+'_slice_x2-r', slice_param=dict(slice_dim=2, slice_point=[1]))
+    _x6, x2 = caffe.layers.Slice(x2, ntop=2, name=name+'_slice_x2-c', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name=name+"_x2Multi")
+
+    res = caffe.layers.Eltwise(res1, res2, name=name+"_xMultix") # x1 * noise1 + x2 * (1 - noise1)
+
+    # x = tf.pad(x, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT', name='padding')
+    res = convs(data=res, num_filter=filters, kernel=1, stride=1, pad=1, group=filters, dilation=1, name=name+"_depthwise_pad") # 257*257
+    _res_r, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-r", slice_param=dict(slice_dim = 2, slice_point=[1]))
+    _res_c, res = caffe.layers.Slice(res, ntop = 2, name = name + "_slice_res-c", slice_param=dict(slice_dim = 3, slice_point=[1]))
+    _res_r = caffe.layers.Reshape(_res_r, name = name + "_reshape_257_1")
+    _res_c = caffe.layers.Reshape(_res_c, name = name + "_reshape_1_256")
+    _x_256_1 = caffe.layers.Concat(_x3, _x5, axis = 1, name = name + "_others_256_1") # (256 * 1)
+    _x_1_255 = caffe.layers.Concat(_x4, _x6, axis = 1, name = name + "_others_1_255") # (1 * 255)
+    _x_256_1 = caffe.layers.Reshape(_x_256_1, name = name + "_reshape_256_1")
+    _x_1_255 = caffe.layers.Reshape(_x_1_255, name = name + "_reshape_1_255")
+    _x1 = caffe.layers.Reshape(_x1, name = name + "_reshape_512_257")
+    _x2 = caffe.layers.Reshape(_x2, name = name + "_reshape_257_255")
+    _x = caffe.layers.Concat(_res_r, _res_c, _x_256_1, _x_1_255, _x1, _x2, axis = 1, name = name + "_concat_others")
+    return res, _x
+
+# caffe (N*C*H*W)  tensorflow (默认 N*H*W*C， 也支持 N*C*H*W)
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    randomoff2 = 1 # for test11
+    #输入为512*512*6
+    input_x, net.noise = caffe.layers.Slice(net.data, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    input_x = caffe.layers.Scale(input_x, bias_term = True, name = "input_x_Scale")
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+    x_2 = gen_gate_conv(x_1, 2 * cnum, 5, 2, name='conv2')
+    x_3 = gen_gate_conv(x_2, 4 * cnum, 3, 2, name='conv3')
+    x_4 = gen_gate_conv(x_3,  4 * cnum, 3, 2, name='conv4')
+
+    x = gen_normal_deconv(x_4, 4*cnum, name='upsample2')
+    # x = tf.concat([x, x_3], axis=3, name='concat2')
+    x = caffe.layers.Concat(x, x_3, axis = 1, name = 'upsample2_concat2')
+    # x = tf.layers.conv2d(x, 4 * cnum, 3, 1, activation=tf.nn.leaky_relu, padding='same', name='up_conv1')
+    x = gen_gate_conv(x, 2*cnum, 3, 1, padding='same',name='up_conv6')
+
+    x = gen_normal_deconv(x, 2*cnum, name='upsample3')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'upsample3_concat3')
+    x = gen_gate_conv(x, cnum, 3, 1, padding='same',name='up_conv7')
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+    x = caffe.layers.Concat(x, x_1, axis = 1, name = 'upsample3_concat8')
+    # x = conv(data = x, num_filter = cnum, kernel = 3, stride = 1, pad = 1, dilation = 1, name = "conv8")
+    x = gen_gate_conv(x, cnum, 3, 1, padding='same', name='up_conv8')
+
+    x = gen_normal_deconv(x, cnum, name='upsample5')
+    # x = tf.concat([x, input_x], axis=3, name='concat9')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    # x = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9-1')
+    x = leakyReLU(x, name = 'conv9-1-ReLU')
+    net.output = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9')
+
+
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test10-t11-0115-6c.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: SaveProtxt_inpainting0917.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting0917.py	(date 1569387955000)
+++ SaveProtxt_inpainting0917.py	(date 1569387955000)
@@ -0,0 +1,382 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1)
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name):
+    convs_depthwise = convs(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, group=num_filter, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+# v7 2gateCon
+def gen_gate_conv_1(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=caffe.params.ReLU, training=True):
+    print('gen_gate_conv_1 (%s) start...'%name)
+    padding = ksize // 2 * rate
+    #conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+"_conv_f")
+    # out_f = leaky_relu(conv_f)
+    out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+
+    # # Soft 5*5 gating
+    # conv_g5 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g5 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g5")
+    # conv_g5 = leaky_relu(conv_g5)
+    conv_g5 = leakyReLU(conv_g5, name = name+"_conv_g5_ReLU")
+    # conv_g5 = tf.layers.separable_conv2d(inputs=conv_g5, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g5 = separable_convs(data = conv_g5, num_filter = cnum, kernel = ksize, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g5_separable")
+    # conv_g5 = tf.nn.sigmoid(conv_g5)
+    conv_g5 = caffe.layers.Sigmoid(conv_g5, in_place = True, name = name+"_conv_g5_separable_Sigmoid")
+
+    # # Soft 3*3 gating
+    padding = 3 // 2 * rate
+    # conv_g3 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g3 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g3")
+    # conv_g3 = leaky_relu(conv_g3)
+    conv_g3 = leakyReLU(conv_g3, name = name+"_conv_g3_ReLU")
+    # conv_g3 = tf.layers.separable_conv2d(inputs=conv_g3, filters=cnum, kernel_size=3, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g3 = separable_convs(data = conv_g3, num_filter = cnum, kernel = 3, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g3_separable")
+    # conv_g3 = tf.nn.sigmoid(conv_g3)
+    conv_g3 = caffe.layers.Sigmoid(conv_g3, in_place = True, name = "_conv_g3_separable_Sigmoid")
+
+    # # Soft 1*1 gating
+    # conv_g1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g1 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g1")
+    # conv_g1 = leaky_relu(conv_g1)
+    conv_g1 = leakyReLU(conv_g1, name = name + "_conv_g1_ReLU")
+    # conv_g1 = tf.layers.separable_conv2d(inputs=conv_g1, filters=cnum, kernel_size=1, padding='same',  strides=stride, dilation_rate=1)
+    conv_g1 = separable_convs(data = conv_g1, num_filter = cnum, kernel = 1, stride = stride, pad = 0, dilation = 1, name = name+"_conv_g1_separable")
+    # conv_g1 = tf.nn.tanh(conv_g1)
+    conv_g1 = caffe.layers.TanH(conv_g1, in_place = True, name = name + "_conv_g1_separable_TanH")
+
+    # # concat the gating
+    # conv_g = tf.concat([conv_g5, conv_g3], axis=3)
+    conv_g = caffe.layers.Concat(conv_g5, conv_g3, axis = 1)
+    # conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = conv(data = conv_g, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g")
+    # conv_g = tf.nn.sigmoid(conv_g)
+    conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+    # # Elementwise Multiply
+    # res_mul = tf.multiply(out_f, conv_g)
+    res_mul = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")#'PROD'-0,'SUM'-1,'MAX'-2
+    # x = tf.add(res_mul, conv_g1)
+    x = caffe.layers.Eltwise(res_mul, conv_g1, name = name+"_add")
+    print('gen_gate_conv_1 end!')
+    return x
+
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.sigmoid(conv_f)
+    out_f = caffe.layers.Sigmoid(conv_f, in_place = True, name = name + '_out_f')
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_g_1')
+    # conv_g_1 = leaky_relu(conv_g_1)
+    # conv_g = tf.nn.sigmoid(conv_g_1)
+    conv_g = caffe.layers.Sigmoid(conv_g_1, in_place = True, name = name + '_out_g')
+    # conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=3, strides=1, padding='same')
+    # conv_g = tf.nn.sigmoid(conv_g_2)
+    # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")
+    return x
+
+def gate_res_block(x, cnum, name):
+    print("gate_res_block (%s) start ..."%name)
+    x_1 = gen_gate_conv(x, cnum = cnum, ksize=3, name=name+'_conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_BatchNorm")#测试时true，训练时false
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+    #x_1 = leaky_relu(x_1)
+    x_1 = leakyReLU(x_1, name = "_conv1_BatchNorm_Scale_ReLU")
+
+    x_1 = gen_gate_conv(x_1, cnum = cnum, ksize=3, name=name+'_conv2')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_BatchNorm")
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_BatchNorm_Scale")
+
+    #out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    #out = leaky_relu(out)
+    out = leakyReLU(out, name = name+"_out")
+    print("gate_res_block end!")
+    return out
+    
+def resnetblock(x, cnum, name = 'res'):
+    x_1 = gen_gate_conv(x, cnum, 3, name = name + '_conv1')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn1')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_bn1")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_bn1_Scale")#BatchNorm+Scale实现
+    # x_1 = tf.nn.relu(x_1)
+    x_1 = caffe.layers.ReLU(x_1, in_place = True, name = name + '_conv1_bn1_Scale_ReLU')
+    x_1 = gen_gate_conv(x_1, cnum, 3, name= name + '_conv2')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn2')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_bn2")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_bn2_Scale")#BatchNorm+Scale实现
+    # out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    return out
+
+def gen_layer_deconv(x, cnum, kernel_size = 3, stride = 2, name='upsample', pad='same', training=True):
+    # x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose')
+    k = kernel_size + 1
+    padding = k // 2 - 1
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=k,stride=2,pad=padding,bias_term=True), name=name+"_deconv")
+    # x = tf.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = name+"_deconv_Sigmoid")
+    return x
+
+def gen_layer_gate_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', pad='same', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data =caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.relu(data)
+    data = caffe.layers.ReLU(data, in_place = True, name = name+"_deconv_ReLU")
+
+    # data_gate = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv_gate")
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place = True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name+'_Eltwise')
+    return x
+
+def gen_deconv2(x, cnum, stride=1, name='upsample', padding='SAME', training=True):
+    data = deconv2(data=x, num_filter=cnum, kernel=3, stride=2, pad=1, name=name+'_dconv')
+    return data
+
+def gen_deconv_1(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    #x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding='same')
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    #x = tf.nn.relu(x)
+    x = caffe.layers.ReLU(x, in_place = True, name = name + '_deconv_ReLU')
+    return x
+
+def gen_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    # filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+    # data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], padding)
+    # data = tf.nn.bias_add(data, bias)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+'_deconv')
+    # data = leaky_relu(data, alpha=0.1)
+    data = leakyReLU(data, name = name + '_deconv_ReLU', alpha=0.1)
+    # data = caffe.layers.ReLU(data, in_place=True, name=name+"_deconv_ReLU")
+
+    # filt_gate = tf.get_variable('conv2d_transpose_gate/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias_gate = tf.get_variable('conv2d_transpose_gate/bias', [cnum], tf.float32)
+    # data_gate = tf.nn.conv2d_transpose(x, filt_gate, [b, scale_size * w, scale_size * h, c], [1, stride, stride, 1], padding)
+    # data_gate = tf.nn.bias_add(data_gate, bias_gate)
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum,kernel_size=4,stride=2,pad=1,bias_term=True),name=name+'_deconv_gate')
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place=True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name + '_Eltwise')
+    return x
+
+
+def resize_conv(x, cnum, kernel_size=3,stride=1, name='upsample', padding='SAME', training=True):
+    # x = resize(x, func=tf.image.resize_bilinear)
+    factor = 2
+    k = 2 * factor - factor % 2
+    p = int(math.ceil((factor - 1) / 2.))
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, group=cnum,kernel_size=k,stride=2,pad=p,weight_filler={"type": "bilinear"},bias_term=False), name=name+"_deconv")
+    # x = tf.layers.conv2d(
+    #     x, cnum, kernel_size=kernel_size, strides=(stride, stride),
+    #     padding=padding, trainable=training,
+    #     name=name + '_dconv'
+    # )
+    p = kernel_size // 2
+    x = conv(data = x, num_filter = cnum, kernel=kernel_size, stride=stride, pad=p, dilation=1, name=name+"_deconv_conv")
+    return x
+
+def gen_normal_layers_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_Sigmoid')
+    return data
+
+def my_create_net(lmdb, noise1, noise2, noise3, noise4, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    noise1_1 = caffe.layers.Data(source=noise1, backend=caffe.params.Data.LMDB, ntop=1)
+    noise1_2 = caffe.layers.Data(source=noise2, backend=caffe.params.Data.LMDB, ntop=1)
+    noise2_1 = caffe.layers.Data(source=noise3, backend=caffe.params.Data.LMDB, ntop=1)
+    noise2_2 = caffe.layers.Data(source=noise4, backend=caffe.params.Data.LMDB, ntop=1)
+    cnum = 32
+    input_x = net.data
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1-1')
+    x_2 = gen_gate_conv(x_1, 2 * cnum, 5, 2, name='conv2')
+    x_3 = gen_gate_conv(x_2, 4 * cnum, 3, 2, name='conv3')
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 3, 2, name='conv4')
+    x_5 = gen_gate_conv(x_4, 4 * cnum, 3, 2, name='conv4-1')
+
+    x = resnetblock(x_5, 4 * cnum, name='block1')
+    x = resnetblock(x, 4 * cnum, name='block2')
+    x = resnetblock(x, 4 * cnum, name='block3')
+    x = resnetblock(x, 4 * cnum, name='block4')
+    x = resnetblock(x, 4 * cnum, name='block5')
+    # x = gate_res_block(x, 8 * cnum, name='block6')
+    # x = gate_res_block(x, 8 * cnum, name='block7')
+    # x = gate_res_block(x, 8 * cnum, name='block8')
+
+    x = gen_normal_layers_deconv(x, 4 * cnum, kernel_size=4, pad=1 ,name='upsample1')
+    # x = tf.concat([x, x_4], axis=3, name='concat1')
+    x = caffe.layers.Concat(x, x_4, axis = 1, name='concat1')
+    # x = tf.layers.conv2d(x, 4 * cnum, 3, 1, padding='same', name='conv5')
+    x = conv(data = x, num_filter = 4*cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv5')
+
+    x = gen_normal_layers_deconv(x, 4*cnum, kernel_size=4, name='upsample2')
+    # x = tf.concat([x, x_3], axis=3, name='concat2')
+    x = caffe.layers.Concat(x, x_3, axis = 1, name = 'concat2')
+    # x = tf.layers.conv2d(x, 2*cnum, 3, 1, padding='same',name='conv6')
+    x = conv(data = x, num_filter = 2*cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv6')
+
+    x = gen_normal_layers_deconv(x, 2*cnum, kernel_size=4, name='upsample3')
+    # x = tf.concat([x, x_2], axis=3, name='concat3')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'concat3')
+    # x = tf.layers.conv2d(x, cnum, 3, 1, padding='same',name='conv7')
+    x = conv(data = x, num_filter = cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv7')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = "conv7_Sigmoid")
+
+    x = gen_normal_layers_deconv(x, cnum, kernel_size=4, name='upsample4')
+    # x = tf.concat([x, x_1], axis=3, name='concat8')
+    x = caffe.layers.Concat(x, x_1, axis=1, name='concat8')
+    # x = tf.layers.conv2d(x, cnum, 3, 1, padding='same', name='conv8')
+    x = conv(data = x, num_filter = cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv8')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = 'conv8_Sigmoid')
+
+    x = gen_normal_layers_deconv(x, cnum, kernel_size=4, name='upsample5')
+    # x = tf.concat([x, input_x], axis=3, name='concat9')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    # x = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9-1')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name='conv9-1_Sigmoid')
+
+    # x = tf.concat([x, noise], axis=3, name='noise')
+    # x = caffe.layers.Concat(x, noise, axis=1, name = 'noise')
+    # x = tf.layers.conv2d(x, 3, 5, 1, padding='same',name='conv9-2')
+    x = conv(data = x, num_filter = 3, kernel=5, stride=1, pad=2, dilation=1, name='conv9-2')
+    # x_1 = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x_1 = conv(data = x, num_filter = 3, kernel = 3, stride = 1, pad = 1, dilation = 1, name = 'post_conv9-1')
+    # x_1 = tf.nn.relu(x_1)
+    # x_2 = tf.layers.conv2d(x, 3, 1, 1, padding='same', name='conv10-1')
+    x_2 = conv(data = x, num_filter = 3, kernel = 1, stride = 1, pad = 0, dilation = 1, name = 'post_conv10-1')
+    # x_2 = tf.nn.relu(x_2)
+    x_2 = caffe.layers.ReLU(x_2, in_place = True, name = 'post_conv10-1_Relu')
+    # x_2 = tf.layers.conv2d(x_2, 3, 3, 1, padding='same', name='conv10-2')
+    x_2 = conv(data = x_2, num_filter = 3, kernel = 3, stride = 1, pad = 1, dilation = 1, name = 'post_conv10-2')
+    # x_2 = tf.nn.relu(x_2)
+    x_2 = caffe.layers.ReLU(x_2, in_place = True, name = 'post_conv10-2_Relu')
+    # x_2 = tf.layers.conv2d(x_2, 3, 1, 1, padding='same', name='conv10-3')
+    x_2 = conv(data = x_2, num_filter = 3, kernel = 1, stride = 1, pad = 0, dilation = 1, name = 'post_conv10-3')
+    # x_2 = tf.nn.sigmoid(x_2)
+    x_2 = caffe.layers.Sigmoid(x_2, in_place = True, name = 'post_conv10-3_Sigmoid')
+    # x = tf.multiply(x_1, x_2, name='conv9-2')
+    x = caffe.layers.Eltwise(x_1, x_2, operation=0, name = "post_conv9-2")
+
+    # post processing
+    _, x1 = caffe.layers.Slice(x, ntop=2, name='slice1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    x1, _ = caffe.layers.Slice(x1, ntop=2, name='slice1-2', slice_param=dict(slice_dim=3, slice_point=[511]))
+    res1 = caffe.layers.Eltwise(x1, noise1_1, operation=0, name = "x1Multi")
+
+    x2, _ = caffe.layers.Slice(x, ntop=2, name='slice2-1', slice_param=dict(slice_dim=2, slice_point=[510]))
+    _, x2 = caffe.layers.Slice(x2, ntop=2, name='slice2-2', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res2 = caffe.layers.Eltwise(x2, noise1_2, operation=0, name="x2Multi")
+    res = caffe.layers.Eltwise(res1, res2, name="x1_add_x2")
+
+    _, x1 = caffe.layers.Slice(res, ntop=2, name='slice3', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res1 = caffe.layers.Eltwise(x1, noise2_1, operation=0, name="x1Multi_1")
+
+    x2, _ = caffe.layers.Slice(x, ntop=2, name='slice4', slice_param=dict(slice_dim=3, slice_point=[510]))
+    res2 = caffe.layers.Eltwise(x2, noise2_2, operation=0, name="x2Multi_1")
+
+    res = caffe.layers.Eltwise(res1, res2, name="x1_add_x2_1")
+
+    # Gauss blur
+    dt_b, dt_g, dt_r = caffe.layers.Slice(res, ntop=3, name='slice5', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    dt_b = conv(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, dilation=1, name='convB_1')
+    dt_b = conv(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, dilation=1, name='convB_2')
+    dt_b = conv(data=dt_b, num_filter=1, kernel=1, stride=1, pad=1, dilation=1, name='fcn_pad_b')
+
+    dt_g = conv(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, dilation=1, name='convG_1')
+    dt_g = conv(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, dilation=1, name='convG_2')
+    dt_g = conv(data=dt_g, num_filter=1, kernel=1, stride=1, pad=1, dilation=1, name='fcn_pad_g')
+
+    dt_r = conv(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, dilation=1, name='convR_1')
+    dt_r = conv(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, dilation=1, name='convR_2')
+    dt_r = conv(data=dt_r, num_filter=1, kernel=1, stride=1, pad=1, dilation=1, name='fcn_pad_r')
+
+    x = caffe.layers.Concat(dt_b, dt_g, dt_r, axis=1)
+
+    net.output = x
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                           #train.lmdb文件的位置
+    noise1 = caffe_root+'noise1_lmdb'
+    noise2 = caffe_root + 'noise2_lmdb'
+    noise3 = caffe_root + 'noise3_lmdb'
+    noise4 = caffe_root + 'noise4_lmdb'
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test-eden.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(train_lmdb, noise1, noise2, noise3, noise4,mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
Index: weights_tensor2caffe_inpainting_6c.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting_6c.py	(date 1455227184000)
+++ weights_tensor2caffe_inpainting_6c.py	(date 1455227184000)
@@ -0,0 +1,298 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+def tensor2caffe(net,tensor):
+
+    ct_map={"conv1-1_conv_f:weights"                              :   "inpaint_net/conv2d/kernel" ,
+            "conv1-1_conv_f:bias"                                 :   "inpaint_net/conv2d/bias" ,
+            "conv1-1_conv_g_1:weights"                            :   "inpaint_net/conv2d_1/kernel" ,
+            "conv1-1_conv_g_1:bias"                               :   "inpaint_net/conv2d_1/bias" ,
+            "conv2_conv_f:weights"                              :   "inpaint_net/conv2d_2/kernel" ,
+            "conv2_conv_f:bias"                                 :   "inpaint_net/conv2d_2/bias" ,
+            "conv2_conv_g_1:weights"                            :   "inpaint_net/conv2d_3/kernel" ,
+            "conv2_conv_g_1:bias"                               :   "inpaint_net/conv2d_3/bias" ,
+            "conv3_conv_f:weights"                              :   "inpaint_net/conv2d_4/kernel" ,
+            "conv3_conv_f:bias"                                 :   "inpaint_net/conv2d_4/bias" ,
+            "conv3_conv_g_1:weights"                            :   "inpaint_net/conv2d_5/kernel" ,
+            "conv3_conv_g_1:bias"                               :   "inpaint_net/conv2d_5/bias" ,
+            "conv4_conv_f:weights"                              :   "inpaint_net/conv2d_6/kernel" ,
+            "conv4_conv_f:bias"                                 :   "inpaint_net/conv2d_6/bias" ,
+            "conv4_conv_g_1:weights"                            :   "inpaint_net/conv2d_7/kernel" ,
+            "conv4_conv_g_1:bias"                               :   "inpaint_net/conv2d_7/bias" ,
+            "conv4-1_conv_f:weights"                            :   "inpaint_net/conv2d_8/kernel" ,
+            "conv4-1_conv_f:bias"                               :   "inpaint_net/conv2d_8/bias" ,
+            "conv4-1_conv_g_1:weights"                          :   "inpaint_net/conv2d_9/kernel" ,
+            "conv4-1_conv_g_1:bias"                             :   "inpaint_net/conv2d_9/bias" ,
+            "block1_conv1_conv_f:weights"                       :   "inpaint_net/block1/conv2d/kernel" ,
+            "block1_conv1_conv_f:bias"                          :   "inpaint_net/block1/conv2d/bias" ,
+            "block1_conv1_conv_g_1:weights"                     :   "inpaint_net/block1/conv2d_1/kernel" ,
+            "block1_conv1_conv_g_1:bias"                        :   "inpaint_net/block1/conv2d_1/bias" ,
+            "block1_conv2_conv_f:weights"                       :   "inpaint_net/block1/conv2d_2/kernel" ,
+            "block1_conv2_conv_f:bias"                          :   "inpaint_net/block1/conv2d_2/bias" ,
+            "block1_conv2_conv_g_1:weights"                     :   "inpaint_net/block1/conv2d_3/kernel" ,
+            "block1_conv2_conv_g_1:bias"                        :   "inpaint_net/block1/conv2d_3/bias" ,
+            "block2_conv1_conv_f:weights"                       :   "inpaint_net/block2/conv2d/kernel" ,
+            "block2_conv1_conv_f:bias"                          :   "inpaint_net/block2/conv2d/bias" ,
+            "block2_conv1_conv_g_1:weights"                     :   "inpaint_net/block2/conv2d_1/kernel" ,
+            "block2_conv1_conv_g_1:bias"                        :   "inpaint_net/block2/conv2d_1/bias" ,
+            "block2_conv2_conv_f:weights"                       :   "inpaint_net/block2/conv2d_2/kernel" ,
+            "block2_conv2_conv_f:bias"                          :   "inpaint_net/block2/conv2d_2/bias" ,
+            "block2_conv2_conv_g_1:weights"                     :   "inpaint_net/block2/conv2d_3/kernel" ,
+            "block2_conv2_conv_g_1:bias"                        :   "inpaint_net/block2/conv2d_3/bias" ,
+            "block3_conv1_conv_f:weights"                       :   "inpaint_net/block3/conv2d/kernel" ,
+            "block3_conv1_conv_f:bias"                          :   "inpaint_net/block3/conv2d/bias" ,
+            "block3_conv1_conv_g_1:weights"                     :   "inpaint_net/block3/conv2d_1/kernel" ,
+            "block3_conv1_conv_g_1:bias"                        :   "inpaint_net/block3/conv2d_1/bias" ,
+            "block3_conv2_conv_f:weights"                       :   "inpaint_net/block3/conv2d_2/kernel" ,
+            "block3_conv2_conv_f:bias"                          :   "inpaint_net/block3/conv2d_2/bias" ,
+            "block3_conv2_conv_g_1:weights"                     :   "inpaint_net/block3/conv2d_3/kernel" ,
+            "block3_conv2_conv_g_1:bias"                        :   "inpaint_net/block3/conv2d_3/bias" ,
+            "block4_conv1_conv_f:weights"                       :   "inpaint_net/block4/conv2d/kernel" ,
+            "block4_conv1_conv_f:bias"                          :   "inpaint_net/block4/conv2d/bias" ,
+            "block4_conv1_conv_g_1:weights"                     :   "inpaint_net/block4/conv2d_1/kernel" ,
+            "block4_conv1_conv_g_1:bias"                        :   "inpaint_net/block4/conv2d_1/bias" ,
+            "block4_conv2_conv_f:weights"                       :   "inpaint_net/block4/conv2d_2/kernel" ,
+            "block4_conv2_conv_f:bias"                          :   "inpaint_net/block4/conv2d_2/bias" ,
+            "block4_conv2_conv_g_1:weights"                     :   "inpaint_net/block4/conv2d_3/kernel" ,
+            "block4_conv2_conv_g_1:bias"                        :   "inpaint_net/block4/conv2d_3/bias" ,
+            "block5_conv1_conv_f:weights"                       :   "inpaint_net/block5/conv2d/kernel" ,
+            "block5_conv1_conv_f:bias"                          :   "inpaint_net/block5/conv2d/bias" ,
+            "block5_conv1_conv_g_1:weights"                     :   "inpaint_net/block5/conv2d_1/kernel" ,
+            "block5_conv1_conv_g_1:bias"                        :   "inpaint_net/block5/conv2d_1/bias" ,
+            "block5_conv2_conv_f:weights"                       :   "inpaint_net/block5/conv2d_2/kernel" ,
+            "block5_conv2_conv_f:bias"                          :   "inpaint_net/block5/conv2d_2/bias" ,
+            "block5_conv2_conv_g_1:weights"                     :   "inpaint_net/block5/conv2d_3/kernel" ,
+            "block5_conv2_conv_g_1:bias"                        :   "inpaint_net/block5/conv2d_3/bias" ,
+            "upsample1_deconv:weights"                     :   "inpaint_net/upsample1/conv2d_transpose/kernel" ,
+            "upsample1_deconv:bias"                        :   "inpaint_net/upsample1/conv2d_transpose/bias" ,
+            "conv5:weights"                                     :   "inpaint_net/conv5/kernel" ,
+            "conv5:bias"                                        :   "inpaint_net/conv5/bias" ,
+            "upsample2_deconv:weights"                     :   "inpaint_net/upsample2/conv2d_transpose/kernel" ,
+            "upsample2_deconv:bias"                        :   "inpaint_net/upsample2/conv2d_transpose/bias" ,
+            "conv6:weights"                                     :   "inpaint_net/conv6/kernel" ,
+            "conv6:bias"                                        :   "inpaint_net/conv6/bias" ,
+            "upsample3_deconv:weights"                     :   "inpaint_net/upsample3/conv2d_transpose/kernel" ,
+            "upsample3_deconv:bias"                        :   "inpaint_net/upsample3/conv2d_transpose/bias" ,
+            "conv7:weights"                                     :   "inpaint_net/conv7/kernel" ,
+            "conv7:bias"                                        :   "inpaint_net/conv7/bias" ,
+            "upsample4_deconv:weights"                     :   "inpaint_net/upsample4/conv2d_transpose/kernel" ,
+            "upsample4_deconv:bias"                        :   "inpaint_net/upsample4/conv2d_transpose/bias" ,
+            "conv8:weights"                                     :   "inpaint_net/conv8/kernel" ,
+            "conv8:bias"                                        :   "inpaint_net/conv8/bias" ,
+            "upsample5_deconv:weights"                     :   "inpaint_net/upsample5/conv2d_transpose/kernel" ,
+            "upsample5_deconv:bias"                        :   "inpaint_net/upsample5/conv2d_transpose/bias" ,
+            "conv9-1:weights"                                   :   "inpaint_net/conv9-1/kernel" ,
+            "conv9-1:bias"                                      :   "inpaint_net/conv9-1/bias" ,
+            "conv9-2:weights"                                   :   "inpaint_net/conv9-2/kernel" ,
+            "conv9-2:bias"                                      :   "inpaint_net/conv9-2/bias" ,
+            "post_conv9-1:weights"                              :   "inpaint_net/post/conv9-1/kernel" ,
+            "post_conv9-1:bias"                                 :   "inpaint_net/post/conv9-1/bias" ,
+            "post_conv10-1:weights"                             :   "inpaint_net/post/conv10-1/kernel" ,
+            "post_conv10-1:bias"                                :   "inpaint_net/post/conv10-1/bias" ,
+            "post_conv10-2:weights"                             :   "inpaint_net/post/conv10-2/kernel" ,
+            "post_conv10-2:bias"                                :   "inpaint_net/post/conv10-2/bias" ,
+            "post_conv10-3:weights"                             :   "inpaint_net/post/conv10-3/kernel" ,
+            "post_conv10-3:bias"                                :   "inpaint_net/post/conv10-3/bias" ,
+      }
+    
+    num=0
+    net_keys=net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s'%(i, len(net_keys), le, net_keys[i]))
+        if le==1:
+            if(net_keys[i] == 'upsample1_deconv'): continue
+            if(net_keys[i] == 'upsample2_deconv'): continue
+            if(net_keys[i] == 'upsample3_deconv'): continue
+            if(net_keys[i] == 'upsample4_deconv'): continue
+            if(net_keys[i] == 'upsample5_deconv'): continue
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value=weights_value.transpose((3,2,0,1)) 
+            else:
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1)) 
+            try:
+                net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+            except:
+                net.params[net_keys[i]][0].data[:,:,:,:]=0
+                net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+            
+            
+            print(weights_value.shape,end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+            
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                print("scale layer!")
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+                continue
+                print("le_last = %d"%le_last)
+                # for BN layers
+                key0=net_keys[i]+':gamma'
+                value0 = ct_map[key0]
+                gamma_value=tensor[value0] 
+
+                key1=net_keys[i]+':beta'
+                value1=ct_map[key1]
+                beta_value=tensor[value1]
+
+                value2 = value0.replace('gamma', 'moving_variance')
+                mv_value=tensor[value2] 
+
+                value3 = value1.replace('beta', 'moving_mean')
+                mm_value=tensor[value3] 
+
+
+                net.params[net_keys[i]][0].data[:] = gamma_value / np.sqrt(mv_value)
+                print(gamma_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                net.params[net_keys[i]][1].data[:] = beta_value - mm_value/np.sqrt(mv_value)*gamma_value
+                print(beta_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+            else:   
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                
+                weights_value=tensor[value0]
+                if "filter"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1))
+                    
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value[:,:,:,:]
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value[:,:,:,:]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+#                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+#                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':bias'
+                value1=ct_map[key1]
+                
+                bias_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=bias_value
+                print(bias_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+    print(num)
+    print('net_keys[0]',net_keys[0])
+    net.params[net_keys[0]][0].data[:] = np.array([1./128, 1./128, 1./128, 1./255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0.])
+
+
+
+def print_caffemodel(net):
+    net_keys=net.params.keys()
+    le_last = 0
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le==1:
+            n,c,w,h=net.params[net_keys[i]][0].data.shape
+            print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                # BatchNorm layer后面的scale layer, 训练gamma和deta
+                w=net.params[net_keys[i]][0].data.shape
+                print("%s:gamma ->  [%d]"%(net_keys[i], w[0]))
+
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:beta  ->  [%d]"%(net_keys[i], w[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+            
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]"%(net_keys[i],w[0]))
+        elif le==3:
+            #BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]"%(net_keys[i],m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]"%(net_keys[i],v[0]))
+
+            #f = net.params[net_keys[i]][2].data.shape
+            #print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys=tensor.keys()
+    for i in range(0,len(tensor_keys)):
+        le=len(tensor[tensor_keys[i]].shape)
+        if le==1:
+            w=tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]"%(tensor_keys[i],w))
+        elif le==4:
+            weights=tensor[tensor_keys[i]]
+            #if "filter"  in tensor_keys[i]:
+                #weights=tensor[tensor_keys[i]].transpose((2,3,1,0)) 
+            #else:
+                #weights=tensor[tensor_keys[i]].transpose((3,2,1,0)) 
+            n,c,w,h=weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]"%(tensor_keys[i],n,c,w,h))
+
+#print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net('/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test_eden_post_6c_1.prototxt', caffe.TEST)
+    print_caffemodel(net)
+    print("########################################################################################################################")
+    tensor=np.load('inpainting/test_eden.npy').item()
+    print_tensorflow(tensor)
+    print(
+        "########################################################################################################################")
+    tensor2caffe(net,tensor)
+    net.save("inpainting/test_eden_post_6c_1.caffemodel")
Index: print_pb_node_name.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- print_pb_node_name.py	(date 1577777921000)
+++ print_pb_node_name.py	(date 1577777921000)
@@ -0,0 +1,15 @@
+import tensorflow as tf
+from tensorflow.python.platform import gfile
+
+
+GRAPH_PB_PATH = 'inpainting/test2-1.pb' #path to your .pb file
+with tf.Session() as sess:
+    print("load graph")
+    with gfile.FastGFile(GRAPH_PB_PATH,'rb') as f:
+        graph_def = tf.GraphDef()
+    # Note: one of the following two lines work if required libraries are available
+        #text_format.Merge(f.read(), graph_def)
+        graph_def.ParseFromString(f.read())
+        tf.import_graph_def(graph_def, name='')
+        for i,n in enumerate(graph_def.node):
+            print("Name of the node - %s" % n.name)
\ No newline at end of file
Index: SaveProtxt_inpainting_6c.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting_6c.py	(date 1569486587000)
+++ SaveProtxt_inpainting_6c.py	(date 1569486587000)
@@ -0,0 +1,428 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1)
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name):
+    convs_depthwise = convs(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, group=num_filter, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+# v7 2gateCon
+def gen_gate_conv_1(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=caffe.params.ReLU, training=True):
+    print('gen_gate_conv_1 (%s) start...'%name)
+    padding = ksize // 2 * rate
+    #conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+"_conv_f")
+    # out_f = leaky_relu(conv_f)
+    out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+
+    # # Soft 5*5 gating
+    # conv_g5 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g5 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g5")
+    # conv_g5 = leaky_relu(conv_g5)
+    conv_g5 = leakyReLU(conv_g5, name = name+"_conv_g5_ReLU")
+    # conv_g5 = tf.layers.separable_conv2d(inputs=conv_g5, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g5 = separable_convs(data = conv_g5, num_filter = cnum, kernel = ksize, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g5_separable")
+    # conv_g5 = tf.nn.sigmoid(conv_g5)
+    conv_g5 = caffe.layers.Sigmoid(conv_g5, in_place = True, name = name+"_conv_g5_separable_Sigmoid")
+
+    # # Soft 3*3 gating
+    padding = 3 // 2 * rate
+    # conv_g3 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g3 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g3")
+    # conv_g3 = leaky_relu(conv_g3)
+    conv_g3 = leakyReLU(conv_g3, name = name+"_conv_g3_ReLU")
+    # conv_g3 = tf.layers.separable_conv2d(inputs=conv_g3, filters=cnum, kernel_size=3, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g3 = separable_convs(data = conv_g3, num_filter = cnum, kernel = 3, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g3_separable")
+    # conv_g3 = tf.nn.sigmoid(conv_g3)
+    conv_g3 = caffe.layers.Sigmoid(conv_g3, in_place = True, name = "_conv_g3_separable_Sigmoid")
+
+    # # Soft 1*1 gating
+    # conv_g1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g1 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g1")
+    # conv_g1 = leaky_relu(conv_g1)
+    conv_g1 = leakyReLU(conv_g1, name = name + "_conv_g1_ReLU")
+    # conv_g1 = tf.layers.separable_conv2d(inputs=conv_g1, filters=cnum, kernel_size=1, padding='same',  strides=stride, dilation_rate=1)
+    conv_g1 = separable_convs(data = conv_g1, num_filter = cnum, kernel = 1, stride = stride, pad = 0, dilation = 1, name = name+"_conv_g1_separable")
+    # conv_g1 = tf.nn.tanh(conv_g1)
+    conv_g1 = caffe.layers.TanH(conv_g1, in_place = True, name = name + "_conv_g1_separable_TanH")
+
+    # # concat the gating
+    # conv_g = tf.concat([conv_g5, conv_g3], axis=3)
+    conv_g = caffe.layers.Concat(conv_g5, conv_g3, axis = 1)
+    # conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = conv(data = conv_g, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g")
+    # conv_g = tf.nn.sigmoid(conv_g)
+    conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+    # # Elementwise Multiply
+    # res_mul = tf.multiply(out_f, conv_g)
+    res_mul = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")#'PROD'-0,'SUM'-1,'MAX'-2
+    # x = tf.add(res_mul, conv_g1)
+    x = caffe.layers.Eltwise(res_mul, conv_g1, name = name+"_add")
+    print('gen_gate_conv_1 end!')
+    return x
+
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.sigmoid(conv_f)
+    out_f = caffe.layers.Sigmoid(conv_f, in_place = True, name = name + '_out_f')
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_g_1')
+    # conv_g_1 = leaky_relu(conv_g_1)
+    # conv_g = tf.nn.sigmoid(conv_g_1)
+    conv_g = caffe.layers.Sigmoid(conv_g_1, in_place = True, name = name + '_out_g')
+    # conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=3, strides=1, padding='same')
+    # conv_g = tf.nn.sigmoid(conv_g_2)
+    # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")
+    return x
+
+def gate_res_block(x, cnum, name):
+    print("gate_res_block (%s) start ..."%name)
+    x_1 = gen_gate_conv(x, cnum = cnum, ksize=3, name=name+'_conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_BatchNorm")#测试时true，训练时false
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+    #x_1 = leaky_relu(x_1)
+    x_1 = leakyReLU(x_1, name = "_conv1_BatchNorm_Scale_ReLU")
+
+    x_1 = gen_gate_conv(x_1, cnum = cnum, ksize=3, name=name+'_conv2')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_BatchNorm")
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_BatchNorm_Scale")
+
+    #out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    #out = leaky_relu(out)
+    out = leakyReLU(out, name = name+"_out")
+    print("gate_res_block end!")
+    return out
+    
+def resnetblock(x, cnum, name = 'res'):
+    x_1 = gen_gate_conv(x, cnum, 3, name = name + '_conv1')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn1')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_bn1")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_bn1_Scale")#BatchNorm+Scale实现
+    # x_1 = tf.nn.relu(x_1)
+    x_1 = caffe.layers.ReLU(x_1, in_place = True, name = name + '_conv1_bn1_Scale_ReLU')
+    x_1 = gen_gate_conv(x_1, cnum, 3, name= name + '_conv2')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn2')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_bn2")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_bn2_Scale")#BatchNorm+Scale实现
+    # out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    return out
+
+def gen_layer_deconv(x, cnum, kernel_size = 3, stride = 2, name='upsample', pad='same', training=True):
+    # x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose')
+    k = kernel_size + 1
+    padding = k // 2 - 1
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=k,stride=2,pad=padding,bias_term=True), name=name+"_deconv")
+    # x = tf.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = name+"_deconv_Sigmoid")
+    return x
+
+def gen_layer_gate_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', pad='same', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data =caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.relu(data)
+    data = caffe.layers.ReLU(data, in_place = True, name = name+"_deconv_ReLU")
+
+    # data_gate = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv_gate")
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place = True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name+'_Eltwise')
+    return x
+
+def gen_deconv2(x, cnum, stride=1, name='upsample', padding='SAME', training=True):
+    data = deconv2(data=x, num_filter=cnum, kernel=3, stride=2, pad=1, name=name+'_dconv')
+    return data
+
+def gen_deconv_1(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    #x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding='same')
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    #x = tf.nn.relu(x)
+    x = caffe.layers.ReLU(x, in_place = True, name = name + '_deconv_ReLU')
+    return x
+
+def gen_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    # filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+    # data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], padding)
+    # data = tf.nn.bias_add(data, bias)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+'_deconv')
+    # data = leaky_relu(data, alpha=0.1)
+    data = leakyReLU(data, name = name + '_deconv_ReLU', alpha=0.1)
+    # data = caffe.layers.ReLU(data, in_place=True, name=name+"_deconv_ReLU")
+
+    # filt_gate = tf.get_variable('conv2d_transpose_gate/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias_gate = tf.get_variable('conv2d_transpose_gate/bias', [cnum], tf.float32)
+    # data_gate = tf.nn.conv2d_transpose(x, filt_gate, [b, scale_size * w, scale_size * h, c], [1, stride, stride, 1], padding)
+    # data_gate = tf.nn.bias_add(data_gate, bias_gate)
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum,kernel_size=4,stride=2,pad=1,bias_term=True),name=name+'_deconv_gate')
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place=True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name + '_Eltwise')
+    return x
+
+
+def resize_conv(x, cnum, kernel_size=3,stride=1, name='upsample', padding='SAME', training=True):
+    # x = resize(x, func=tf.image.resize_bilinear)
+    factor = 2
+    k = 2 * factor - factor % 2
+    p = int(math.ceil((factor - 1) / 2.))
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, group=cnum,kernel_size=k,stride=2,pad=p,weight_filler={"type": "bilinear"},bias_term=False), name=name+"_deconv")
+    # x = tf.layers.conv2d(
+    #     x, cnum, kernel_size=kernel_size, strides=(stride, stride),
+    #     padding=padding, trainable=training,
+    #     name=name + '_dconv'
+    # )
+    p = kernel_size // 2
+    x = conv(data = x, num_filter = cnum, kernel=kernel_size, stride=stride, pad=p, dilation=1, name=name+"_deconv_conv")
+    return x
+
+def gen_normal_layers_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_Sigmoid')
+    return data
+
+def getNoise(offset_h, offset_w, width, height):
+    noiseMask = np.random.rand(height, width)
+    if offset_h and offset_w:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    elif offset_h:
+        noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+    elif offset_w:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    return noiseMask1, noiseMask2
+
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.input, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    input_x, noise1, noise2 = caffe.layers.Slice(net.input, ntop = 3, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4, 5]))
+    input_x = caffe.layers.Scale(input_x, bias_term = True, name = "input_x_Scale")
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1-1')
+    x_2 = gen_gate_conv(x_1, 2 * cnum, 5, 2, name='conv2')
+    x_3 = gen_gate_conv(x_2, 4 * cnum, 3, 2, name='conv3')
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 3, 2, name='conv4')
+    x_5 = gen_gate_conv(x_4, 4 * cnum, 3, 2, name='conv4-1')
+
+    x = resnetblock(x_5, 4 * cnum, name='block1')
+    x = resnetblock(x, 4 * cnum, name='block2')
+    x = resnetblock(x, 4 * cnum, name='block3')
+    x = resnetblock(x, 4 * cnum, name='block4')
+    x = resnetblock(x, 4 * cnum, name='block5')
+    # x = gate_res_block(x, 8 * cnum, name='block6')
+    # x = gate_res_block(x, 8 * cnum, name='block7')
+    # x = gate_res_block(x, 8 * cnum, name='block8')
+
+    x = gen_normal_layers_deconv(x, 4 * cnum, kernel_size=4, pad=1 ,name='upsample1')
+    # x = tf.concat([x, x_4], axis=3, name='concat1')
+    x = caffe.layers.Concat(x, x_4, axis = 1, name='concat1')
+    # x = tf.layers.conv2d(x, 4 * cnum, 3, 1, padding='same', name='conv5')
+    x = conv(data = x, num_filter = 4*cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv5')
+
+    x = gen_normal_layers_deconv(x, 4*cnum, kernel_size=4, name='upsample2')
+    # x = tf.concat([x, x_3], axis=3, name='concat2')
+    x = caffe.layers.Concat(x, x_3, axis = 1, name = 'concat2')
+    # x = tf.layers.conv2d(x, 2*cnum, 3, 1, padding='same',name='conv6')
+    x = conv(data = x, num_filter = 2*cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv6')
+
+    x = gen_normal_layers_deconv(x, 2*cnum, kernel_size=4, name='upsample3')
+    # x = tf.concat([x, x_2], axis=3, name='concat3')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'concat3')
+    # x = tf.layers.conv2d(x, cnum, 3, 1, padding='same',name='conv7')
+    x = conv(data = x, num_filter = cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv7')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = "conv7_Sigmoid")
+
+    x = gen_normal_layers_deconv(x, cnum, kernel_size=4, name='upsample4')
+    # x = tf.concat([x, x_1], axis=3, name='concat8')
+    x = caffe.layers.Concat(x, x_1, axis=1, name='concat8')
+    # x = tf.layers.conv2d(x, cnum, 3, 1, padding='same', name='conv8')
+    x = conv(data = x, num_filter = cnum, kernel=3, stride=1, pad=1, dilation=1, name='conv8')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = 'conv8_Sigmoid')
+
+    x = gen_normal_layers_deconv(x, cnum, kernel_size=4, name='upsample5')
+    # x = tf.concat([x, input_x], axis=3, name='concat9')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat9')
+    # x = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9-1')
+    # x = tf.nn.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name='conv9-1_Sigmoid')
+
+    # x = tf.concat([x, noise], axis=3, name='noise')
+    # x = caffe.layers.Concat(x, noise, axis=1, name = 'noise')
+    # x = tf.layers.conv2d(x, 3, 5, 1, padding='same',name='conv9-2')
+    x = conv(data = x, num_filter = 3, kernel=5, stride=1, pad=2, dilation=1, name='conv9-2')
+    # x_1 = tf.layers.conv2d(x, 3, 3, 1, padding='same',name='conv9-1')
+    x_1 = conv(data = x, num_filter = 3, kernel = 3, stride = 1, pad = 1, dilation = 1, name = 'post_conv9-1')
+    # x_1 = tf.nn.relu(x_1)
+    # x_2 = tf.layers.conv2d(x, 3, 1, 1, padding='same', name='conv10-1')
+    x_2 = conv(data = x, num_filter = 3, kernel = 1, stride = 1, pad = 0, dilation = 1, name = 'post_conv10-1')
+    # x_2 = tf.nn.relu(x_2)
+    x_2 = caffe.layers.ReLU(x_2, in_place = True, name = 'post_conv10-1_Relu')
+    # x_2 = tf.layers.conv2d(x_2, 3, 3, 1, padding='same', name='conv10-2')
+    x_2 = conv(data = x_2, num_filter = 3, kernel = 3, stride = 1, pad = 1, dilation = 1, name = 'post_conv10-2')
+    # x_2 = tf.nn.relu(x_2)
+    x_2 = caffe.layers.ReLU(x_2, in_place = True, name = 'post_conv10-2_Relu')
+    # x_2 = tf.layers.conv2d(x_2, 3, 1, 1, padding='same', name='conv10-3')
+    x_2 = conv(data = x_2, num_filter = 3, kernel = 1, stride = 1, pad = 0, dilation = 1, name = 'post_conv10-3')
+    # x_2 = tf.nn.sigmoid(x_2)
+    x_2 = caffe.layers.Sigmoid(x_2, in_place = True, name = 'post_conv10-3_Sigmoid')
+    # x = tf.multiply(x_1, x_2, name='conv9-2')
+    x = caffe.layers.Eltwise(x_1, x_2, operation=0, name = "post_conv9-2")
+
+    #post
+    height = 512
+    width = height
+    # noise = caffe.io.load_image(caffe_root + "noise_512.jpg",  False)
+    # net.noise = caffe.layers.ImageData(name="noise",source="noise.txt", batch_size=1, new_width=512, new_height=512, 
+    #     ntop=1,is_color=True,root_folder='/inpainting/')
+    print str(net.to_proto())
+    # noise1_m0, noise1_m1 = caffe.layers.Slice(noise, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[1]))
+    _x1, noise1 = caffe.layers.Slice(noise1, ntop=2, name='slice_noise1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    _x2, noise1 = caffe.layers.Slice(noise1, ntop=2, name='slice_noise1-2', slice_param=dict(slice_dim=3, slice_point=[1]))
+    noise1_1 = caffe.layers.Scale(noise1, bias_term = True, name = "_noise1_Scale")
+    noise1 = caffe.layers.Concat(noise1, noise1, noise1)
+    noise1_1 = caffe.layers.Concat(noise1_1, noise1_1, noise1_1)
+
+    _x3, noise2 = caffe.layers.Slice(noise2, ntop=2, name='slice_noisem1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    _x4, noise2 = caffe.layers.Slice(noise2, ntop=2, name='slice_noisem1-2', slice_param=dict(slice_dim=3, slice_point=[2]))
+    noise2_1 = caffe.layers.Scale(noise2, bias_term = True, name = "_noise2_1_Scale")
+    noise2 = caffe.layers.Concat(noise2, noise2, noise2)
+    noise2_1 = caffe.layers.Concat(noise2_1, noise2_1, noise2_1)
+
+    _x5, x1 = caffe.layers.Slice(x, ntop=2, name='slice1-1', slice_param=dict(slice_dim=2, slice_point=[2]))
+    x1, _x6 = caffe.layers.Slice(x1, ntop=2, name='slice1-2', slice_param=dict(slice_dim=3, slice_point=[width-1]))
+    # x1_c1, x1_c2, x1_c3 = caffe.layers.Slice(x1, ntop=3, name='slice1-3', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    # res1_c1 = caffe.layers.Eltwise(x1_c1, noise1, operation=0, name = "x1Multi")
+    # res1_c2 = caffe.layers.Eltwise(x1_c2, noise1, operation=0, name = "x1Multi")
+    # res1_c3 = caffe.layers.Eltwise(x1_c3, noise1, operation=0, name = "x1Multi")
+    res1 = caffe.layers.Eltwise(x1, noise1, operation=0, name = "x1Multi")
+
+    x2, _x7 = caffe.layers.Slice(x, ntop=2, name='slice2-1', slice_param=dict(slice_dim=2, slice_point=[height-2]))
+    _x8, x2 = caffe.layers.Slice(x2, ntop=2, name='slice2-2', slice_param=dict(slice_dim=3, slice_point=[1]))
+    # x2_c1, x2_c2, x2_c3 = caffe.layers.Slice(x2, ntop=3, name='slice1-3', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    # res2_c1 = caffe.layers.Eltwise(x2_c1, noise1_1, operation=0, name = "x1Multi")
+    # res2_c2 = caffe.layers.Eltwise(x2_c2, noise1_1, operation=0, name = "x1Multi")
+    # res2_c3 = caffe.layers.Eltwise(x2_c3, noise1_1, operation=0, name = "x1Multi")
+    res2 = caffe.layers.Eltwise(x2, noise1_1, operation=0, name="x2Multi")
+    res = caffe.layers.Eltwise(res1, res2, name="x1Multix2")
+
+    _x9, x1 = caffe.layers.Slice(res, ntop=2, name='slice3', slice_param=dict(slice_dim=3, slice_point=[1]))
+    res1 = caffe.layers.Eltwise(x1, noise2, operation=0, name="x1Multi_1")
+
+    x2, _x10 = caffe.layers.Slice(res, ntop=2, name='slice4', slice_param=dict(slice_dim=3, slice_point=[width-2]))
+    res2 = caffe.layers.Eltwise(x2, noise2_1, operation=0, name="x2Multi_1")
+
+    res = caffe.layers.Eltwise(res1, res2, name="x1Multix2_1")
+
+    # Gauss blur
+    dt_b, dt_g, dt_r = caffe.layers.Slice(res, ntop=3, name='slice5', slice_param=dict(slice_dim=1, slice_point=[1,2]))
+    dt_b = convs(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convB_1')
+    dt_b = convs(data=dt_b, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convB_2')
+    dt_b = convs(data=dt_b, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_b')
+
+    dt_g = convs(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convG_1')
+    dt_g = convs(data=dt_g, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convG_2')
+    dt_g = convs(data=dt_g, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_g')
+
+    dt_r = convs(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convR_1')
+    dt_r = convs(data=dt_r, num_filter=1, kernel=3, stride=1, pad=1, group=1, dilation=1, name='Gaussian_convR_2')
+    dt_r = convs(data=dt_r, num_filter=1, kernel=1, stride=1, pad=1, group=1, dilation=1, name='Gaussian_fcn_pad_r')
+
+    x = caffe.layers.Concat(dt_b, dt_g, dt_r, axis=1, name = 'concat_rgb')
+    net.output = x
+
+    _x_2_512 = caffe.layers.Concat(_x1, _x3, _x5, _x7, axis=1, name='concat_2_512')
+    _x_510_1 = caffe.layers.Concat(_x2, _x6, _x8, _x9, _x10, axis=1, name='concat_510_1')
+    _x_2_512 = caffe.layers.Reshape(_x_2_512, name='reshape_2_512')
+    _x_510_1 = caffe.layers.Reshape(_x_510_1, name='reshape_510_1')
+    _x_510_2 = caffe.layers.Reshape(_x4, name='reshape_510_2')
+    _x = caffe.layers.Concat(_x_2_512, _x_510_1, _x_510_2, axis=1, name='concat_others')
+    net.output1 = _x
+    # x = tf.clip_by_value(x, -1., 1., name='result')
+    print("end!")
+    # x = tf.nn.tanh(x, name='result')
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test_eden_post_6c_1.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: run_hed_inpainting.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- run_hed_inpainting.py	(date 1569473813000)
+++ run_hed_inpainting.py	(date 1569473813000)
@@ -0,0 +1,348 @@
+import numpy as np
+import matplotlib.pyplot as plt
+import matplotlib.pylab as pylab
+import matplotlib.cm as cm
+import scipy.misc
+from datetime import datetime
+from PIL import Image
+import scipy.io
+import os
+import sys
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#caffe_root = '/home/bo718.wang/sl1015.liu/caffe_RCNN_Mobile_Shuffe_HED/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+import caffe
+from skimage import io, transform
+import random
+import cv2
+#remove the following two lines if testing with cpu
+#caffe.set_mode_gpu()
+#caffe.set_device(7)
+
+# tensor=np.load('weights.npy').item()
+def test1(im0, mask0, save_name):
+	print(im0.shape, mask0.shape)
+	s0 = im0.shape
+	sc = 512. / max(im0.shape)
+	im = transform.rescale(im0, sc)
+	mask = transform.rescale(mask0, sc)
+	mask[mask >= 0.1] = 1.0
+	mask[mask < 0.1] = 0.0
+	# guide = transform.rescale(guide0, sc)
+	# guide[guide >= 0.1] = 1.0
+	# guide[guide < 0.1] = 0.0
+	im = im * 2.0 - 1.0
+	im = im * (1. - mask)
+	# io.imsave(model_root + "mask_sc.jpg", mask)
+	# io.imsave(model_root + "guide_sc.jpg", guide)
+	# image_incomplete = transform.resize(im, s0)
+	# io.imsave(model_root + "test_incomplete_sc.jpg", im)
+	# print(image_incomplete.shape)
+	# im *= 255
+	im = im[...,::-1]#RGB->BGR
+	print(im.shape) 
+	canvas = np.zeros([512,512,5], np.float32)
+	canvas[:im.shape[0], :im.shape[1], 0:3] = im[:,:,:]
+	canvas[:im.shape[0], :im.shape[1], 3] = 1
+	canvas[:im.shape[0], :im.shape[1], 4] = mask[:,:,0]
+	# canvas[:im.shape[0], :im.shape[1], 5] = 0
+	# im[:] = 1
+	canvas = np.transpose(canvas, [2,0,1])
+	canvas = np.expand_dims(canvas, 0)
+	print(canvas.shape)
+	net.blobs['data'].data[...]= canvas
+
+	# run net and take argmax for prediction
+	d1=datetime.now()
+	net.forward()
+	fuse = net.blobs['output'].data[0][:,:,:]
+	print(fuse.shape)
+	fuse = np.transpose(fuse, [1,2,0])
+	print(fuse.shape)
+	fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+	fuse += 1
+	fuse /= 2
+	fuse_im = transform.resize(fuse, s0)
+	new_im = np.zeros_like(im0)
+	new_im[:,:,0] = (255*fuse_im[:,:,2]).astype(np.uint8)#RGB->BGR
+	new_im[:,:,1] = (255*fuse_im[:,:,1]).astype(np.uint8)
+	new_im[:,:,2] = (255*fuse_im[:,:,0]).astype(np.uint8)
+	print(new_im.shape)
+	# new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+	# new_im = new_im.astype(np.uint8)
+	io.imsave(model_root + save_name, new_im)
+
+def test2(im0, mask0, noise0, save_name):
+	print(im0.shape, mask0.shape)
+	s0 = im0.shape
+	sc = 512. / max(im0.shape)
+	im = transform.rescale(im0, sc)
+	mask = transform.rescale(mask0, sc)
+	mask[mask >= 0.1] = 1.0
+	mask[mask < 0.1] = 0.0
+	noise = noise0 / 255.
+	# guide = transform.rescale(guide0, sc)
+	# guide[guide >= 0.1] = 1.0
+	# guide[guide < 0.1] = 0.0
+	im = im * 2.0 - 1.0
+	im = im * (1. - mask)
+	# io.imsave(model_root + "mask_sc.jpg", mask)
+	# io.imsave(model_root + "guide_sc.jpg", guide)
+	# image_incomplete = transform.resize(im, s0)
+	# io.imsave(model_root + "test_incomplete_sc.jpg", im)
+	# print(image_incomplete.shape)
+	# im *= 255
+	im = im[...,::-1]#RGB->BGR
+	print(im.shape) 
+	canvas = np.zeros([512,512,6], np.float32)
+	canvas[:im.shape[0], :im.shape[1], 0:3] = im[:,:,:]
+	canvas[:im.shape[0], :im.shape[1], 3] = 1
+	canvas[:im.shape[0], :im.shape[1], 4] = mask[:,:,0]
+	canvas[:im.shape[0], :im.shape[1], 5] = noise[:,:,0] #random.random()
+	# im[:] = 1
+	canvas = np.transpose(canvas, [2,0,1])
+	canvas = np.expand_dims(canvas, 0)
+	print(canvas.shape)
+	net.blobs['data'].data[...]= canvas
+
+	# run net and take argmax for prediction
+	d1=datetime.now()
+	net.forward()
+	fuse = net.blobs['output'].data[0][:,:,:]
+	print(fuse.shape)
+	fuse = np.transpose(fuse, [1,2,0])
+	print(fuse.shape)
+	fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+	fuse += 1
+	fuse /= 2
+	fuse_im = transform.resize(fuse, s0)
+	new_im = np.zeros_like(im0)
+	new_im[:,:,0] = (255*fuse_im[:,:,2])#RGB->BGR
+	new_im[:,:,1] = (255*fuse_im[:,:,1])
+	new_im[:,:,2] = (255*fuse_im[:,:,0])
+	print(new_im.shape)
+	# new_im = im0 * (1. - mask) + new_im * mask
+	new_im = new_im.astype(np.uint8)
+	# new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+	# new_im = new_im.astype(np.uint8)
+	io.imsave(model_root + save_name, new_im)
+
+# processMask --- input data range is [-1,1]
+def test3(im0, mask0, noise0, save_name):
+	print(im0.shape, mask0.shape)
+	s0 = im0.shape
+	sc = 512. / max(im0.shape)
+	im = transform.rescale(im0, sc)
+	mask = transform.rescale(mask0, sc)
+	blendMask = mask
+	mask = cv2.GaussianBlur(mask, ksize=(25,25), sigmaX=0, sigmaY=0)
+	# blendMask = cv2.
+	mask[mask >= 0.1] = 1.0
+	mask[mask < 0.1] = 0.0
+	noise = noise0 / 255.
+	# guide = transform.rescale(guide0, sc)
+	# guide[guide >= 0.1] = 1.0
+	# guide[guide < 0.1] = 0.0
+	im = im * 2.0 - 1.0
+	im = im * (1. - mask)
+	# io.imsave(model_root + "mask_sc.jpg", mask)
+	# io.imsave(model_root + "guide_sc.jpg", guide)
+	# image_incomplete = transform.resize(im, s0)
+	# io.imsave(model_root + "test_incomplete_sc.jpg", im)
+	# print(image_incomplete.shape)
+	# im *= 255
+	im = im[...,::-1]#RGB->BGR
+	print(im.shape) 
+	canvas = np.zeros([512,512,6], np.float32)
+	canvas[:im.shape[0], :im.shape[1], 0:3] = im[:,:,:]
+	canvas[:im.shape[0], :im.shape[1], 3] = 1
+	canvas[:im.shape[0], :im.shape[1], 4] = mask[:,:,0]
+	canvas[:im.shape[0], :im.shape[1], 5] = noise[:,:,0] #random.random()
+	# im[:] = 1
+	canvas = np.transpose(canvas, [2,0,1])
+	canvas = np.expand_dims(canvas, 0)
+	print(canvas.shape)
+	net.blobs['data'].data[...]= canvas
+
+	# run net and take argmax for prediction
+	d1=datetime.now()
+	net.forward()
+	fuse = net.blobs['output'].data[0][:,:,:]
+	print(fuse.shape)
+	fuse = np.transpose(fuse, [1,2,0])
+	print(fuse.shape)
+	fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+	fuse += 1
+	fuse /= 2
+	fuse_im = transform.resize(fuse, s0)
+	new_im = np.zeros_like(im0)
+	new_im[:,:,0] = (255*fuse_im[:,:,2])#RGB->BGR
+	new_im[:,:,1] = (255*fuse_im[:,:,1])
+	new_im[:,:,2] = (255*fuse_im[:,:,0])
+	print(new_im.shape)
+	# io.imsave(model_root + "mask_blurred.jpg", blendMask)
+	# new_im = cv2.warpAffine(new_im,)
+	blendMask[blendMask <= 0.2] = 0.0
+	# blendMask -= 0.2
+	# blendMask /= 0.8
+	new_im = im0 * (1. - blendMask) + new_im * blendMask
+	new_im = new_im.astype(np.uint8)
+	# new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+	# new_im = new_im.astype(np.uint8)
+	io.imsave(model_root + save_name, new_im)
+
+# input data range is [-1,1]
+def test_4c(im0, mask0, save_name):
+	print(im0.shape, mask0.shape)
+	s0 = im0.shape
+	sc = 512. / max(im0.shape)
+	im = transform.rescale(im0, sc)
+	mask = transform.rescale(mask0, sc)
+	mask[mask >= 0.1] = 1.0
+	mask[mask < 0.1] = 0.0
+	im = im * 2.0 - 1.0
+	im = im * (1. - mask)
+	im = im[...,::-1]#RGB->BGR
+	print(im.shape) 
+	canvas = np.zeros([512,512,4], np.float32)
+	canvas[:im.shape[0], :im.shape[1], 0:3] = im[:,:,:]
+	canvas[:im.shape[0], :im.shape[1], 3] = mask[:,:,0]
+	# im[:] = 1
+	canvas = np.transpose(canvas, [2,0,1])
+	canvas = np.expand_dims(canvas, 0)
+	print(canvas.shape)
+	net.blobs['data'].data[...]= canvas
+
+	# run net and take argmax for prediction
+	d1=datetime.now()
+	net.forward()
+	fuse = net.blobs['output'].data[0][:,:,:]
+	print(fuse.shape)
+	fuse = np.transpose(fuse, [1,2,0])
+	print(fuse.shape)
+	fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+	fuse += 1
+	fuse /= 2
+	fuse_im = transform.resize(fuse, s0)
+	new_im = np.zeros_like(im0)
+	new_im[:,:,0] = (255*fuse_im[:,:,2])#RGB->BGR
+	new_im[:,:,1] = (255*fuse_im[:,:,1])
+	new_im[:,:,2] = (255*fuse_im[:,:,0])
+	print(new_im.shape)
+	# new_im = im0 * (1. - mask) + new_im * mask
+	new_im = new_im.astype(np.uint8)
+	# new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+	# new_im = new_im.astype(np.uint8)
+	io.imsave(model_root + save_name, new_im)
+
+# input data range is [0,255]
+def test_4c_uc(im0, mask0, save_name):
+	print(im0.shape, mask0.shape)
+	s0 = im0.shape
+	sc = 512. / max(im0.shape)
+	im = transform.rescale(im0, sc)
+	mask = transform.rescale(mask0, sc)
+	mask[mask >= 0.1] = 1.0
+	mask[mask < 0.1] = 0.0
+	im = im[...,::-1]#RGB->BGR
+	im *= 2.0
+	im -= 1.0
+	im = im * (1. - mask)
+	im += 1.0
+	im /= 2.0
+	im *= 255
+	mask *= 255
+	print(im.shape) 
+	canvas = np.zeros([512,512,4], np.float32)
+	canvas[:im.shape[0], :im.shape[1], 0:3] = im[:,:,:]
+	canvas[:im.shape[0], :im.shape[1], 3] = mask[:,:,0]
+	# im[:] = 1
+	canvas = np.transpose(canvas, [2,0,1])
+	canvas = np.expand_dims(canvas, 0)
+	print(canvas.shape)
+	net.blobs['input'].data[...]= canvas
+
+	# run net and take argmax for prediction
+	d1=datetime.now()
+	net.forward()
+	fuse = net.blobs['output'].data[0][:,:,:]
+	print(fuse.shape)
+	fuse = np.transpose(fuse, [1,2,0])
+	print(fuse.shape)
+	fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+	fuse += 1
+	fuse /= 2
+	fuse_im = transform.resize(fuse, s0)
+	new_im = np.zeros_like(im0)
+	new_im[:,:,0] = (255*fuse_im[:,:,2])#RGB->BGR
+	new_im[:,:,1] = (255*fuse_im[:,:,1])
+	new_im[:,:,2] = (255*fuse_im[:,:,0])
+	print(new_im.shape)
+	# new_im = im0 * (1. - mask) + new_im * mask
+	new_im = new_im.astype(np.uint8)
+	# new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+	# new_im = new_im.astype(np.uint8)
+	io.imsave(model_root + save_name, new_im)
+
+# input data range is [0,255]
+def test_5c_uc(im0, mask0, save_name):
+	print(im0.shape, mask0.shape)
+	s0 = im0.shape
+	sc = 512. / max(im0.shape)
+	im = transform.rescale(im0, sc)
+	mask = transform.rescale(mask0, sc)
+	mask[mask >= 0.1] = 1.0
+	mask[mask < 0.1] = 0.0
+	im = im[...,::-1]#RGB->BGR
+	im *= 2.0
+	im -= 1.0
+	im = im * (1. - mask)
+	im += 1.0
+	im /= 2.0
+	im *= 255
+	mask *= 255
+	noise = np.random.rand(512, 512)
+	noise = (noise[:, :] > 0.5).astype(np.float32)
+	print(im.shape) 
+	canvas = np.zeros([512,512,5], np.float32)
+	canvas[:im.shape[0], :im.shape[1], 0:3] = im[:,:,:]
+	canvas[:im.shape[0], :im.shape[1], 3] = mask[:,:,0]
+	canvas[:im.shape[0], :im.shape[1], 4] = noise
+	# im[:] = 1
+	canvas = np.transpose(canvas, [2,0,1])
+	canvas = np.expand_dims(canvas, 0)
+	print(canvas.shape)
+	net.blobs['input'].data[...]= canvas
+
+	# run net and take argmax for prediction
+	d1=datetime.now()
+	net.forward()
+	fuse = net.blobs['output'].data[0][:,:,:]
+	print(fuse.shape)
+	fuse = np.transpose(fuse, [1,2,0])
+	print(fuse.shape)
+	fuse = np.clip(fuse, -1, 1)[:im.shape[0], :im.shape[1], :]
+	fuse += 1
+	fuse /= 2
+	fuse_im = transform.resize(fuse, s0)
+	new_im = np.zeros_like(im0)
+	new_im[:,:,0] = (255*fuse_im[:,:,2])#RGB->BGR
+	new_im[:,:,1] = (255*fuse_im[:,:,1])
+	new_im[:,:,2] = (255*fuse_im[:,:,0])
+	print(new_im.shape)
+	# new_im = im0 * (1. - mask) + new_im * mask
+	new_im = new_im.astype(np.uint8)
+	# new_im = new_im * (mask0 /255) + im0 * (1 - mask0/255)
+	# new_im = new_im.astype(np.uint8)
+	io.imsave(model_root + save_name, new_im)
+
+# load net
+model_root = './inpainting/'
+net = caffe.Net(model_root+'test_eden_post.prototxt', model_root+'test_eden_post.caffemodel', caffe.TEST)
+
+im0 = io.imread(model_root + "test0.jpg")
+# guide0 = io.imread(model_root + "sketch0.jpg", True)
+mask0 = io.imread(model_root + "mask_test1.jpg")
+# noise0 = io.imread(model_root + "noise_512.jpg")
+# test3(im0, mask0, noise0, "test_eden_resizeonly_0903_2.jpg")
+test_5c_uc(im0, mask0, "test_eden_post.jpg")
\ No newline at end of file
Index: inpainting/test10-t11.prototxt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- inpainting/test10-t11.prototxt	(date 1578883989000)
+++ inpainting/test10-t11.prototxt	(date 1578883989000)
@@ -0,0 +1,605 @@
+layer {
+  name: "input"
+  type: "Input"
+  top: "input"
+  input_param {
+    shape {
+      dim: 1
+      dim: 4
+      dim: 512
+      dim: 512
+    }
+  }
+}
+layer {
+  name: "input_x_Scale"
+  type: "Scale"
+  bottom: "input"
+  top: "Scale1"
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "conv1_conv_f"
+  type: "Convolution"
+  bottom: "Scale1"
+  top: "Convolution1"
+  convolution_param {
+    num_output: 32
+    pad: 2
+    kernel_size: 5
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv1_conv_f_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution1"
+  top: "Convolution1"
+}
+layer {
+  name: "conv1_conv_g_1"
+  type: "Convolution"
+  bottom: "Scale1"
+  top: "Convolution2"
+  convolution_param {
+    num_output: 32
+    pad: 2
+    kernel_size: 5
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv1_conv_g_1_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution2"
+  top: "Convolution2"
+}
+layer {
+  name: "conv1_Eltwise"
+  type: "Eltwise"
+  bottom: "Convolution1"
+  bottom: "Convolution2"
+  top: "Eltwise1"
+  eltwise_param {
+    operation: PROD
+  }
+}
+layer {
+  name: "conv2_conv_f"
+  type: "Convolution"
+  bottom: "Eltwise1"
+  top: "Convolution3"
+  convolution_param {
+    num_output: 64
+    pad: 2
+    kernel_size: 5
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv2_conv_f_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution3"
+  top: "Convolution3"
+}
+layer {
+  name: "conv2_conv_g_1"
+  type: "Convolution"
+  bottom: "Eltwise1"
+  top: "Convolution4"
+  convolution_param {
+    num_output: 64
+    pad: 2
+    kernel_size: 5
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv2_conv_g_1_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution4"
+  top: "Convolution4"
+}
+layer {
+  name: "conv2_Eltwise"
+  type: "Eltwise"
+  bottom: "Convolution3"
+  bottom: "Convolution4"
+  top: "Eltwise2"
+  eltwise_param {
+    operation: PROD
+  }
+}
+layer {
+  name: "conv3_conv_f"
+  type: "Convolution"
+  bottom: "Eltwise2"
+  top: "Convolution5"
+  convolution_param {
+    num_output: 128
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv3_conv_f_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution5"
+  top: "Convolution5"
+}
+layer {
+  name: "conv3_conv_g_1"
+  type: "Convolution"
+  bottom: "Eltwise2"
+  top: "Convolution6"
+  convolution_param {
+    num_output: 128
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv3_conv_g_1_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution6"
+  top: "Convolution6"
+}
+layer {
+  name: "conv3_Eltwise"
+  type: "Eltwise"
+  bottom: "Convolution5"
+  bottom: "Convolution6"
+  top: "Eltwise3"
+  eltwise_param {
+    operation: PROD
+  }
+}
+layer {
+  name: "conv4_conv_f"
+  type: "Convolution"
+  bottom: "Eltwise3"
+  top: "Convolution7"
+  convolution_param {
+    num_output: 128
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv4_conv_f_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution7"
+  top: "Convolution7"
+}
+layer {
+  name: "conv4_conv_g_1"
+  type: "Convolution"
+  bottom: "Eltwise3"
+  top: "Convolution8"
+  convolution_param {
+    num_output: 128
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv4_conv_g_1_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution8"
+  top: "Convolution8"
+}
+layer {
+  name: "conv4_Eltwise"
+  type: "Eltwise"
+  bottom: "Convolution7"
+  bottom: "Convolution8"
+  top: "Eltwise4"
+  eltwise_param {
+    operation: PROD
+  }
+}
+layer {
+  name: "upsample2_deconv"
+  type: "Deconvolution"
+  bottom: "Eltwise4"
+  top: "Deconvolution1"
+  convolution_param {
+    num_output: 128
+    bias_term: true
+    pad: 1
+    kernel_size: 4
+    stride: 2
+  }
+}
+layer {
+  name: "upsample2_deconv_ReLU_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Deconvolution1"
+  top: "Deconvolution1"
+}
+layer {
+  name: "upsample2_concat2"
+  type: "Concat"
+  bottom: "Deconvolution1"
+  bottom: "Eltwise3"
+  top: "Concat1"
+  concat_param {
+    axis: 1
+  }
+}
+layer {
+  name: "up_conv6_conv_f"
+  type: "Convolution"
+  bottom: "Concat1"
+  top: "Convolution9"
+  convolution_param {
+    num_output: 64
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "up_conv6_conv_f_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution9"
+  top: "Convolution9"
+}
+layer {
+  name: "up_conv6_conv_g_1"
+  type: "Convolution"
+  bottom: "Concat1"
+  top: "Convolution10"
+  convolution_param {
+    num_output: 64
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "up_conv6_conv_g_1_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution10"
+  top: "Convolution10"
+}
+layer {
+  name: "up_conv6_Eltwise"
+  type: "Eltwise"
+  bottom: "Convolution9"
+  bottom: "Convolution10"
+  top: "Eltwise5"
+  eltwise_param {
+    operation: PROD
+  }
+}
+layer {
+  name: "upsample3_deconv"
+  type: "Deconvolution"
+  bottom: "Eltwise5"
+  top: "Deconvolution2"
+  convolution_param {
+    num_output: 64
+    bias_term: true
+    pad: 1
+    kernel_size: 4
+    stride: 2
+  }
+}
+layer {
+  name: "upsample3_deconv_ReLU_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Deconvolution2"
+  top: "Deconvolution2"
+}
+layer {
+  name: "upsample3_concat3"
+  type: "Concat"
+  bottom: "Deconvolution2"
+  bottom: "Eltwise2"
+  top: "Concat2"
+  concat_param {
+    axis: 1
+  }
+}
+layer {
+  name: "up_conv7_conv_f"
+  type: "Convolution"
+  bottom: "Concat2"
+  top: "Convolution11"
+  convolution_param {
+    num_output: 32
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "up_conv7_conv_f_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution11"
+  top: "Convolution11"
+}
+layer {
+  name: "up_conv7_conv_g_1"
+  type: "Convolution"
+  bottom: "Concat2"
+  top: "Convolution12"
+  convolution_param {
+    num_output: 32
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "up_conv7_conv_g_1_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution12"
+  top: "Convolution12"
+}
+layer {
+  name: "up_conv7_Eltwise"
+  type: "Eltwise"
+  bottom: "Convolution11"
+  bottom: "Convolution12"
+  top: "Eltwise6"
+  eltwise_param {
+    operation: PROD
+  }
+}
+layer {
+  name: "upsample4_deconv"
+  type: "Deconvolution"
+  bottom: "Eltwise6"
+  top: "Deconvolution3"
+  convolution_param {
+    num_output: 32
+    bias_term: true
+    pad: 1
+    kernel_size: 4
+    stride: 2
+  }
+}
+layer {
+  name: "upsample4_deconv_ReLU_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Deconvolution3"
+  top: "Deconvolution3"
+}
+layer {
+  name: "upsample3_concat8"
+  type: "Concat"
+  bottom: "Deconvolution3"
+  bottom: "Eltwise1"
+  top: "Concat3"
+  concat_param {
+    axis: 1
+  }
+}
+layer {
+  name: "up_conv8_conv_f"
+  type: "Convolution"
+  bottom: "Concat3"
+  top: "Convolution13"
+  convolution_param {
+    num_output: 32
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "up_conv8_conv_f_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution13"
+  top: "Convolution13"
+}
+layer {
+  name: "up_conv8_conv_g_1"
+  type: "Convolution"
+  bottom: "Concat3"
+  top: "Convolution14"
+  convolution_param {
+    num_output: 32
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "up_conv8_conv_g_1_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Convolution14"
+  top: "Convolution14"
+}
+layer {
+  name: "up_conv8_Eltwise"
+  type: "Eltwise"
+  bottom: "Convolution13"
+  bottom: "Convolution14"
+  top: "Eltwise7"
+  eltwise_param {
+    operation: PROD
+  }
+}
+layer {
+  name: "upsample5_deconv"
+  type: "Deconvolution"
+  bottom: "Eltwise7"
+  top: "Deconvolution4"
+  convolution_param {
+    num_output: 32
+    bias_term: true
+    pad: 1
+    kernel_size: 4
+    stride: 2
+  }
+}
+layer {
+  name: "upsample5_deconv_ReLU_Sigmoid"
+  type: "Sigmoid"
+  bottom: "Deconvolution4"
+  top: "Deconvolution4"
+}
+layer {
+  name: "concat9"
+  type: "Concat"
+  bottom: "Deconvolution4"
+  bottom: "Scale1"
+  top: "Concat4"
+  concat_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv9-1"
+  type: "Convolution"
+  bottom: "Concat4"
+  top: "Convolution15"
+  convolution_param {
+    num_output: 3
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
+layer {
+  name: "conv9-1-ReLU"
+  type: "ReLU"
+  bottom: "Convolution15"
+  top: "Convolution15"
+  relu_param {
+    negative_slope: 0.20000000298
+  }
+}
+layer {
+  name: "conv9"
+  type: "Convolution"
+  bottom: "Convolution15"
+  top: "output"
+  convolution_param {
+    num_output: 3
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "constant"
+    }
+    bias_filler {
+      type: "constant"
+    }
+    dilation: 1
+  }
+}
Index: SaveProtxt_inpainting1111-no-post.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SaveProtxt_inpainting1111-no-post.py	(date 1578043992000)
+++ SaveProtxt_inpainting1111-no-post.py	(date 1578043992000)
@@ -0,0 +1,311 @@
+# -*- coding: UTF-8 -*-
+import caffe    
+import math
+import numpy as np
+import random
+
+#conv
+def conv(data, num_filter, kernel=1, stride=1, pad=0, dilation=1, name="conv"):
+    # num_output: 卷积核（filter)的个数, weight_filler: 权值初始化, bias_filler: 偏置项的初始化
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, weight_filler={"type": "constant"}, bias_filler={"type": "constant"}, name = name)
+    return data
+
+# stride always equals 2, padding equals 1
+def deconv2(data, num_filter, kernel, stride, pad, name):
+#    data = caffe.layers.Deconvolution(data,kernel=s)
+    data = caffe.layers.Deconvolution(data, convolution_param=dict(num_output=num_filter,kernel_size=kernel,
+        stride=stride, pad=pad, bias_term=True), name=name)
+    return data
+
+#depthwise-conv2d
+def convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        dilation = dilation, group=group, bias_term=False, name = name)
+    return data
+
+#depthwise-conv2d
+def depthwise_convs(data, num_filter, kernel, stride, pad, group, dilation, name):
+    data = caffe.layers.Convolution(data, num_output=num_filter, kernel_size=kernel,stride = stride, pad=pad,
+        group=group, dilation = dilation, bias_term=False)
+    return data
+
+#separable_convs(depthwise-conv2d+conv2d_1*1)
+def separable_convs(data, num_filter, kernel, stride, pad, dilation, name):
+    convs_depthwise = convs(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, group=num_filter, dilation=dilation, name=name+"_depthwise")
+    convs_pointwise = conv(data=convs_depthwise, num_filter=num_filter, kernel=1, stride=1, pad=0, name=name+"_pointwise")
+    return convs_pointwise
+
+#LeakyReLU
+def leakyReLU(x, name, alpha = 0.2):
+    data = caffe.layers.ReLU(x, in_place=True, relu_param={'negative_slope':alpha}, name=name)
+    return data
+
+# v7 2gateCon
+def gen_gate_conv_1(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', activation=caffe.params.ReLU, training=True):
+    print('gen_gate_conv_1 (%s) start...'%name)
+    padding = ksize // 2 * rate
+    #conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+"_conv_f")
+    # out_f = leaky_relu(conv_f)
+    out_f = leakyReLU(conv_f, name=name+"_conv_f_ReLU")
+
+    # # Soft 5*5 gating
+    # conv_g5 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g5 = conv(data = x, num_filter = cnum, kernel = 1, stride = 1, pad = 0, dilation = 1, name = name+"_conv_g5")
+    # conv_g5 = leaky_relu(conv_g5)
+    conv_g5 = leakyReLU(conv_g5, name = name+"_conv_g5_ReLU")
+    # conv_g5 = tf.layers.separable_conv2d(inputs=conv_g5, filters=cnum, kernel_size=ksize, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g5 = separable_convs(data = conv_g5, num_filter = cnum, kernel = ksize, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g5_separable")
+    # conv_g5 = tf.nn.sigmoid(conv_g5)
+    conv_g5 = caffe.layers.Sigmoid(conv_g5, in_place = True, name = name+"_conv_g5_separable_Sigmoid")
+
+    # # Soft 3*3 gating
+    padding = 3 // 2 * rate
+    # conv_g3 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g3 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g3")
+    # conv_g3 = leaky_relu(conv_g3)
+    conv_g3 = leakyReLU(conv_g3, name = name+"_conv_g3_ReLU")
+    # conv_g3 = tf.layers.separable_conv2d(inputs=conv_g3, filters=cnum, kernel_size=3, padding='same',  strides=stride, dilation_rate=rate)
+    conv_g3 = separable_convs(data = conv_g3, num_filter = cnum, kernel = 3, stride = stride, pad = padding, dilation = rate, name = name+"_conv_g3_separable")
+    # conv_g3 = tf.nn.sigmoid(conv_g3)
+    conv_g3 = caffe.layers.Sigmoid(conv_g3, in_place = True, name = "_conv_g3_separable_Sigmoid")
+
+    # # Soft 1*1 gating
+    # conv_g1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g1 = conv(data = x, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g1")
+    # conv_g1 = leaky_relu(conv_g1)
+    conv_g1 = leakyReLU(conv_g1, name = name + "_conv_g1_ReLU")
+    # conv_g1 = tf.layers.separable_conv2d(inputs=conv_g1, filters=cnum, kernel_size=1, padding='same',  strides=stride, dilation_rate=1)
+    conv_g1 = separable_convs(data = conv_g1, num_filter = cnum, kernel = 1, stride = stride, pad = 0, dilation = 1, name = name+"_conv_g1_separable")
+    # conv_g1 = tf.nn.tanh(conv_g1)
+    conv_g1 = caffe.layers.TanH(conv_g1, in_place = True, name = name + "_conv_g1_separable_TanH")
+
+    # # concat the gating
+    # conv_g = tf.concat([conv_g5, conv_g3], axis=3)
+    conv_g = caffe.layers.Concat(conv_g5, conv_g3, axis = 1)
+    # conv_g = tf.layers.conv2d(inputs=conv_g, filters=cnum, kernel_size=1, strides=1, dilation_rate=1)
+    conv_g = conv(data = conv_g, num_filter = cnum, kernel=1, stride=1, pad=0, dilation=1, name=name+"_conv_g")
+    # conv_g = tf.nn.sigmoid(conv_g)
+    conv_g = caffe.layers.Sigmoid(conv_g, in_place = True, name = name + "_conv_g_Sigmoid")
+
+    # # Elementwise Multiply
+    # res_mul = tf.multiply(out_f, conv_g)
+    res_mul = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")#'PROD'-0,'SUM'-1,'MAX'-2
+    # x = tf.add(res_mul, conv_g1)
+    x = caffe.layers.Eltwise(res_mul, conv_g1, name = name+"_add")
+    print('gen_gate_conv_1 end!')
+    return x
+
+def gen_gate_conv(x, cnum, ksize, stride=1, rate=1, name='conv',
+             padding='SAME', training=True):
+    padding = ksize // 2 * rate
+    # padding = 0
+    # pad = ksize // 2 * rate
+    # input = tf.pad(tensor=x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]], mode="CONSTANT")
+    # feature
+    # conv_f = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_f = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_f')
+    # out_f = tf.nn.Sigmoid(conv_f)
+    out_f = caffe.layers.ReLU(conv_f, in_place = True, name = name + '_out_f')
+    # Soft mask gating
+    # conv_g_1 = tf.layers.conv2d(inputs=x, filters=cnum, kernel_size=ksize, strides=stride, padding='same')
+    conv_g_1 = conv(data = x, num_filter = cnum, kernel=ksize, stride=stride, pad=padding, dilation=rate, name=name+'_conv_g_1')
+    # conv_g_1 = leaky_relu(conv_g_1)
+    # conv_g = tf.nn.sigmoid(conv_g_1)
+    conv_g = caffe.layers.Sigmoid(conv_g_1, in_place = True, name = name + '_out_g')
+    # conv_g_2 = tf.layers.conv2d(inputs=conv_g_1, filters=cnum, kernel_size=3, strides=1, padding='same')
+    # conv_g = tf.nn.sigmoid(conv_g_2)
+    # Elementwise Multiply
+    # x = out_f * conv_g
+    x = caffe.layers.Eltwise(out_f, conv_g, operation=0, name = name+"_Eltwise")
+    return x
+
+def gate_res_block(x, cnum, name):
+    print("gate_res_block (%s) start ..."%name)
+    x_1 = gen_gate_conv(x, cnum = cnum, ksize=3, name=name+'_conv1')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_BatchNorm")#测试时true，训练时false
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_BatchNorm_Scale")#BatchNorm+Scale实现
+    #x_1 = leaky_relu(x_1)
+    x_1 = leakyReLU(x_1, name = "_conv1_BatchNorm_Scale_ReLU")
+
+    x_1 = gen_gate_conv(x_1, cnum = cnum, ksize=3, name=name+'_conv2')
+    #x_1 = tf.layers.batch_normalization(x_1)
+    x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_BatchNorm")
+    x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_BatchNorm_Scale")
+
+    #out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    #out = leaky_relu(out)
+    out = leakyReLU(out, name = name+"_out")
+    print("gate_res_block end!")
+    return out
+    
+def resnetblock(x, cnum, name = 'res'):
+    x_1 = gen_gate_conv(x, cnum, 3, name = name + '_conv1')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn1')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv1_bn1")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv1_bn1_Scale")#BatchNorm+Scale实现
+    # x_1 = tf.nn.relu(x_1)
+    x_1 = caffe.layers.ReLU(x_1, in_place = True, name = name + '_conv1_bn1_Scale_ReLU')
+    x_1 = gen_gate_conv(x_1, cnum, 3, name= name + '_conv2')
+    # # x_1 = tf.layers.batch_normalization(x_1, name='bn2')
+    # x_1 = caffe.layers.BatchNorm(x_1, use_global_stats = True, name = name + "_conv2_bn2")#测试时true，训练时false
+    # x_1 = caffe.layers.Scale(x_1, bias_term = True, name = name + "_conv2_bn2_Scale")#BatchNorm+Scale实现
+    # out = x + x_1
+    out = caffe.layers.Eltwise(x, x_1, name = name + "_Eltwise")
+    return out
+
+def gen_layer_deconv(x, cnum, kernel_size = 3, stride = 2, name='upsample', pad='same', training=True):
+    # x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose')
+    k = kernel_size + 1
+    padding = k // 2 - 1
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=k,stride=2,pad=padding,bias_term=True), name=name+"_deconv")
+    # x = tf.sigmoid(x)
+    x = caffe.layers.Sigmoid(x, in_place = True, name = name+"_deconv_Sigmoid")
+    return x
+
+def gen_layer_gate_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', pad='same', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data =caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.nn.relu(data)
+    data = caffe.layers.ReLU(data, in_place = True, name = name+"_deconv_ReLU")
+
+    # data_gate = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding=pad, name='conv2d_transpose_gate')
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv_gate")
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place = True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name+'_Eltwise')
+    return x
+
+def gen_deconv2(x, cnum, stride=1, name='upsample', padding='SAME', training=True):
+    data = deconv2(data=x, num_filter=cnum, kernel=3, stride=2, pad=1, name=name+'_dconv')
+    return data
+
+def gen_deconv_1(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    #x = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding='same')
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+"_deconv")
+    #x = tf.nn.relu(x)
+    x = caffe.layers.ReLU(x, in_place = True, name = name + '_deconv_ReLU')
+    return x
+
+def gen_deconv(x, cnum, kernel_size = 4, stride = 2, name='upsample', padding='SAME', training=True):
+    # filt = tf.get_variable('conv2d_transpose/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias = tf.get_variable('conv2d_transpose/bias', [cnum], tf.float32)
+    # data = tf.nn.conv2d_transpose(x, filt, [b, scale_size*w, scale_size*h, c], [1, stride, stride, 1], padding)
+    # data = tf.nn.bias_add(data, bias)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=4,stride=2,pad=1,bias_term=True), name=name+'_deconv')
+    # data = leaky_relu(data, alpha=0.1)
+    data = leakyReLU(data, name = name + '_deconv_ReLU', alpha=0.1)
+    # data = caffe.layers.ReLU(data, in_place=True, name=name+"_deconv_ReLU")
+
+    # filt_gate = tf.get_variable('conv2d_transpose_gate/kernel', [kernel_size, kernel_size, cnum, c], tf.float32)
+    # bias_gate = tf.get_variable('conv2d_transpose_gate/bias', [cnum], tf.float32)
+    # data_gate = tf.nn.conv2d_transpose(x, filt_gate, [b, scale_size * w, scale_size * h, c], [1, stride, stride, 1], padding)
+    # data_gate = tf.nn.bias_add(data_gate, bias_gate)
+    data_gate = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum,kernel_size=4,stride=2,pad=1,bias_term=True),name=name+'_deconv_gate')
+    # data_gate = tf.sigmoid(data_gate)
+    data_gate = caffe.layers.Sigmoid(data_gate, in_place=True, name = name + '_deconv_gate_Sigmoid')
+
+    # x = data * data_gate
+    x = caffe.layers.Eltwise(data, data_gate, operation = 0, name = name + '_Eltwise')
+    return x
+
+
+def resize_conv(x, cnum, kernel_size=3,stride=1, name='upsample', padding='SAME', training=True):
+    # x = resize(x, func=tf.image.resize_bilinear)
+    factor = 2
+    k = 2 * factor - factor % 2
+    p = int(math.ceil((factor - 1) / 2.))
+    x = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, group=cnum,kernel_size=k,stride=2,pad=p,weight_filler={"type": "bilinear"},bias_term=False), name=name+"_deconv")
+    # x = tf.layers.conv2d(
+    #     x, cnum, kernel_size=kernel_size, strides=(stride, stride),
+    #     padding=padding, trainable=training,
+    #     name=name + '_dconv'
+    # )
+    p = kernel_size // 2
+    x = conv(data = x, num_filter = cnum, kernel=kernel_size, stride=stride, pad=p, dilation=1, name=name+"_deconv_conv")
+    return x
+
+def gen_normal_deconv(x, cnum, kernel_size = 4, scale_size = 2,stride = 2, padding='SAME', pad=2, name='upsample', training=True):
+    # data = tf.layers.conv2d_transpose(x, cnum, kernel_size, stride, padding)
+    data = caffe.layers.Deconvolution(x, convolution_param=dict(num_output=cnum, kernel_size=kernel_size,stride=2, pad=1,bias_term=True), name=name+"_deconv")
+    # data = tf.sigmoid(data)
+    data = caffe.layers.Sigmoid(data, in_place = True, name = name + '_deconv_Sigmoid')
+    return data
+
+def getNoise(offset_h, offset_w, width, height):
+    noiseMask = np.random.rand(height, width)
+    if offset_h and offset_w:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    elif offset_h:
+        noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+    elif offset_w:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    return noiseMask1, noiseMask2
+
+def my_create_net(caffe_root, lmdb, mean_file, size=[512, 512], include_acc=False):
+    net = caffe.NetSpec()
+    net.data, net.label = caffe.layers.Data(source=lmdb, backend=caffe.params.Data.LMDB, ntop=2)
+    cnum = 32
+    # input_x, noise = caffe.layers.Slice(net.data, ntop = 2, name = 'slice_noise', slice_param=dict(slice_dim=1,slice_point=[4]))
+    input_x = caffe.layers.Scale(net.data, bias_term = True, name = "input_x_Scale")
+    #stage 1
+    # encode
+    x_1 = gen_gate_conv(input_x, cnum, 5, 2, name='conv1')
+    x_2 = gen_gate_conv(x_1, 2 * cnum, 5, 2, name='conv2')
+    x_3 = gen_gate_conv(x_2, 4 * cnum, 3, 2, name='conv3')
+    x_4 = gen_gate_conv(x_3, 4 * cnum, 3, 2, name='conv4')
+
+    x = resnetblock(x_4, 4 * cnum, name='block1')
+    x = resnetblock(x, 4 * cnum, name='block2')
+    x = resnetblock(x, 4 * cnum, name='block3')
+    x = resnetblock(x, 4 * cnum, name='block4')
+
+    x = gen_normal_deconv(x, 4*cnum, name='upsample1')
+    x = caffe.layers.Concat(x, x_3, axis = 1, name = 'concat1')
+    x = gen_gate_conv(x, 3*cnum, 3, 1, padding='same',name='conv5')
+
+    x = gen_normal_deconv(x, 3*cnum, name='upsample2')
+    x = caffe.layers.Concat(x, x_2, axis = 1, name = 'concat2')
+    x = gen_gate_conv(x, 2*cnum, 3, 1, padding='same',name='conv6')
+
+    x = gen_normal_deconv(x, 2*cnum, name='upsample3')
+    x = caffe.layers.Concat(x, x_1, axis=1, name='concat3')
+    x = gen_gate_conv(x, cnum, 3, 1, padding='same', name='conv7')
+
+    x = gen_normal_deconv(x, cnum, name='upsample4')
+    x = caffe.layers.Concat(x, input_x, axis = 1, name = 'concat8')
+    x = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv8')
+    x = caffe.layers.ReLU(x, in_place=True)
+    net.output = conv(data = x, num_filter = 3, kernel=3, stride=1, pad=1, dilation=1, name='conv9')
+    print("end!")
+    return str(net.to_proto())
+
+
+def write_net():
+    caffe_root = "inpainting/"    #my-caffe-project目录
+    train_lmdb = caffe_root + "train.lmdb"                            #train.lmdb文件的位置
+    mean_file = caffe_root + "mean.binaryproto"                     #均值文件的位置
+#    train_proto = caffe_root + "my_train1.prototxt"                        #保存train_prototxt文件的位置
+    train_proto = caffe_root + "test2-1.prototxt"                        #保存train_prototxt文件的位置
+    #写入prototxt文件
+    with open(train_proto, 'w') as f:
+        f.write(str(my_create_net(caffe_root, train_lmdb, mean_file)))
+
+if __name__ == '__main__':
+    write_net()
+   # net = caffe.Net('train.prototxt', caffe.TEST)
+   # net.save("train.caffemodel")
+
+
Index: weights_tensor2caffe_inpainting1028.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- weights_tensor2caffe_inpainting1028.py	(date 1576045765000)
+++ weights_tensor2caffe_inpainting1028.py	(date 1576045765000)
@@ -0,0 +1,246 @@
+#!usr/bin/python
+# -*- coding: utf-8 -*-
+from __future__ import print_function
+import sys
+import numpy as np
+#caffe_root = '../../caffe_RCNN_Mobile_Shuffe/caffe/' 
+#sys.path.insert(0, caffe_root + 'python')
+
+import caffe
+# 3*3 gauss filter
+gauss_filter_3 = np.array(
+    [1, 1, 1, 1, 40, 1, 1, 1, 1]) / 48.0
+gauss_filter_3 = gauss_filter_3.astype(dtype=np.float32)
+k1 = np.array([1.0]).astype(dtype=np.float32)
+k1 = k1.reshape([1, 1, 1, 1])
+
+def tensor2caffe(net,tensor):
+
+    ct_map={"conv1_conv_f:weights"                              :   "inpaint_net/conv1/conv2d/kernel" ,
+            "conv1_conv_f:bias"                                 :   "inpaint_net/conv1/conv2d/bias" ,
+            "conv1_conv_g_1:weights"                            :   "inpaint_net/conv1/conv2d_1/kernel" ,
+            "conv1_conv_g_1:bias"                               :   "inpaint_net/conv1/conv2d_1/bias" ,
+            "conv2_conv_f:weights"                              :   "inpaint_net/conv2/conv2d/kernel" ,
+            "conv2_conv_f:bias"                                 :   "inpaint_net/conv2/conv2d/bias" ,
+            "conv2_conv_g_1:weights"                            :   "inpaint_net/conv2/conv2d_1/kernel" ,
+            "conv2_conv_g_1:bias"                               :   "inpaint_net/conv2/conv2d_1/bias" ,
+            "conv3_conv_f:weights"                              :   "inpaint_net/conv3/conv2d/kernel" ,
+            "conv3_conv_f:bias"                                 :   "inpaint_net/conv3/conv2d/bias" ,
+            "conv3_conv_g_1:weights"                            :   "inpaint_net/conv3/conv2d_1/kernel" ,
+            "conv3_conv_g_1:bias"                               :   "inpaint_net/conv3/conv2d_1/bias" ,
+            "conv4_conv_f:weights"                              :   "inpaint_net/conv4/conv2d/kernel" ,
+            "conv4_conv_f:bias"                                 :   "inpaint_net/conv4/conv2d/bias" ,
+            "conv4_conv_g_1:weights"                            :   "inpaint_net/conv4/conv2d_1/kernel" ,
+            "conv4_conv_g_1:bias"                               :   "inpaint_net/conv4/conv2d_1/bias" ,
+            "upsample2_deconv:weights"                     :   "inpaint_net/upsample2/conv2d_transpose/kernel" ,
+            "upsample2_deconv:bias"                        :   "inpaint_net/upsample2/conv2d_transpose/bias" ,
+            "conv6_conv_f:weights"                                     :   "inpaint_net/conv6/conv2d/kernel" ,
+            "conv6_conv_f:bias"                                        :   "inpaint_net/conv6/conv2d/bias" ,
+            "conv6_conv_g_1:weights"                                     :   "inpaint_net/conv6/conv2d_1/kernel" ,
+            "conv6_conv_g_1:bias"                                        :   "inpaint_net/conv6/conv2d_1/bias" ,
+            "upsample3_deconv:weights"                     :   "inpaint_net/upsample3/conv2d_transpose/kernel" ,
+            "upsample3_deconv:bias"                        :   "inpaint_net/upsample3/conv2d_transpose/bias" ,
+            "conv7_conv_f:weights"                                     :   "inpaint_net/conv7/conv2d/kernel" ,
+            "conv7_conv_f:bias"                                        :   "inpaint_net/conv7/conv2d/bias" ,
+            "conv7_conv_g_1:weights"                                     :   "inpaint_net/conv7/conv2d_1/kernel" ,
+            "conv7_conv_g_1:bias"                                        :   "inpaint_net/conv7/conv2d_1/bias" ,
+            "upsample4_deconv:weights"                     :   "inpaint_net/upsample4/conv2d_transpose/kernel" ,
+            "upsample4_deconv:bias"                        :   "inpaint_net/upsample4/conv2d_transpose/bias" ,
+            "conv8_conv_f:weights"                                     :   "inpaint_net/conv8/conv2d/kernel" ,
+            "conv8_conv_f:bias"                                        :   "inpaint_net/conv8/conv2d/bias" ,
+            "conv8_conv_g_1:weights"                                     :   "inpaint_net/conv8/conv2d_1/kernel" ,
+            "conv8_conv_g_1:bias"                                        :   "inpaint_net/conv8/conv2d_1/bias" ,
+            "upsample5_deconv:weights"                     :   "inpaint_net/upsample5/conv2d_transpose/kernel" ,
+            "upsample5_deconv:bias"                        :   "inpaint_net/upsample5/conv2d_transpose/bias" ,
+            "conv9:weights"                                   :   "inpaint_net/conv9/kernel" ,
+            "conv9:bias"                                      :   "inpaint_net/conv9/bias" ,
+      }
+    
+    num=0
+    net_keys=net.params.keys()
+    le_last = 0
+    length = len(net_keys)
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        print('i = %s, len = %s, le=%s, key = %s'%(i, len(net_keys), le, net_keys[i]))
+        if le==1:
+            if(net_keys[i] == 'upsample1_deconv'): continue
+            if(net_keys[i] == 'upsample2_deconv'): continue
+            if(net_keys[i] == 'upsample3_deconv'): continue
+            if(net_keys[i] == 'upsample4_deconv'): continue
+            if(net_keys[i] == 'upsample5_deconv'): continue
+            if 'Gaussian' in net_keys[i]:
+                if 'fcn' in net_keys[i]:
+                    weights_value = k1
+                else:
+                    weights_value = gauss_filter_3.reshape([3, 3, 1, 1])
+                weights_value=weights_value.transpose((3,2,0,1)) 
+            else:
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                weights_value=tensor[value0]
+                if "depthwise"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1)) 
+            try:
+                net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+            except:
+                net.params[net_keys[i]][0].data[:,:,:,:]=0
+                net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+            
+            
+            print(weights_value.shape,end=' ')
+            print(net.params[net_keys[i]][0].data.shape)
+            print("%s  <-  %s"%(key0,value0))
+            num=num+1
+            
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                print("scale layer!")
+                net.params[net_keys[i]][0].data[:] = np.array([-1.])
+                net.params[net_keys[i]][1].data[:] = np.array([1.])
+                continue
+                print("le_last = %d"%le_last)
+                # for BN layers
+                key0=net_keys[i]+':gamma'
+                value0 = ct_map[key0]
+                gamma_value=tensor[value0] 
+
+                key1=net_keys[i]+':beta'
+                value1=ct_map[key1]
+                beta_value=tensor[value1]
+
+                value2 = value0.replace('gamma', 'moving_variance')
+                mv_value=tensor[value2] 
+
+                value3 = value1.replace('beta', 'moving_mean')
+                mm_value=tensor[value3] 
+
+
+                net.params[net_keys[i]][0].data[:] = gamma_value / np.sqrt(mv_value)
+                print(gamma_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                net.params[net_keys[i]][1].data[:] = beta_value - mm_value/np.sqrt(mv_value)*gamma_value
+                print(beta_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+            else:   
+                key0=net_keys[i]+':weights'
+                value0=ct_map[key0]
+                
+                weights_value=tensor[value0]
+                if "filter"  in value0:
+                    weights_value=tensor[value0].transpose((2,3,0,1)) 
+                else:
+                    weights_value=tensor[value0].transpose((3,2,0,1))
+                    
+                if net.params[net_keys[i]][0].data.shape[0] == 3:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value[:,:,:,:]
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value[:,:,:,:]
+                else:
+                    try:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=weights_value
+                    except:
+                        net.params[net_keys[i]][0].data[:,:,:,:]=0
+                        net.params[net_keys[i]][0].data[:,:,1:,1:]=weights_value
+#                     assert np.all(weights_value==net.params[net_keys[i]][0].data[:,:,1:,1:])
+#                     assert weights_value.sum() == net.params[net_keys[i]][0].data[:,:,1:,1:].sum()
+                print(weights_value.shape,end=' ')
+                print(net.params[net_keys[i]][0].data.shape)
+                print("%s  <-  %s"%(key0,value0))
+                num=num+1
+            
+                key1=net_keys[i]+':bias'
+                value1=ct_map[key1]
+                
+                bias_value=tensor[value1]
+                net.params[net_keys[i]][1].data[:]=bias_value
+                print(bias_value.shape,end=' ')
+                print(net.params[net_keys[i]][1].data.shape)
+                print("%s  <-  %s"%(key1,value1))
+                num=num+1
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+    print(num)
+    print('net_keys[0]',net_keys[0])
+    net.params[net_keys[0]][0].data[:] = np.array([1./128, 1./128, 1./128, 1./255])
+    net.params[net_keys[0]][1].data[:] = np.array([-1., -1., -1., 0.])
+
+
+
+def print_caffemodel(net):
+    net_keys=net.params.keys()
+    le_last = 0
+    for i in range(1,len(net_keys)):
+        le=len(net.params[net_keys[i]])
+        # print("i(%d).lenth = %d"%(i,le))
+        # print(net_keys[i])
+        if le==1:
+            n,c,w,h=net.params[net_keys[i]][0].data.shape
+            print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+        elif le==2:
+            if '_Sc' in net_keys[i]:
+                # BatchNorm layer后面的scale layer, 训练gamma和deta
+                w=net.params[net_keys[i]][0].data.shape
+                print("%s:gamma ->  [%d]"%(net_keys[i], w[0]))
+
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:beta  ->  [%d]"%(net_keys[i], w[0]))
+            else:
+                n,c,w,h=net.params[net_keys[i]][0].data.shape
+                print("%s:weights  ->  [%d,%d,%d,%d]"%(net_keys[i],n,c,w,h))
+            
+                w=net.params[net_keys[i]][1].data.shape
+                print("%s:bias  ->  [%d]"%(net_keys[i],w[0]))
+        elif le==3:
+            #BatchNorm layer, 此处没有训练gamma和deta， 后面接scale layer
+            m = net.params[net_keys[i]][0].data.shape
+            print("%s:moving_mean    ->  [%d]"%(net_keys[i],m[0]))
+
+            v = net.params[net_keys[i]][1].data.shape
+            print("%s:moving_variance    ->  [%d]"%(net_keys[i],v[0]))
+
+            #f = net.params[net_keys[i]][2].data.shape
+            #print("%s:fraction    ->  [%d]"%(net_keys[i],f[0]))
+        else:
+            print("error: %s"%(net_keys[i]))
+        le_last = le
+
+
+def print_tensorflow(tensor):
+    tensor_keys=tensor.keys()
+    for i in range(0,len(tensor_keys)):
+        le=len(tensor[tensor_keys[i]].shape)
+        if le==1:
+            w=tensor[tensor_keys[i]].shape[0]
+            print("%s  ->  [%d]"%(tensor_keys[i],w))
+        elif le==4:
+            weights=tensor[tensor_keys[i]]
+            #if "filter"  in tensor_keys[i]:
+                #weights=tensor[tensor_keys[i]].transpose((2,3,1,0)) 
+            #else:
+                #weights=tensor[tensor_keys[i]].transpose((3,2,1,0)) 
+            n,c,w,h=weights.shape;
+            print("%s  ->  [%d,%d,%d,%d]"%(tensor_keys[i],n,c,w,h))
+
+#print(tensor_keys)
+
+if __name__ == "__main__":
+    net = caffe.Net('/home/samsung/Data/mm.bai/inpainting/pb2caffe_weihua/inpainting/test10-t5-512.prototxt', caffe.TEST)
+    print_caffemodel(net)
+    print("########################################################################################################################")
+    tensor=np.load('inpainting/test10-t7-512.npy').item()
+    print_tensorflow(tensor)
+    print(
+        "########################################################################################################################")
+    tensor2caffe(net,tensor)
+    net.save("inpainting/test10-t7-512.caffemodel")
Index: save_noise_to_lmdb.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- save_noise_to_lmdb.py	(date 1569387955000)
+++ save_noise_to_lmdb.py	(date 1569387955000)
@@ -0,0 +1,106 @@
+import lmdb
+import caffe
+import numpy as np
+
+
+def getNoise(offset_h, offset_w):
+    noiseMask = np.random.rand(512, 512)
+    if offset_h and offset_w:
+        noiseMask1 = (noiseMask[offset_h:, offset_w:] > 0.5).astype(np.float32)
+    elif offset_h:
+        noiseMask1 = (noiseMask[offset_h:, :] > 0.5).astype(np.float32)
+    elif offset_w:
+        noiseMask1 = (noiseMask[:, offset_w:] > 0.5).astype(np.float32)
+    # noiseMask1 = noiseMask[2:, 1:].astype(np.float32)
+    noiseMask2 = 1 - noiseMask1
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask1 = np.expand_dims(noiseMask1, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    noiseMask2 = np.expand_dims(noiseMask2, axis=0)
+    noiseMask1 = np.concatenate((noiseMask1, noiseMask1, noiseMask1), axis=1)
+    noiseMask2 = np.concatenate((noiseMask2, noiseMask2, noiseMask2), axis=1)
+    return noiseMask1, noiseMask2
+
+noiseMask1_1, noiseMask1_2 = getNoise(2 ,1)
+noiseMask2_1, noiseMask2_2 = getNoise(2, 2)
+
+map_size1 = noiseMask1_1.nbytes
+env1 = lmdb.open('inpainting/noise1_lmdb', map_size=map_size1)
+with env1.begin(write=True) as txn:
+    # txn is a Transaction object
+    for i in range(1):
+        datum1 = caffe.proto.caffe_pb2.Datum()
+        datum1.channels = noiseMask1_1.shape[1]
+        datum1.height = noiseMask1_1.shape[2]
+        datum1.width = noiseMask1_1.shape[3]
+        datum1.data = noiseMask1_1[i].tobytes()
+        str_id = '{:08}'.format(i)
+
+map_size2 = noiseMask1_2.nbytes
+env1 = lmdb.open('inpainting/noise2_lmdb', map_size=map_size2)
+with env1.begin(write=True) as txn:
+    # txn is a Transaction object
+    for i in range(1):
+        datum1 = caffe.proto.caffe_pb2.Datum()
+        datum1.channels = noiseMask1_2.shape[1]
+        datum1.height = noiseMask1_2.shape[2]
+        datum1.width = noiseMask1_2.shape[3]
+        datum1.data = noiseMask1_2[i].tobytes()
+        str_id = '{:08}'.format(i)
+
+map_size3 = noiseMask2_1.nbytes
+env1 = lmdb.open('inpainting/noise3_lmdb', map_size=map_size3)
+with env1.begin(write=True) as txn:
+    # txn is a Transaction object
+    for i in range(1):
+        datum1 = caffe.proto.caffe_pb2.Datum()
+        datum1.channels = noiseMask2_1.shape[1]
+        datum1.height = noiseMask2_1.shape[2]
+        datum1.width = noiseMask2_1.shape[3]
+        datum1.data = noiseMask2_1[i].tobytes()
+        str_id = '{:08}'.format(i)
+
+map_size4 = noiseMask2_2.nbytes
+env1 = lmdb.open('inpainting/noise4_lmdb', map_size=map_size4)
+with env1.begin(write=True) as txn:
+    # txn is a Transaction object
+    for i in range(1):
+        datum1 = caffe.proto.caffe_pb2.Datum()
+        datum1.channels = noiseMask2_2.shape[1]
+        datum1.height = noiseMask2_2.shape[2]
+        datum1.width = noiseMask2_2.shape[3]
+        datum1.data = noiseMask2_2[i].tobytes()
+        str_id = '{:08}'.format(i)
+
+# Let's pretend this is interesting data
+# noiseDt1 = np.concatenate((noiseMask1_1,noiseMask1_2), axis=0)
+# map_size1 = noiseDt1.nbytes
+# env1 = lmdb.open('inpainting/noise1_lmdb', map_size=map_size1)
+# with env1.begin(write=True) as txn:
+#     # txn is a Transaction object
+#     for i in range(1):
+#         datum1 = caffe.proto.caffe_pb2.Datum()
+#         datum1.channels = noiseDt1.shape[1]
+#         datum1.height = noiseDt1.shape[2]
+#         datum1.width = noiseDt1.shape[3]
+#         datum1.data = noiseDt1[i].tobytes()  # or .tostring() if numpy < 1.9
+#         str_id = '{:08}'.format(i)
+#
+#         # The encode is only essential in Python 3
+#         # txn.put(str_id.encode('ascii'), datum.SerializeToString())
+
+# noiseDt2 = np.concatenate((noiseMask2_1,noiseMask2_2), axis=0)
+# map_size2 = noiseDt2.nbytes
+# env2 = lmdb.open('inpainting/noise2_lmdb', map_size=map_size2)
+# with env2.begin(write=True) as txn:
+#     # txn is a Transaction object
+#     for i in range(1):
+#         datum2 = caffe.proto.caffe_pb2.Datum()
+#         datum2.channels = noiseDt2.shape[1]
+#         datum2.height = noiseDt2.shape[2]
+#         datum2.width = noiseDt2.shape[3]
+#         datum2.data = noiseDt2[i].tobytes()  # or .tostring() if numpy < 1.9
+#         str_id = '{:08}'.format(i)
+#
+#         # The encode is only essential in Python 3
+#         # txn.put(str_id.encode('ascii'), datum.SerializeToString())
\ No newline at end of file
Index: pb2npy_inpainting.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pb2npy_inpainting.py	(date 1579682149000)
+++ pb2npy_inpainting.py	(date 1579682149000)
@@ -0,0 +1,110 @@
+# -*- coding: utf-8 -*-
+"""
+Created on Thu Sep 27 14:29:56 2018
+
+@author: wei17.zhao
+"""
+
+
+import time
+import random
+import os
+import tensorflow as tf
+import numpy as np
+np.set_printoptions(precision=2)
+import argparse
+import json
+
+from matplotlib import pyplot as plt
+from tensorflow.python.tools import freeze_graph
+from skimage import io, transform, exposure, color
+from scipy.io import loadmat
+from skimage.filters import gaussian
+from skimage.feature import canny
+
+
+def load_graph(model_file):
+  graph = tf.Graph()
+  graph_def = tf.GraphDef()
+  with open(model_file, "rb") as f:
+    graph_def.ParseFromString(f.read())
+  with graph.as_default():
+    tf.import_graph_def(graph_def)
+  return graph
+
+def get_npy(pb_name):
+    graph = load_graph(pb_name)
+    ops = graph.get_operations()
+    to_save = {}
+    sess = tf.Session(graph=graph)    
+    for op in ops:
+        #print(op.name)
+        if op.name.endswith('kernel/read') or op.name.endswith('bias/read') or op.name.endswith('filter/read'):
+            x = sess.run(op.outputs[0])
+            print(op.name,'shape:%s'%str(x.shape))
+            to_save['/'.join(op.name.split('/')[1:-1])] = x
+    np.save(pb_name[:-3], to_save)
+    
+def get_npy_segmentation(pb_name):
+    graph = load_graph('seg_pbs/%s'%pb_name)
+    ops = graph.get_operations()
+    to_save = {}
+    sess = tf.Session(graph=graph)    
+    for op in ops:
+#         print('/'.join(op.name.split('/')[1:-1]))
+        if op.name.endswith('kernel/read') or op.name.endswith('bias/read') or op.name.endswith('filter/read') or op.name.endswith('beta/read') or op.name.endswith('gamma/read') or op.name.endswith('moving_mean/read') or op.name.endswith('moving_variance/read') or op.name.endswith('weights/read') or op.name.endswith('biases/read'):
+            x = sess.run(op.outputs[0])
+            print('/'.join(op.name.split('/')[1:-1]), 'shape:%s'%str(x.shape))
+            to_save['/'.join(op.name.split('/')[1:-1])] = x
+    np.save('seg_pbs/%s'%pb_name[:-3], to_save)
+
+def get_npy_inpainting(pb_name):
+    graph = load_graph('inpainting/%s'%pb_name)
+    ops = graph.get_operations()
+    to_save = {}
+    sess = tf.Session(graph=graph)    
+    for op in ops:
+#         print('/'.join(op.name.split('/')[1:-1]))
+        if op.name.endswith('kernel/read') or \
+                op.name.endswith('bias/read') or \
+                op.name.endswith('filter/read') or \
+                op.name.endswith('beta/read') or \
+                op.name.endswith('gamma/read') or \
+                op.name.endswith('moving_mean/read') or \
+                op.name.endswith('moving_variance/read') or \
+                op.name.endswith('weights/read') or \
+                op.name.endswith('biases/read'):
+            x = sess.run(op.outputs[0])
+            print('/'.join(op.name.split('/')[1:-1]), 'shape:%s'%str(x.shape))
+            to_save['/'.join(op.name.split('/')[1:-1])] = x
+    np.save('inpainting/%s'%pb_name[:-3], to_save)    
+
+def get_middle(pb_name):
+    image = np.ones([1,1984,1152,3], np.float32)
+    graph = load_graph(pb_name)
+    ops = graph.get_operations()
+    to_save = {}
+    input_name = 'import/Placeholder'
+    input_op = graph.get_operation_by_name(input_name)
+    sess = tf.Session(graph=graph)
+    for op in ops:
+        if op.name.endswith('Relu'):
+            x = sess.run(op.outputs[0], feed_dict={input_op.outputs[0]: image})
+            to_save['/'.join(op.name.split('/')[1:])] = x
+    print(to_save.keys())
+    return to_save
+    
+#get_middle('style_pbs/lowres_smaller_res_dc_bn_linearout_%s_snpe.pb'%'starry')
+#get_npy_segmentation('exported_psp50b3_samdata-65000_prob.pb')
+#get_npy_segmentation('exported_sub_train_eden_0618_2019_seg_db_from_cv_continue_single_clean_prob.pb')
+#for style in ['campo', 'comp5', 'georges_seurat', 'pwave', 'starry']:
+#    get_npy('style_pbs/g_s2k4_%s_snpe.pb'%style)
+#get_npy('style_pbs/lowres_smaller_res_dc_bn_linearout_%s_snpe.pb'%'campo')
+
+#for style in ['campo-1101-histogram2', 'campo-1101-histogram', 'comp5-1101-histogram2', 'comp5-1101-histogram', 'Georges_Seurat-1102-histogram2', 'Georges_Seurat-1102-histogram', 'neon_1', 'neon_2', 'neon_3', 'neon_4', 'new_watercolor_1', 'new_watercolor_2', 'new_watercolor_3', 'new_watercolor_4', 'new_watercolor_5', 'new_watercolor_6', 'new_watercolor_7', 'new_watercolor_8', 'new_watercolor_9', 'pwave-1102-histogram2', 'pwave-1102-histogram', 'starry-1106-histogram2']:
+#    get_npy('style_pbs/1207_1/g_s2k4_%s_snpe.pb'%style)
+
+# for style in ['starry-1106-histogram2']:
+#     #get_npy('style_pbs/1211/g_s2k4_%s_snpe.pb'%style)
+#     get_npy('style_pbs/g_s2k4_gradation-32100_snpe.pb')
+get_npy_inpainting("test10-t11-0121.pb")
\ No newline at end of file
